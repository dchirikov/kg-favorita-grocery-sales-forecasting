{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gc\n",
    "import hyperdash as hd\n",
    "\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#'store_nbr', 'n_city', 'n_state', 'n_type', 'cluster', 'item_nbr', 'n_family', 'class', 'perishable'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unit_mean, unit_std = pd.read_csv('data/mean_std.csv', index_col=0).T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_items['class'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4100 entries, 0 to 4099\n",
      "Data columns (total 4 columns):\n",
      "item_nbr      4100 non-null int32\n",
      "n_family      4100 non-null uint32\n",
      "class         4100 non-null int32\n",
      "perishable    4100 non-null int8\n",
      "dtypes: int32(2), int8(1), uint32(1)\n",
      "memory usage: 52.1 KB\n"
     ]
    }
   ],
   "source": [
    "df_items.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_stores = pd.read_csv(\n",
    "    'data/num_stores.csv.gz',\n",
    "     dtype={\n",
    "         'store_nbr': np.uint8,\n",
    "         'n_city': np.uint32,\n",
    "         'n_state': np.uint32,\n",
    "         'n_type': np.uint32,\n",
    "         'cluster': np.uint32\n",
    "     }\n",
    "\n",
    ")\n",
    "df_items = pd.read_csv(\n",
    "    'data/num_items.csv.gz',\n",
    "    dtype={\n",
    "        'item_nbr': np.int32,\n",
    "        'n_family': np.int32,\n",
    "        'class': np.int32,\n",
    "        'perishable': np.int8,\n",
    "    }\n",
    ")\n",
    "for stores_col in ['n_city', 'n_state', 'n_type', 'cluster']:\n",
    "    df_stores[stores_col] = df_stores[stores_col] - df_stores[stores_col].min()\n",
    "    \n",
    "for items_col in ['n_family', 'class', 'perishable']:\n",
    "    df_items[items_col] = df_items[items_col] - df_items[items_col].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 34s, sys: 18.3 s, total: 5min 52s\n",
      "Wall time: 5min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_csv(\n",
    "    'data/ts.csv.gz',\n",
    "    parse_dates=[0],\n",
    "    #nrows=1000000,\n",
    "    dtype={\n",
    "        'item_nbr': np.int32,\n",
    "        'store_nbr': np.int8,\n",
    "        'unit_sales': np.float32,\n",
    "        'onpromotion': np.int8,\n",
    "        'holiday': np.int8,\n",
    "        'weekend': np.int8,\n",
    "        'waged_day': np.int8,\n",
    "        'dow_0': np.int8,\n",
    "        'dow_1': np.int8,\n",
    "        'dow_2': np.int8,\n",
    "        'dow_3': np.int8,\n",
    "        'dow_4': np.int8,\n",
    "        'dow_5': np.int8,\n",
    "        'dow_6': np.int8,\n",
    "    }\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmitry/miniconda3/envs/tf_intel/lib/python3.5/site-packages/pandas/core/reshape/merge.py:551: UserWarning: merging between different levels can give an unintended result (2 levels on the left, 1 on the right)\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 49s, sys: 2min 43s, total: 15min 32s\n",
      "Wall time: 12min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ts_columns = df.columns[3:]\n",
    "      \n",
    "attr_cols = [\n",
    "    'store_nbr', 'n_city', 'n_state', 'n_type', 'cluster',\n",
    "    'item_nbr', 'n_family', 'class',\n",
    "    #'perishable'\n",
    "]\n",
    "\n",
    "df_pivot = df.pivot_table(\n",
    "    index=['store_nbr', 'item_nbr'],\n",
    "    columns=['date'],\n",
    "    values=ts_columns\n",
    ").reset_index()\n",
    "\n",
    "df_pivot = df_pivot.merge(df_items, on='item_nbr')\n",
    "df_pivot['store_nbr'] = df_pivot[('store_nbr', '')]\n",
    "df_pivot = df_pivot.merge(df_stores, on='store_nbr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 210654 entries, 0 to 210653\n",
      "Columns: 9887 entries, item_nbr to cluster\n",
      "dtypes: float64(823), int32(1), int64(4), int8(9054), uint32(5)\n",
      "memory usage: 3.1 GB\n"
     ]
    }
   ],
   "source": [
    "df_pivot.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(onpromotion, 2017-07-31 00:00:00)</th>\n",
       "      <th>(holiday, 2017-07-31 00:00:00)</th>\n",
       "      <th>(weekend, 2017-07-31 00:00:00)</th>\n",
       "      <th>(waged_day, 2017-07-31 00:00:00)</th>\n",
       "      <th>(dow_0, 2017-07-31 00:00:00)</th>\n",
       "      <th>(dow_1, 2017-07-31 00:00:00)</th>\n",
       "      <th>(dow_2, 2017-07-31 00:00:00)</th>\n",
       "      <th>(dow_3, 2017-07-31 00:00:00)</th>\n",
       "      <th>(dow_4, 2017-07-31 00:00:00)</th>\n",
       "      <th>(dow_5, 2017-07-31 00:00:00)</th>\n",
       "      <th>...</th>\n",
       "      <th>(holiday, 2017-08-15 00:00:00)</th>\n",
       "      <th>(weekend, 2017-08-15 00:00:00)</th>\n",
       "      <th>(waged_day, 2017-08-15 00:00:00)</th>\n",
       "      <th>(dow_0, 2017-08-15 00:00:00)</th>\n",
       "      <th>(dow_1, 2017-08-15 00:00:00)</th>\n",
       "      <th>(dow_2, 2017-08-15 00:00:00)</th>\n",
       "      <th>(dow_3, 2017-08-15 00:00:00)</th>\n",
       "      <th>(dow_4, 2017-08-15 00:00:00)</th>\n",
       "      <th>(dow_5, 2017-08-15 00:00:00)</th>\n",
       "      <th>(dow_6, 2017-08-15 00:00:00)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 176 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   (onpromotion, 2017-07-31 00:00:00)  (holiday, 2017-07-31 00:00:00)  \\\n",
       "0                                   0                               0   \n",
       "1                                   0                               0   \n",
       "2                                   0                               0   \n",
       "3                                   0                               0   \n",
       "4                                   0                               0   \n",
       "\n",
       "   (weekend, 2017-07-31 00:00:00)  (waged_day, 2017-07-31 00:00:00)  \\\n",
       "0                               0                                 1   \n",
       "1                               0                                 1   \n",
       "2                               0                                 1   \n",
       "3                               0                                 1   \n",
       "4                               0                                 1   \n",
       "\n",
       "   (dow_0, 2017-07-31 00:00:00)  (dow_1, 2017-07-31 00:00:00)  \\\n",
       "0                             1                             0   \n",
       "1                             1                             0   \n",
       "2                             1                             0   \n",
       "3                             1                             0   \n",
       "4                             1                             0   \n",
       "\n",
       "   (dow_2, 2017-07-31 00:00:00)  (dow_3, 2017-07-31 00:00:00)  \\\n",
       "0                             0                             0   \n",
       "1                             0                             0   \n",
       "2                             0                             0   \n",
       "3                             0                             0   \n",
       "4                             0                             0   \n",
       "\n",
       "   (dow_4, 2017-07-31 00:00:00)  (dow_5, 2017-07-31 00:00:00)  \\\n",
       "0                             0                             0   \n",
       "1                             0                             0   \n",
       "2                             0                             0   \n",
       "3                             0                             0   \n",
       "4                             0                             0   \n",
       "\n",
       "               ...               (holiday, 2017-08-15 00:00:00)  \\\n",
       "0              ...                                            0   \n",
       "1              ...                                            0   \n",
       "2              ...                                            0   \n",
       "3              ...                                            0   \n",
       "4              ...                                            0   \n",
       "\n",
       "   (weekend, 2017-08-15 00:00:00)  (waged_day, 2017-08-15 00:00:00)  \\\n",
       "0                               0                                 1   \n",
       "1                               0                                 1   \n",
       "2                               0                                 1   \n",
       "3                               0                                 1   \n",
       "4                               0                                 1   \n",
       "\n",
       "   (dow_0, 2017-08-15 00:00:00)  (dow_1, 2017-08-15 00:00:00)  \\\n",
       "0                             0                             1   \n",
       "1                             0                             1   \n",
       "2                             0                             1   \n",
       "3                             0                             1   \n",
       "4                             0                             1   \n",
       "\n",
       "   (dow_2, 2017-08-15 00:00:00)  (dow_3, 2017-08-15 00:00:00)  \\\n",
       "0                             0                             0   \n",
       "1                             0                             0   \n",
       "2                             0                             0   \n",
       "3                             0                             0   \n",
       "4                             0                             0   \n",
       "\n",
       "   (dow_4, 2017-08-15 00:00:00)  (dow_5, 2017-08-15 00:00:00)  \\\n",
       "0                             0                             0   \n",
       "1                             0                             0   \n",
       "2                             0                             0   \n",
       "3                             0                             0   \n",
       "4                             0                             0   \n",
       "\n",
       "   (dow_6, 2017-08-15 00:00:00)  \n",
       "0                             0  \n",
       "1                             0  \n",
       "2                             0  \n",
       "3                             0  \n",
       "4                             0  \n",
       "\n",
       "[5 rows x 176 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_date_cols(date, history=20, predict_days=16, ts_columns=ts_columns, skip=0):\n",
    "                  #date, days=1, attr_cols=attr_columns_wo_means, ts_cols=ts_columns, attr=True):\n",
    "    \n",
    "    if type(date) != pd.Timestamp:\n",
    "        date = pd.to_datetime(date)\n",
    "        \n",
    "    X_start_date = date - pd.Timedelta('{} days'.format(history-1))\n",
    "    #X_end_date = date\n",
    "    y_start_date = date + pd.Timedelta('{} days'.format(skip+1))\n",
    "    #y_end_date = date + pd.Timedelta('{} days'.format(predict_days))\n",
    "\n",
    "    X_cols, y_cols, y_day_attr_cols = [], [], []\n",
    "    \n",
    "    for d in pd.date_range(X_start_date, periods=history, freq='D'):\n",
    "        for elem in ts_columns:\n",
    "            X_cols.append((elem, d))\n",
    "            \n",
    "    for d in pd.date_range(y_start_date, periods=predict_days, freq='D'):\n",
    "        y_cols.append(('unit_sales_scaled', d))\n",
    "        for elem in ts_columns[1:]:\n",
    "            y_day_attr_cols.append((elem, d))\n",
    "            \n",
    "    return X_cols, y_cols, y_day_attr_cols\n",
    "\n",
    "\n",
    "\n",
    "X_cols, y_cols, y_day_attr_cols = get_date_cols('2017-07-30', predict_days=16)\n",
    "    \n",
    "df_pivot.head().loc[:, y_day_attr_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2017-07-01', '2017-07-08', '2017-07-15'], dtype='datetime64[ns]', freq='7D')\n",
      "(90, 12) (5, 11) (5,) ()\n"
     ]
    }
   ],
   "source": [
    "def get_random_train_test(df_pivot,\n",
    "        date, window=21, freq=7, size=2000, history=1, predict_days=16, epochs=2, \n",
    "        shuffle_dates=True, shuffle_indexes=True, attr_cols=attr_cols, ts_columns=ts_columns, skip=0):\n",
    "    \n",
    "    \n",
    "    window = freq * (window//freq)\n",
    "    num_items = df_pivot.shape[0]\n",
    "    \n",
    "    date = pd.to_datetime(date)\n",
    "    start_window =  date - pd.Timedelta('{} days'.format(window))\n",
    "    end_date = date\n",
    "    \n",
    "    dates = pd.date_range(start=start_window, end=end_date)\n",
    "    dates = dates[::freq]\n",
    "    \n",
    "    print(dates)\n",
    "    \n",
    "    patches = []\n",
    "    #end_X_date = end_date - pd.Timedelta('{} days'.format(label_dates))\n",
    "    if shuffle_dates and shuffle_indexes:\n",
    "        permutated_dates = np.random.permutation(dates)\n",
    "        permutated_indx = np.random.permutation(num_items)   \n",
    "        for epoch in range(epochs):\n",
    "            for i in range(num_items//size+1):\n",
    "                s = size * i\n",
    "                e = size * (i+1)\n",
    "                indexes = permutated_indx[s:e]\n",
    "\n",
    "                for date in permutated_dates:\n",
    "                    patches.append([indexes, date])\n",
    "\n",
    "        patches = np.random.permutation(patches)\n",
    "        \n",
    "    elif not shuffle_dates and shuffle_indexes:\n",
    "        permutated_indx = np.random.permutation(num_items)\n",
    "        for date in dates:\n",
    "            for epoch in range(epochs):\n",
    "                for i in range(num_items//size+1):\n",
    "                    s = size * i\n",
    "                    e = size * (i+1)\n",
    "                    indexes = permutated_indx[s:e]\n",
    "                    patches.append([indexes, date])\n",
    "\n",
    "    for indexes, date in patches:\n",
    "        df_pivot_slice = df_pivot.iloc[indexes]\n",
    "        X_cols, y_cols, y_day_attr_cols = get_date_cols(\n",
    "            date, history=history, predict_days=predict_days, ts_columns=ts_columns, skip=skip\n",
    "        )\n",
    "\n",
    "        X = np.array(\n",
    "            df_pivot_slice.loc[:, X_cols]\n",
    "        ).reshape([-1, history, len(ts_columns)])\n",
    "\n",
    "        y_day_attr = np.array(\n",
    "            df_pivot_slice.loc[:, y_day_attr_cols]\n",
    "        ).reshape([-1, predict_days, len(ts_columns)-1])\n",
    "        \n",
    "        y = np.array(df_pivot_slice.loc[:, y_cols])\n",
    "        features = [X, y_day_attr, y]\n",
    "        for feature in attr_cols:\n",
    "            features.append(\n",
    "                np.array(df_pivot_slice.loc[:, feature])\n",
    "            )\n",
    "        for i in range(len(indexes)):\n",
    "            yield tuple([elem[i] for elem in features])\n",
    "\n",
    "tmp = get_random_train_test(df_pivot, '2017-07-15', window=20, history=90, predict_days=5)\n",
    "tmp1 = next(tmp)\n",
    "print(tmp1[0].shape, tmp1[1].shape, tmp1[2].shape, tmp1[3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(2000):\n",
    "    next(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.71689729,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [-0.71689729,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [-0.71689729,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        ..., \n",
       "        [-0.71689729,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [-0.71689729,  0.        ,  0.        , ...,  1.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [-0.71689729,  0.        ,  0.        , ...,  0.        ,\n",
       "          1.        ,  0.        ]]), array([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]], dtype=int8), array([-0.71689729, -0.71689729, -0.71689729, -0.71689729, -0.71689729]), 36, 11, 6, 4, 9, 1960806, 30, 106)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210654, 90, 12) (210654, 5, 11) (210654, 5) (210654,)\n"
     ]
    }
   ],
   "source": [
    "def get_validation(df_pivot,\n",
    "        date, history=1, predict_days=16, attr_cols=attr_cols, ts_columns=ts_columns, skip=0):\n",
    "    \n",
    "    X_cols, y_cols, y_day_attr_cols = get_date_cols(\n",
    "        date, history=history, predict_days=predict_days, ts_columns=ts_columns, skip=skip\n",
    "    )\n",
    "\n",
    "    X = np.array(\n",
    "        df_pivot.loc[:, X_cols]\n",
    "    ).reshape([-1, history, len(ts_columns)])\n",
    "    \n",
    "    y_day_attr = np.array(\n",
    "        df_pivot.loc[:, y_day_attr_cols]\n",
    "    ).reshape([-1, predict_days, len(ts_columns)-1])\n",
    "\n",
    "    y = np.array(df_pivot.loc[:, y_cols])\n",
    "    features = [X, y_day_attr, y]\n",
    "    for feature in attr_cols:\n",
    "        features.append(\n",
    "            np.array(df_pivot.loc[:, feature])\n",
    "        )\n",
    "\n",
    "    return features\n",
    "\n",
    "tmp = get_validation(df_pivot, '2017-07-15', history=90, predict_days=5)\n",
    "print(tmp[0].shape, tmp[1].shape, tmp[2].shape, tmp[3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-30 00:00:00\n",
      "1\n",
      "DatetimeIndex(['2016-10-03', '2016-10-04', '2016-10-05', '2016-10-06',\n",
      "               '2016-10-07', '2016-10-08', '2016-10-09', '2016-10-10',\n",
      "               '2016-10-11', '2016-10-12',\n",
      "               ...\n",
      "               '2017-07-21', '2017-07-22', '2017-07-23', '2017-07-24',\n",
      "               '2017-07-25', '2017-07-26', '2017-07-27', '2017-07-28',\n",
      "               '2017-07-29', '2017-07-30'],\n",
      "              dtype='datetime64[ns]', length=301, freq='D')\n",
      "g_step: 100 loss std/mean: 0.18680866062641144 0.42838025093078613\n",
      "| Loss std:   0.186809 |\n",
      "| Loss mean:   0.428380 |\n",
      "g_step: 200 loss std/mean: 0.041691552847623825 0.3153567314147949\n",
      "| Loss std:   0.041692 |\n",
      "| Loss mean:   0.315357 |\n",
      "g_step: 300 loss std/mean: 0.05267135426402092 0.30759337544441223\n",
      "| Loss std:   0.052671 |\n",
      "| Loss mean:   0.307593 |\n",
      "g_step: 400 loss std/mean: 0.043438930064439774 0.29899081587791443\n",
      "| Loss std:   0.043439 |\n",
      "| Loss mean:   0.298991 |\n",
      "g_step: 500 loss std/mean: 0.05342321842908859 0.30510950088500977\n",
      "| Loss std:   0.053423 |\n",
      "| Loss mean:   0.305110 |\n",
      "g_step: 600 loss std/mean: 0.04753962159156799 0.29573583602905273\n",
      "| Loss std:   0.047540 |\n",
      "| Loss mean:   0.295736 |\n",
      "g_step: 700 loss std/mean: 0.04360854625701904 0.2946521043777466\n",
      "| Loss std:   0.043609 |\n",
      "| Loss mean:   0.294652 |\n",
      "g_step: 800 loss std/mean: 0.036156609654426575 0.28715646266937256\n",
      "| Loss std:   0.036157 |\n",
      "| Loss mean:   0.287156 |\n",
      "g_step: 900 loss std/mean: 0.05371902137994766 0.3067854344844818\n",
      "| Loss std:   0.053719 |\n",
      "| Loss mean:   0.306785 |\n",
      "g_step: 1000 loss std/mean: 0.04271848127245903 0.2862386107444763\n",
      "| Loss std:   0.042718 |\n",
      "| Loss mean:   0.286239 |\n",
      "g_step: 1100 loss std/mean: 0.034109700471162796 0.28885185718536377\n",
      "| Loss std:   0.034110 |\n",
      "| Loss mean:   0.288852 |\n",
      "g_step: 1200 loss std/mean: 0.050271496176719666 0.2974852919578552\n",
      "| Loss std:   0.050271 |\n",
      "| Loss mean:   0.297485 |\n",
      "g_step: 1300 loss std/mean: 0.0529918447136879 0.2918495535850525\n",
      "| Loss std:   0.052992 |\n",
      "| Loss mean:   0.291850 |\n",
      "g_step: 1400 loss std/mean: 0.039884068071842194 0.2842138409614563\n",
      "| Loss std:   0.039884 |\n",
      "| Loss mean:   0.284214 |\n",
      "g_step: 1500 loss std/mean: 0.03834519907832146 0.2825517952442169\n",
      "| Loss std:   0.038345 |\n",
      "| Loss mean:   0.282552 |\n",
      "g_step: 1600 loss std/mean: 0.04766711965203285 0.28758639097213745\n",
      "| Loss std:   0.047667 |\n",
      "| Loss mean:   0.287586 |\n",
      "g_step: 1700 loss std/mean: 0.03868362307548523 0.27812325954437256\n",
      "| Loss std:   0.038684 |\n",
      "| Loss mean:   0.278123 |\n",
      "g_step: 1800 loss std/mean: 0.05190631374716759 0.2855203449726105\n",
      "| Loss std:   0.051906 |\n",
      "| Loss mean:   0.285520 |\n",
      "g_step: 1900 loss std/mean: 0.04036761447787285 0.2836638391017914\n",
      "| Loss std:   0.040368 |\n",
      "| Loss mean:   0.283664 |\n",
      "g_step: 2000 loss std/mean: 0.05346009135246277 0.29170674085617065\n",
      "| Loss std:   0.053460 |\n",
      "| Loss mean:   0.291707 |\n",
      "g_step: 2100 loss std/mean: 0.03589368611574173 0.27681171894073486\n",
      "| Loss std:   0.035894 |\n",
      "| Loss mean:   0.276812 |\n",
      "g_step: 2200 loss std/mean: 0.04200420901179314 0.28015193343162537\n",
      "| Loss std:   0.042004 |\n",
      "| Loss mean:   0.280152 |\n",
      "g_step: 2300 loss std/mean: 0.030819185078144073 0.27457335591316223\n",
      "| Loss std:   0.030819 |\n",
      "| Loss mean:   0.274573 |\n",
      "g_step: 2400 loss std/mean: 0.04284646734595299 0.27401337027549744\n",
      "| Loss std:   0.042846 |\n",
      "| Loss mean:   0.274013 |\n",
      "g_step: 2500 loss std/mean: 0.03828174248337746 0.27711033821105957\n",
      "| Loss std:   0.038282 |\n",
      "| Loss mean:   0.277110 |\n",
      "g_step: 2600 loss std/mean: 0.04906680807471275 0.28246086835861206\n",
      "| Loss std:   0.049067 |\n",
      "| Loss mean:   0.282461 |\n",
      "g_step: 2700 loss std/mean: 0.03864028677344322 0.27963483333587646\n",
      "| Loss std:   0.038640 |\n",
      "| Loss mean:   0.279635 |\n",
      "g_step: 2800 loss std/mean: 0.03777974098920822 0.28074783086776733\n",
      "| Loss std:   0.037780 |\n",
      "| Loss mean:   0.280748 |\n",
      "g_step: 2900 loss std/mean: 0.04073821380734444 0.28283053636550903\n",
      "| Loss std:   0.040738 |\n",
      "| Loss mean:   0.282831 |\n",
      "g_step: 3000 loss std/mean: 0.0334167405962944 0.27634289860725403\n",
      "| Loss std:   0.033417 |\n",
      "| Loss mean:   0.276343 |\n",
      "g_step: 3100 loss std/mean: 0.043438784778118134 0.2866019606590271\n",
      "| Loss std:   0.043439 |\n",
      "| Loss mean:   0.286602 |\n",
      "g_step: 3200 loss std/mean: 0.03552168235182762 0.2736637592315674\n",
      "| Loss std:   0.035522 |\n",
      "| Loss mean:   0.273664 |\n",
      "g_step: 3300 loss std/mean: 0.03584016487002373 0.28020229935646057\n",
      "| Loss std:   0.035840 |\n",
      "| Loss mean:   0.280202 |\n",
      "g_step: 3400 loss std/mean: 0.035338401794433594 0.2708635628223419\n",
      "| Loss std:   0.035338 |\n",
      "| Loss mean:   0.270864 |\n",
      "g_step: 3500 loss std/mean: 0.03742599114775658 0.2826792597770691\n",
      "| Loss std:   0.037426 |\n",
      "| Loss mean:   0.282679 |\n",
      "g_step: 3600 loss std/mean: 0.036021966487169266 0.2794315218925476\n",
      "| Loss std:   0.036022 |\n",
      "| Loss mean:   0.279432 |\n",
      "g_step: 3700 loss std/mean: 0.03552200272679329 0.27021142840385437\n",
      "| Loss std:   0.035522 |\n",
      "| Loss mean:   0.270211 |\n",
      "g_step: 3800 loss std/mean: 0.02911689504981041 0.26890042424201965\n",
      "| Loss std:   0.029117 |\n",
      "| Loss mean:   0.268900 |\n",
      "g_step: 3900 loss std/mean: 0.03656676039099693 0.2768147587776184\n",
      "| Loss std:   0.036567 |\n",
      "| Loss mean:   0.276815 |\n",
      "g_step: 4000 loss std/mean: 0.03682738170027733 0.27100953459739685\n",
      "| Loss std:   0.036827 |\n",
      "| Loss mean:   0.271010 |\n",
      "g_step: 4100 loss std/mean: 0.03293564170598984 0.27558812499046326\n",
      "| Loss std:   0.032936 |\n",
      "| Loss mean:   0.275588 |\n",
      "g_step: 4200 loss std/mean: 0.033289678394794464 0.2725861966609955\n",
      "| Loss std:   0.033290 |\n",
      "| Loss mean:   0.272586 |\n",
      "g_step: 4300 loss std/mean: 0.04426956549286842 0.27544283866882324\n",
      "| Loss std:   0.044270 |\n",
      "| Loss mean:   0.275443 |\n",
      "g_step: 4400 loss std/mean: 0.031004581600427628 0.2733103036880493\n",
      "| Loss std:   0.031005 |\n",
      "| Loss mean:   0.273310 |\n",
      "g_step: 4500 loss std/mean: 0.02701885998249054 0.2685064971446991\n",
      "| Loss std:   0.027019 |\n",
      "| Loss mean:   0.268506 |\n",
      "g_step: 4600 loss std/mean: 0.029830681160092354 0.27182626724243164\n",
      "| Loss std:   0.029831 |\n",
      "| Loss mean:   0.271826 |\n",
      "g_step: 4700 loss std/mean: 0.027809137478470802 0.27587613463401794\n",
      "| Loss std:   0.027809 |\n",
      "| Loss mean:   0.275876 |\n",
      "g_step: 4800 loss std/mean: 0.02902628853917122 0.2742346525192261\n",
      "| Loss std:   0.029026 |\n",
      "| Loss mean:   0.274235 |\n",
      "g_step: 4900 loss std/mean: 0.03285280615091324 0.270862340927124\n",
      "| Loss std:   0.032853 |\n",
      "| Loss mean:   0.270862 |\n",
      "g_step: 5000 loss std/mean: 0.027985423803329468 0.26669853925704956\n",
      "| Loss std:   0.027985 |\n",
      "| Loss mean:   0.266699 |\n",
      "g_step: 5100 loss std/mean: 0.03672276437282562 0.2751394212245941\n",
      "| Loss std:   0.036723 |\n",
      "| Loss mean:   0.275139 |\n",
      "g_step: 5200 loss std/mean: 0.029938068240880966 0.2690374553203583\n",
      "| Loss std:   0.029938 |\n",
      "| Loss mean:   0.269037 |\n",
      "g_step: 5300 loss std/mean: 0.02507266029715538 0.26469185948371887\n",
      "| Loss std:   0.025073 |\n",
      "| Loss mean:   0.264692 |\n",
      "g_step: 5400 loss std/mean: 0.030203985050320625 0.27157220244407654\n",
      "| Loss std:   0.030204 |\n",
      "| Loss mean:   0.271572 |\n",
      "g_step: 5500 loss std/mean: 0.026429370045661926 0.2585243880748749\n",
      "| Loss std:   0.026429 |\n",
      "| Loss mean:   0.258524 |\n",
      "g_step: 5600 loss std/mean: 0.03538522496819496 0.27421265840530396\n",
      "| Loss std:   0.035385 |\n",
      "| Loss mean:   0.274213 |\n",
      "g_step: 5700 loss std/mean: 0.028386440128087997 0.26326364278793335\n",
      "| Loss std:   0.028386 |\n",
      "| Loss mean:   0.263264 |\n",
      "g_step: 5800 loss std/mean: 0.03224918991327286 0.2641914188861847\n",
      "| Loss std:   0.032249 |\n",
      "| Loss mean:   0.264191 |\n",
      "g_step: 5900 loss std/mean: 0.031005172058939934 0.265728235244751\n",
      "| Loss std:   0.031005 |\n",
      "| Loss mean:   0.265728 |\n",
      "g_step: 6000 loss std/mean: 0.028843531385064125 0.2643892765045166\n",
      "| Loss std:   0.028844 |\n",
      "| Loss mean:   0.264389 |\n",
      "g_step: 6100 loss std/mean: 0.03338592126965523 0.2748359441757202\n",
      "| Loss std:   0.033386 |\n",
      "| Loss mean:   0.274836 |\n",
      "g_step: 6200 loss std/mean: 0.02451733499765396 0.2590271830558777\n",
      "| Loss std:   0.024517 |\n",
      "| Loss mean:   0.259027 |\n",
      "g_step: 6300 loss std/mean: 0.023381656035780907 0.26494985818862915\n",
      "| Loss std:   0.023382 |\n",
      "| Loss mean:   0.264950 |\n",
      "g_step: 6400 loss std/mean: 0.031147465109825134 0.2700635492801666\n",
      "| Loss std:   0.031147 |\n",
      "| Loss mean:   0.270064 |\n",
      "g_step: 6500 loss std/mean: 0.02572636865079403 0.26410725712776184\n",
      "| Loss std:   0.025726 |\n",
      "| Loss mean:   0.264107 |\n",
      "g_step: 6600 loss std/mean: 0.025254324078559875 0.26643216609954834\n",
      "| Loss std:   0.025254 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Loss mean:   0.266432 |\n",
      "g_step: 6700 loss std/mean: 0.027530472725629807 0.26132261753082275\n",
      "| Loss std:   0.027530 |\n",
      "| Loss mean:   0.261323 |\n",
      "g_step: 6800 loss std/mean: 0.02660738304257393 0.26949432492256165\n",
      "| Loss std:   0.026607 |\n",
      "| Loss mean:   0.269494 |\n",
      "g_step: 6900 loss std/mean: 0.027821796014904976 0.2603154182434082\n",
      "| Loss std:   0.027822 |\n",
      "| Loss mean:   0.260315 |\n",
      "g_step: 7000 loss std/mean: 0.027715658769011497 0.2733796238899231\n",
      "| Loss std:   0.027716 |\n",
      "| Loss mean:   0.273380 |\n",
      "g_step: 7100 loss std/mean: 0.02370106801390648 0.26285943388938904\n",
      "| Loss std:   0.023701 |\n",
      "| Loss mean:   0.262859 |\n",
      "g_step: 7200 loss std/mean: 0.03714468702673912 0.27416127920150757\n",
      "| Loss std:   0.037145 |\n",
      "| Loss mean:   0.274161 |\n",
      "g_step: 7300 loss std/mean: 0.03158896043896675 0.2616395950317383\n",
      "| Loss std:   0.031589 |\n",
      "| Loss mean:   0.261640 |\n",
      "g_step: 7400 loss std/mean: 0.026022374629974365 0.25453174114227295\n",
      "| Loss std:   0.026022 |\n",
      "| Loss mean:   0.254532 |\n",
      "g_step: 7500 loss std/mean: 0.025787314400076866 0.2611621618270874\n",
      "| Loss std:   0.025787 |\n",
      "| Loss mean:   0.261162 |\n",
      "g_step: 7600 loss std/mean: 0.023830369114875793 0.25976893305778503\n",
      "| Loss std:   0.023830 |\n",
      "| Loss mean:   0.259769 |\n",
      "g_step: 7700 loss std/mean: 0.03282526135444641 0.2657862901687622\n",
      "| Loss std:   0.032825 |\n",
      "| Loss mean:   0.265786 |\n",
      "g_step: 7800 loss std/mean: 0.022823316976428032 0.2589755952358246\n",
      "| Loss std:   0.022823 |\n",
      "| Loss mean:   0.258976 |\n",
      "g_step: 7900 loss std/mean: 0.024521201848983765 0.26950615644454956\n",
      "| Loss std:   0.024521 |\n",
      "| Loss mean:   0.269506 |\n",
      "g_step: 8000 loss std/mean: 0.02662096917629242 0.2616834044456482\n",
      "| Loss std:   0.026621 |\n",
      "| Loss mean:   0.261683 |\n",
      "g_step: 8100 loss std/mean: 0.02679458074271679 0.2673727869987488\n",
      "| Loss std:   0.026795 |\n",
      "| Loss mean:   0.267373 |\n",
      "g_step: 8200 loss std/mean: 0.025750378146767616 0.26771047711372375\n",
      "| Loss std:   0.025750 |\n",
      "| Loss mean:   0.267710 |\n",
      "g_step: 8300 loss std/mean: 0.024172240868210793 0.2599971294403076\n",
      "| Loss std:   0.024172 |\n",
      "| Loss mean:   0.259997 |\n",
      "g_step: 8400 loss std/mean: 0.025114765390753746 0.2640968859195709\n",
      "| Loss std:   0.025115 |\n",
      "| Loss mean:   0.264097 |\n",
      "g_step: 8500 loss std/mean: 0.02311653085052967 0.2624663710594177\n",
      "| Loss std:   0.023117 |\n",
      "| Loss mean:   0.262466 |\n",
      "g_step: 8600 loss std/mean: 0.023876650258898735 0.2593339681625366\n",
      "| Loss std:   0.023877 |\n",
      "| Loss mean:   0.259334 |\n",
      "g_step: 8700 loss std/mean: 0.025767387822270393 0.2613118290901184\n",
      "| Loss std:   0.025767 |\n",
      "| Loss mean:   0.261312 |\n",
      "g_step: 8800 loss std/mean: 0.028434256091713905 0.2607646584510803\n",
      "| Loss std:   0.028434 |\n",
      "| Loss mean:   0.260765 |\n",
      "g_step: 8900 loss std/mean: 0.018755828961730003 0.25952547788619995\n",
      "| Loss std:   0.018756 |\n",
      "| Loss mean:   0.259525 |\n",
      "g_step: 9000 loss std/mean: 0.02541210874915123 0.2660021483898163\n",
      "| Loss std:   0.025412 |\n",
      "| Loss mean:   0.266002 |\n",
      "g_step: 9100 loss std/mean: 0.02007262222468853 0.26139506697654724\n",
      "| Loss std:   0.020073 |\n",
      "| Loss mean:   0.261395 |\n",
      "g_step: 9200 loss std/mean: 0.027824632823467255 0.26359638571739197\n",
      "| Loss std:   0.027825 |\n",
      "| Loss mean:   0.263596 |\n",
      "g_step: 9300 loss std/mean: 0.027811916545033455 0.25985443592071533\n",
      "| Loss std:   0.027812 |\n",
      "| Loss mean:   0.259854 |\n",
      "g_step: 9400 loss std/mean: 0.0220952071249485 0.25888773798942566\n",
      "| Loss std:   0.022095 |\n",
      "| Loss mean:   0.258888 |\n",
      "g_step: 9500 loss std/mean: 0.027593735605478287 0.2622325122356415\n",
      "| Loss std:   0.027594 |\n",
      "| Loss mean:   0.262233 |\n",
      "g_step: 9600 loss std/mean: 0.02144615352153778 0.26237642765045166\n",
      "| Loss std:   0.021446 |\n",
      "| Loss mean:   0.262376 |\n",
      "g_step: 9700 loss std/mean: 0.024035507813096046 0.266002357006073\n",
      "| Loss std:   0.024036 |\n",
      "| Loss mean:   0.266002 |\n",
      "g_step: 9800 loss std/mean: 0.02773098275065422 0.26375603675842285\n",
      "| Loss std:   0.027731 |\n",
      "| Loss mean:   0.263756 |\n",
      "g_step: 9900 loss std/mean: 0.022055426612496376 0.25807297229766846\n",
      "| Loss std:   0.022055 |\n",
      "| Loss mean:   0.258073 |\n",
      "g_step: 10000 loss std/mean: 0.023792501538991928 0.2527221143245697\n",
      "| Loss std:   0.023793 |\n",
      "| Loss mean:   0.252722 |\n",
      "g_step: 10100 loss std/mean: 0.024692116305232048 0.26430901885032654\n",
      "| Loss std:   0.024692 |\n",
      "| Loss mean:   0.264309 |\n",
      "g_step: 10200 loss std/mean: 0.025289466604590416 0.2586001455783844\n",
      "| Loss std:   0.025289 |\n",
      "| Loss mean:   0.258600 |\n",
      "g_step: 10300 loss std/mean: 0.02336176484823227 0.25726398825645447\n",
      "| Loss std:   0.023362 |\n",
      "| Loss mean:   0.257264 |\n",
      "g_step: 10400 loss std/mean: 0.02664506435394287 0.25723838806152344\n",
      "| Loss std:   0.026645 |\n",
      "| Loss mean:   0.257238 |\n",
      "g_step: 10500 loss std/mean: 0.01898784004151821 0.26075974106788635\n",
      "| Loss std:   0.018988 |\n",
      "| Loss mean:   0.260760 |\n",
      "g_step: 10600 loss std/mean: 0.027967438101768494 0.25823187828063965\n",
      "| Loss std:   0.027967 |\n",
      "| Loss mean:   0.258232 |\n",
      "g_step: 10700 loss std/mean: 0.02053871378302574 0.26121529936790466\n",
      "| Loss std:   0.020539 |\n",
      "| Loss mean:   0.261215 |\n",
      "g_step: 10800 loss std/mean: 0.024185698479413986 0.259186327457428\n",
      "| Loss std:   0.024186 |\n",
      "| Loss mean:   0.259186 |\n",
      "g_step: 10900 loss std/mean: 0.023000666871666908 0.25544220209121704\n",
      "| Loss std:   0.023001 |\n",
      "| Loss mean:   0.255442 |\n",
      "g_step: 11000 loss std/mean: 0.01989104226231575 0.2642003297805786\n",
      "| Loss std:   0.019891 |\n",
      "| Loss mean:   0.264200 |\n",
      "g_step: 11100 loss std/mean: 0.023160066455602646 0.25908249616622925\n",
      "| Loss std:   0.023160 |\n",
      "| Loss mean:   0.259082 |\n",
      "g_step: 11200 loss std/mean: 0.022413281723856926 0.2630772292613983\n",
      "| Loss std:   0.022413 |\n",
      "| Loss mean:   0.263077 |\n",
      "g_step: 11300 loss std/mean: 0.02365235611796379 0.259863942861557\n",
      "| Loss std:   0.023652 |\n",
      "| Loss mean:   0.259864 |\n",
      "g_step: 11400 loss std/mean: 0.028315406292676926 0.26376768946647644\n",
      "| Loss std:   0.028315 |\n",
      "| Loss mean:   0.263768 |\n",
      "g_step: 11500 loss std/mean: 0.02223820611834526 0.2616734802722931\n",
      "| Loss std:   0.022238 |\n",
      "| Loss mean:   0.261673 |\n",
      "g_step: 11600 loss std/mean: 0.025244111195206642 0.2596271336078644\n",
      "| Loss std:   0.025244 |\n",
      "| Loss mean:   0.259627 |\n",
      "g_step: 11700 loss std/mean: 0.02641899883747101 0.2588607668876648\n",
      "| Loss std:   0.026419 |\n",
      "| Loss mean:   0.258861 |\n",
      "g_step: 11800 loss std/mean: 0.026665372774004936 0.2562476396560669\n",
      "| Loss std:   0.026665 |\n",
      "| Loss mean:   0.256248 |\n",
      "g_step: 11900 loss std/mean: 0.024607554078102112 0.25610655546188354\n",
      "| Loss std:   0.024608 |\n",
      "| Loss mean:   0.256107 |\n",
      "g_step: 12000 loss std/mean: 0.025637397542595863 0.26106250286102295\n",
      "| Loss std:   0.025637 |\n",
      "| Loss mean:   0.261063 |\n",
      "g_step: 12100 loss std/mean: 0.023645639419555664 0.26084059476852417\n",
      "| Loss std:   0.023646 |\n",
      "| Loss mean:   0.260841 |\n",
      "g_step: 12200 loss std/mean: 0.024445675313472748 0.2543144226074219\n",
      "| Loss std:   0.024446 |\n",
      "| Loss mean:   0.254314 |\n",
      "g_step: 12300 loss std/mean: 0.021817568689584732 0.259207159280777\n",
      "| Loss std:   0.021818 |\n",
      "| Loss mean:   0.259207 |\n",
      "g_step: 12400 loss std/mean: 0.026669632643461227 0.2637861967086792\n",
      "| Loss std:   0.026670 |\n",
      "| Loss mean:   0.263786 |\n",
      "g_step: 12500 loss std/mean: 0.026482833549380302 0.25262343883514404\n",
      "| Loss std:   0.026483 |\n",
      "| Loss mean:   0.252623 |\n",
      "g_step: 12600 loss std/mean: 0.022261353209614754 0.26390543580055237\n",
      "| Loss std:   0.022261 |\n",
      "| Loss mean:   0.263905 |\n",
      "g_step: 12700 loss std/mean: 0.022483322769403458 0.2624776363372803\n",
      "| Loss std:   0.022483 |\n",
      "| Loss mean:   0.262478 |\n",
      "g_step: 12800 loss std/mean: 0.025728102773427963 0.26140594482421875\n",
      "| Loss std:   0.025728 |\n",
      "| Loss mean:   0.261406 |\n",
      "g_step: 12900 loss std/mean: 0.024391334503889084 0.25576671957969666\n",
      "| Loss std:   0.024391 |\n",
      "| Loss mean:   0.255767 |\n",
      "g_step: 13000 loss std/mean: 0.01898067630827427 0.25631508231163025\n",
      "| Loss std:   0.018981 |\n",
      "| Loss mean:   0.256315 |\n",
      "g_step: 13100 loss std/mean: 0.020191356539726257 0.2568697929382324\n",
      "| Loss std:   0.020191 |\n",
      "| Loss mean:   0.256870 |\n",
      "g_step: 13200 loss std/mean: 0.029022719711065292 0.257691353559494\n",
      "| Loss std:   0.029023 |\n",
      "| Loss mean:   0.257691 |\n",
      "g_step: 13300 loss std/mean: 0.022895822301506996 0.2591976225376129\n",
      "| Loss std:   0.022896 |\n",
      "| Loss mean:   0.259198 |\n",
      "g_step: 13400 loss std/mean: 0.02330445684492588 0.2576479911804199\n",
      "| Loss std:   0.023304 |\n",
      "| Loss mean:   0.257648 |\n",
      "g_step: 13500 loss std/mean: 0.020429840311408043 0.2569653391838074\n",
      "| Loss std:   0.020430 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Loss mean:   0.256965 |\n",
      "g_step: 13600 loss std/mean: 0.02232358604669571 0.25543680787086487\n",
      "| Loss std:   0.022324 |\n",
      "| Loss mean:   0.255437 |\n",
      "g_step: 13700 loss std/mean: 0.023311911150813103 0.257748544216156\n",
      "| Loss std:   0.023312 |\n",
      "| Loss mean:   0.257749 |\n",
      "g_step: 13800 loss std/mean: 0.022356431931257248 0.2589523196220398\n",
      "| Loss std:   0.022356 |\n",
      "| Loss mean:   0.258952 |\n",
      "g_step: 13900 loss std/mean: 0.0242281723767519 0.2606947720050812\n",
      "| Loss std:   0.024228 |\n",
      "| Loss mean:   0.260695 |\n",
      "g_step: 14000 loss std/mean: 0.02422790788114071 0.25539523363113403\n",
      "| Loss std:   0.024228 |\n",
      "| Loss mean:   0.255395 |\n",
      "g_step: 14100 loss std/mean: 0.020991899073123932 0.25351396203041077\n",
      "| Loss std:   0.020992 |\n",
      "| Loss mean:   0.253514 |\n",
      "g_step: 14200 loss std/mean: 0.026657741516828537 0.25687530636787415\n",
      "| Loss std:   0.026658 |\n",
      "| Loss mean:   0.256875 |\n",
      "g_step: 14300 loss std/mean: 0.022753367200493813 0.253953754901886\n",
      "| Loss std:   0.022753 |\n",
      "| Loss mean:   0.253954 |\n",
      "g_step: 14400 loss std/mean: 0.018892604857683182 0.2539180517196655\n",
      "| Loss std:   0.018893 |\n",
      "| Loss mean:   0.253918 |\n",
      "g_step: 14500 loss std/mean: 0.02156713232398033 0.2551901042461395\n",
      "| Loss std:   0.021567 |\n",
      "| Loss mean:   0.255190 |\n",
      "g_step: 14600 loss std/mean: 0.02090929076075554 0.25684645771980286\n",
      "| Loss std:   0.020909 |\n",
      "| Loss mean:   0.256846 |\n",
      "g_step: 14700 loss std/mean: 0.02424425631761551 0.25465917587280273\n",
      "| Loss std:   0.024244 |\n",
      "| Loss mean:   0.254659 |\n",
      "g_step: 14800 loss std/mean: 0.021020011976361275 0.25411322712898254\n",
      "| Loss std:   0.021020 |\n",
      "| Loss mean:   0.254113 |\n",
      "g_step: 14900 loss std/mean: 0.024586327373981476 0.25457483530044556\n",
      "| Loss std:   0.024586 |\n",
      "| Loss mean:   0.254575 |\n",
      "g_step: 15000 loss std/mean: 0.01948917657136917 0.25187167525291443\n",
      "| Loss std:   0.019489 |\n",
      "| Loss mean:   0.251872 |\n",
      "g_step: 15100 loss std/mean: 0.025972429662942886 0.2541271150112152\n",
      "| Loss std:   0.025972 |\n",
      "| Loss mean:   0.254127 |\n",
      "g_step: 15200 loss std/mean: 0.023600228130817413 0.25150784850120544\n",
      "| Loss std:   0.023600 |\n",
      "| Loss mean:   0.251508 |\n",
      "g_step: 15300 loss std/mean: 0.024181680753827095 0.25705939531326294\n",
      "| Loss std:   0.024182 |\n",
      "| Loss mean:   0.257059 |\n",
      "g_step: 15400 loss std/mean: 0.021880649030208588 0.25752896070480347\n",
      "| Loss std:   0.021881 |\n",
      "| Loss mean:   0.257529 |\n",
      "g_step: 15500 loss std/mean: 0.021487662568688393 0.24957329034805298\n",
      "| Loss std:   0.021488 |\n",
      "| Loss mean:   0.249573 |\n",
      "g_step: 15600 loss std/mean: 0.024240784347057343 0.2567874789237976\n",
      "| Loss std:   0.024241 |\n",
      "| Loss mean:   0.256787 |\n",
      "g_step: 15700 loss std/mean: 0.020503023639321327 0.2525178790092468\n",
      "| Loss std:   0.020503 |\n",
      "| Loss mean:   0.252518 |\n",
      "g_step: 15800 loss std/mean: 0.0215904638171196 0.2579646110534668\n",
      "| Loss std:   0.021590 |\n",
      "| Loss mean:   0.257965 |\n",
      "g_step: 15900 loss std/mean: 0.021854734048247337 0.25387197732925415\n",
      "| Loss std:   0.021855 |\n",
      "| Loss mean:   0.253872 |\n",
      "g_step: 16000 loss std/mean: 0.020356474444270134 0.2579842507839203\n",
      "| Loss std:   0.020356 |\n",
      "| Loss mean:   0.257984 |\n",
      "g_step: 16100 loss std/mean: 0.024186331778764725 0.2548266649246216\n",
      "| Loss std:   0.024186 |\n",
      "| Loss mean:   0.254827 |\n",
      "g_step: 16200 loss std/mean: 0.021206390112638474 0.2537667155265808\n",
      "| Loss std:   0.021206 |\n",
      "| Loss mean:   0.253767 |\n",
      "g_step: 16300 loss std/mean: 0.0212560947984457 0.25452572107315063\n",
      "| Loss std:   0.021256 |\n",
      "| Loss mean:   0.254526 |\n",
      "g_step: 16400 loss std/mean: 0.02350464090704918 0.24943456053733826\n",
      "| Loss std:   0.023505 |\n",
      "| Loss mean:   0.249435 |\n",
      "g_step: 16500 loss std/mean: 0.022258715704083443 0.25553372502326965\n",
      "| Loss std:   0.022259 |\n",
      "| Loss mean:   0.255534 |\n",
      "g_step: 16600 loss std/mean: 0.020927755162119865 0.2605624198913574\n",
      "| Loss std:   0.020928 |\n",
      "| Loss mean:   0.260562 |\n",
      "g_step: 16700 loss std/mean: 0.026556165888905525 0.25392284989356995\n",
      "| Loss std:   0.026556 |\n",
      "| Loss mean:   0.253923 |\n",
      "g_step: 16800 loss std/mean: 0.023556027561426163 0.2556605339050293\n",
      "| Loss std:   0.023556 |\n",
      "| Loss mean:   0.255661 |\n",
      "g_step: 16900 loss std/mean: 0.022052882239222527 0.2493910789489746\n",
      "| Loss std:   0.022053 |\n",
      "| Loss mean:   0.249391 |\n",
      "g_step: 17000 loss std/mean: 0.02061229571700096 0.2549920380115509\n",
      "| Loss std:   0.020612 |\n",
      "| Loss mean:   0.254992 |\n",
      "g_step: 17100 loss std/mean: 0.020812200382351875 0.25525742769241333\n",
      "| Loss std:   0.020812 |\n",
      "| Loss mean:   0.255257 |\n",
      "g_step: 17200 loss std/mean: 0.02178547903895378 0.25518807768821716\n",
      "| Loss std:   0.021785 |\n",
      "| Loss mean:   0.255188 |\n",
      "g_step: 17300 loss std/mean: 0.02275862917304039 0.24980981647968292\n",
      "| Loss std:   0.022759 |\n",
      "| Loss mean:   0.249810 |\n",
      "g_step: 17400 loss std/mean: 0.02151789702475071 0.2514505684375763\n",
      "| Loss std:   0.021518 |\n",
      "| Loss mean:   0.251451 |\n",
      "g_step: 17500 loss std/mean: 0.021341359242796898 0.24915792047977448\n",
      "| Loss std:   0.021341 |\n",
      "| Loss mean:   0.249158 |\n",
      "g_step: 17600 loss std/mean: 0.02317560464143753 0.25587227940559387\n",
      "| Loss std:   0.023176 |\n",
      "| Loss mean:   0.255872 |\n",
      "g_step: 17700 loss std/mean: 0.02209748513996601 0.24757270514965057\n",
      "| Loss std:   0.022097 |\n",
      "| Loss mean:   0.247573 |\n",
      "g_step: 17800 loss std/mean: 0.02054351009428501 0.25606778264045715\n",
      "| Loss std:   0.020544 |\n",
      "| Loss mean:   0.256068 |\n",
      "g_step: 17900 loss std/mean: 0.017507458105683327 0.25597724318504333\n",
      "| Loss std:   0.017507 |\n",
      "| Loss mean:   0.255977 |\n",
      "g_step: 18000 loss std/mean: 0.022317368537187576 0.2508631646633148\n",
      "| Loss std:   0.022317 |\n",
      "| Loss mean:   0.250863 |\n",
      "g_step: 18100 loss std/mean: 0.02168511040508747 0.25246310234069824\n",
      "| Loss std:   0.021685 |\n",
      "| Loss mean:   0.252463 |\n",
      "g_step: 18200 loss std/mean: 0.019787948578596115 0.257496178150177\n",
      "| Loss std:   0.019788 |\n",
      "| Loss mean:   0.257496 |\n",
      "g_step: 18300 loss std/mean: 0.023796282708644867 0.2525644302368164\n",
      "| Loss std:   0.023796 |\n",
      "| Loss mean:   0.252564 |\n",
      "g_step: 18400 loss std/mean: 0.019492948427796364 0.25052687525749207\n",
      "| Loss std:   0.019493 |\n",
      "| Loss mean:   0.250527 |\n",
      "g_step: 18500 loss std/mean: 0.024764761328697205 0.2516326308250427\n",
      "| Loss std:   0.024765 |\n",
      "| Loss mean:   0.251633 |\n",
      "g_step: 18600 loss std/mean: 0.02098006010055542 0.2511328458786011\n",
      "| Loss std:   0.020980 |\n",
      "| Loss mean:   0.251133 |\n",
      "g_step: 18700 loss std/mean: 0.02335168793797493 0.25179338455200195\n",
      "| Loss std:   0.023352 |\n",
      "| Loss mean:   0.251793 |\n",
      "g_step: 18800 loss std/mean: 0.019351569935679436 0.2517431080341339\n",
      "| Loss std:   0.019352 |\n",
      "| Loss mean:   0.251743 |\n",
      "g_step: 18900 loss std/mean: 0.02582930400967598 0.2560468018054962\n",
      "| Loss std:   0.025829 |\n",
      "| Loss mean:   0.256047 |\n",
      "g_step: 19000 loss std/mean: 0.024424081668257713 0.2501644194126129\n",
      "| Loss std:   0.024424 |\n",
      "| Loss mean:   0.250164 |\n",
      "g_step: 19100 loss std/mean: 0.023976320400834084 0.25530779361724854\n",
      "| Loss std:   0.023976 |\n",
      "| Loss mean:   0.255308 |\n",
      "g_step: 19200 loss std/mean: 0.022219611331820488 0.2512267231941223\n",
      "| Loss std:   0.022220 |\n",
      "| Loss mean:   0.251227 |\n",
      "g_step: 19300 loss std/mean: 0.021189823746681213 0.2527877688407898\n",
      "| Loss std:   0.021190 |\n",
      "| Loss mean:   0.252788 |\n",
      "g_step: 19400 loss std/mean: 0.020428644493222237 0.25670334696769714\n",
      "| Loss std:   0.020429 |\n",
      "| Loss mean:   0.256703 |\n",
      "g_step: 19500 loss std/mean: 0.016326243057847023 0.2532360255718231\n",
      "| Loss std:   0.016326 |\n",
      "| Loss mean:   0.253236 |\n",
      "g_step: 19600 loss std/mean: 0.020204251632094383 0.2505764067173004\n",
      "| Loss std:   0.020204 |\n",
      "| Loss mean:   0.250576 |\n",
      "g_step: 19700 loss std/mean: 0.01923270896077156 0.24861083924770355\n",
      "| Loss std:   0.019233 |\n",
      "| Loss mean:   0.248611 |\n",
      "g_step: 19800 loss std/mean: 0.023756902664899826 0.2561332583427429\n",
      "| Loss std:   0.023757 |\n",
      "| Loss mean:   0.256133 |\n",
      "g_step: 19900 loss std/mean: 0.022493330761790276 0.24909277260303497\n",
      "| Loss std:   0.022493 |\n",
      "| Loss mean:   0.249093 |\n",
      "g_step: 20000 loss std/mean: 0.02331710048019886 0.24959434568881989\n",
      "| Loss std:   0.023317 |\n",
      "| Loss mean:   0.249594 |\n",
      "g_step: 20100 loss std/mean: 0.021120278164744377 0.25416821241378784\n",
      "| Loss std:   0.021120 |\n",
      "| Loss mean:   0.254168 |\n",
      "g_step: 20200 loss std/mean: 0.02018889971077442 0.24979393184185028\n",
      "| Loss std:   0.020189 |\n",
      "| Loss mean:   0.249794 |\n",
      "g_step: 20300 loss std/mean: 0.024045009166002274 0.2534596025943756\n",
      "| Loss std:   0.024045 |\n",
      "| Loss mean:   0.253460 |\n",
      "g_step: 20400 loss std/mean: 0.019403578713536263 0.25126349925994873\n",
      "| Loss std:   0.019404 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Loss mean:   0.251263 |\n",
      "g_step: 20500 loss std/mean: 0.023093726485967636 0.2480752021074295\n",
      "| Loss std:   0.023094 |\n",
      "| Loss mean:   0.248075 |\n",
      "g_step: 20600 loss std/mean: 0.022907646372914314 0.25124019384384155\n",
      "| Loss std:   0.022908 |\n",
      "| Loss mean:   0.251240 |\n",
      "g_step: 20700 loss std/mean: 0.022188808768987656 0.24917249381542206\n",
      "| Loss std:   0.022189 |\n",
      "| Loss mean:   0.249172 |\n",
      "g_step: 20800 loss std/mean: 0.017988527193665504 0.2486860454082489\n",
      "| Loss std:   0.017989 |\n",
      "| Loss mean:   0.248686 |\n",
      "g_step: 20900 loss std/mean: 0.01767357625067234 0.2527101933956146\n",
      "| Loss std:   0.017674 |\n",
      "| Loss mean:   0.252710 |\n",
      "g_step: 21000 loss std/mean: 0.02018549107015133 0.2507101893424988\n",
      "| Loss std:   0.020185 |\n",
      "| Loss mean:   0.250710 |\n",
      "g_step: 21100 loss std/mean: 0.020849797874689102 0.24935390055179596\n",
      "| Loss std:   0.020850 |\n",
      "| Loss mean:   0.249354 |\n",
      "g_step: 21200 loss std/mean: 0.02446189522743225 0.25001242756843567\n",
      "| Loss std:   0.024462 |\n",
      "| Loss mean:   0.250012 |\n",
      "g_step: 21300 loss std/mean: 0.02144051529467106 0.2551775872707367\n",
      "| Loss std:   0.021441 |\n",
      "| Loss mean:   0.255178 |\n",
      "g_step: 21400 loss std/mean: 0.02461504004895687 0.24474799633026123\n",
      "| Loss std:   0.024615 |\n",
      "| Loss mean:   0.244748 |\n",
      "g_step: 21500 loss std/mean: 0.024804744869470596 0.2470855712890625\n",
      "| Loss std:   0.024805 |\n",
      "| Loss mean:   0.247086 |\n",
      "g_step: 21600 loss std/mean: 0.02609182894229889 0.250759482383728\n",
      "| Loss std:   0.026092 |\n",
      "| Loss mean:   0.250759 |\n",
      "g_step: 21700 loss std/mean: 0.020731501281261444 0.2516501843929291\n",
      "| Loss std:   0.020732 |\n",
      "| Loss mean:   0.251650 |\n",
      "g_step: 21800 loss std/mean: 0.01802995428442955 0.25220227241516113\n",
      "| Loss std:   0.018030 |\n",
      "| Loss mean:   0.252202 |\n",
      "g_step: 21900 loss std/mean: 0.01941842772066593 0.24807706475257874\n",
      "| Loss std:   0.019418 |\n",
      "| Loss mean:   0.248077 |\n",
      "g_step: 22000 loss std/mean: 0.02084488607943058 0.24564361572265625\n",
      "| Loss std:   0.020845 |\n",
      "| Loss mean:   0.245644 |\n",
      "g_step: 22100 loss std/mean: 0.0209440179169178 0.24104005098342896\n",
      "| Loss std:   0.020944 |\n",
      "| Loss mean:   0.241040 |\n",
      "g_step: 22200 loss std/mean: 0.02041342295706272 0.24786607921123505\n",
      "| Loss std:   0.020413 |\n",
      "| Loss mean:   0.247866 |\n",
      "g_step: 22300 loss std/mean: 0.023268871009349823 0.24707575142383575\n",
      "| Loss std:   0.023269 |\n",
      "| Loss mean:   0.247076 |\n",
      "g_step: 22400 loss std/mean: 0.017831534147262573 0.2516811192035675\n",
      "| Loss std:   0.017832 |\n",
      "| Loss mean:   0.251681 |\n",
      "g_step: 22500 loss std/mean: 0.01943829469382763 0.24571581184864044\n",
      "| Loss std:   0.019438 |\n",
      "| Loss mean:   0.245716 |\n",
      "g_step: 22600 loss std/mean: 0.019289083778858185 0.24775493144989014\n",
      "| Loss std:   0.019289 |\n",
      "| Loss mean:   0.247755 |\n",
      "g_step: 22700 loss std/mean: 0.024068836122751236 0.2468661665916443\n",
      "| Loss std:   0.024069 |\n",
      "| Loss mean:   0.246866 |\n",
      "g_step: 22800 loss std/mean: 0.018989674746990204 0.2508031129837036\n",
      "| Loss std:   0.018990 |\n",
      "| Loss mean:   0.250803 |\n",
      "g_step: 22900 loss std/mean: 0.024247772991657257 0.24963876605033875\n",
      "| Loss std:   0.024248 |\n",
      "| Loss mean:   0.249639 |\n",
      "g_step: 23000 loss std/mean: 0.022887513041496277 0.2545725107192993\n",
      "| Loss std:   0.022888 |\n",
      "| Loss mean:   0.254573 |\n",
      "g_step: 23100 loss std/mean: 0.02341397851705551 0.25188446044921875\n",
      "| Loss std:   0.023414 |\n",
      "| Loss mean:   0.251884 |\n",
      "g_step: 23200 loss std/mean: 0.022121062502264977 0.24743354320526123\n",
      "| Loss std:   0.022121 |\n",
      "| Loss mean:   0.247434 |\n",
      "g_step: 23300 loss std/mean: 0.021093983203172684 0.2505238652229309\n",
      "| Loss std:   0.021094 |\n",
      "| Loss mean:   0.250524 |\n",
      "g_step: 23400 loss std/mean: 0.020807791501283646 0.24869109690189362\n",
      "| Loss std:   0.020808 |\n",
      "| Loss mean:   0.248691 |\n",
      "g_step: 23500 loss std/mean: 0.02067713625729084 0.25327348709106445\n",
      "| Loss std:   0.020677 |\n",
      "| Loss mean:   0.253273 |\n",
      "g_step: 23600 loss std/mean: 0.021189941093325615 0.25040560960769653\n",
      "| Loss std:   0.021190 |\n",
      "| Loss mean:   0.250406 |\n",
      "g_step: 23700 loss std/mean: 0.027100300416350365 0.24556148052215576\n",
      "| Loss std:   0.027100 |\n",
      "| Loss mean:   0.245561 |\n",
      "g_step: 23800 loss std/mean: 0.022938862442970276 0.24521587789058685\n",
      "| Loss std:   0.022939 |\n",
      "| Loss mean:   0.245216 |\n",
      "g_step: 23900 loss std/mean: 0.024008387699723244 0.246963232755661\n",
      "| Loss std:   0.024008 |\n",
      "| Loss mean:   0.246963 |\n",
      "g_step: 24000 loss std/mean: 0.025417668744921684 0.2493114024400711\n",
      "| Loss std:   0.025418 |\n",
      "| Loss mean:   0.249311 |\n",
      "g_step: 24100 loss std/mean: 0.027763070538640022 0.24208272993564606\n",
      "| Loss std:   0.027763 |\n",
      "| Loss mean:   0.242083 |\n",
      "g_step: 24200 loss std/mean: 0.02069663628935814 0.2519604563713074\n",
      "| Loss std:   0.020697 |\n",
      "| Loss mean:   0.251960 |\n",
      "g_step: 24300 loss std/mean: 0.02022598125040531 0.24838608503341675\n",
      "| Loss std:   0.020226 |\n",
      "| Loss mean:   0.248386 |\n",
      "g_step: 24400 loss std/mean: 0.019901681691408157 0.2490212619304657\n",
      "| Loss std:   0.019902 |\n",
      "| Loss mean:   0.249021 |\n",
      "g_step: 24500 loss std/mean: 0.02291034907102585 0.24797819554805756\n",
      "| Loss std:   0.022910 |\n",
      "| Loss mean:   0.247978 |\n",
      "g_step: 24600 loss std/mean: 0.020807970315217972 0.24459829926490784\n",
      "| Loss std:   0.020808 |\n",
      "| Loss mean:   0.244598 |\n",
      "g_step: 24700 loss std/mean: 0.02509979158639908 0.25021252036094666\n",
      "| Loss std:   0.025100 |\n",
      "| Loss mean:   0.250213 |\n",
      "g_step: 24800 loss std/mean: 0.020570944994688034 0.24260371923446655\n",
      "| Loss std:   0.020571 |\n",
      "| Loss mean:   0.242604 |\n",
      "g_step: 24900 loss std/mean: 0.021991221234202385 0.2517737150192261\n",
      "| Loss std:   0.021991 |\n",
      "| Loss mean:   0.251774 |\n",
      "g_step: 25000 loss std/mean: 0.02178533934056759 0.24606430530548096\n",
      "| Loss std:   0.021785 |\n",
      "| Loss mean:   0.246064 |\n",
      "g_step: 25100 loss std/mean: 0.023457854986190796 0.25043535232543945\n",
      "| Loss std:   0.023458 |\n",
      "| Loss mean:   0.250435 |\n",
      "g_step: 25200 loss std/mean: 0.021018991246819496 0.2499057948589325\n",
      "| Loss std:   0.021019 |\n",
      "| Loss mean:   0.249906 |\n",
      "g_step: 25300 loss std/mean: 0.018747631460428238 0.24953287839889526\n",
      "| Loss std:   0.018748 |\n",
      "| Loss mean:   0.249533 |\n",
      "g_step: 25400 loss std/mean: 0.018179025501012802 0.24543167650699615\n",
      "| Loss std:   0.018179 |\n",
      "| Loss mean:   0.245432 |\n",
      "g_step: 25500 loss std/mean: 0.019483784213662148 0.24815577268600464\n",
      "| Loss std:   0.019484 |\n",
      "| Loss mean:   0.248156 |\n",
      "g_step: 25600 loss std/mean: 0.02137008123099804 0.25090739130973816\n",
      "| Loss std:   0.021370 |\n",
      "| Loss mean:   0.250907 |\n",
      "g_step: 25700 loss std/mean: 0.021785054355859756 0.24895504117012024\n",
      "| Loss std:   0.021785 |\n",
      "| Loss mean:   0.248955 |\n",
      "g_step: 25800 loss std/mean: 0.022992074489593506 0.24665166437625885\n",
      "| Loss std:   0.022992 |\n",
      "| Loss mean:   0.246652 |\n",
      "g_step: 25900 loss std/mean: 0.019413746893405914 0.25390374660491943\n",
      "| Loss std:   0.019414 |\n",
      "| Loss mean:   0.253904 |\n",
      "g_step: 26000 loss std/mean: 0.020045774057507515 0.24535617232322693\n",
      "| Loss std:   0.020046 |\n",
      "| Loss mean:   0.245356 |\n",
      "g_step: 26100 loss std/mean: 0.024651067331433296 0.24582065641880035\n",
      "| Loss std:   0.024651 |\n",
      "| Loss mean:   0.245821 |\n",
      "g_step: 26200 loss std/mean: 0.02088179625570774 0.24920761585235596\n",
      "| Loss std:   0.020882 |\n",
      "| Loss mean:   0.249208 |\n",
      "g_step: 26300 loss std/mean: 0.02165694162249565 0.248677060008049\n",
      "| Loss std:   0.021657 |\n",
      "| Loss mean:   0.248677 |\n",
      "g_step: 26400 loss std/mean: 0.020777784287929535 0.2479042410850525\n",
      "| Loss std:   0.020778 |\n",
      "| Loss mean:   0.247904 |\n",
      "g_step: 26500 loss std/mean: 0.021198786795139313 0.24793070554733276\n",
      "| Loss std:   0.021199 |\n",
      "| Loss mean:   0.247931 |\n",
      "g_step: 26600 loss std/mean: 0.021202178671956062 0.24981272220611572\n",
      "| Loss std:   0.021202 |\n",
      "| Loss mean:   0.249813 |\n",
      "g_step: 26700 loss std/mean: 0.02037668228149414 0.248824805021286\n",
      "| Loss std:   0.020377 |\n",
      "| Loss mean:   0.248825 |\n",
      "g_step: 26800 loss std/mean: 0.02195485308766365 0.24686089158058167\n",
      "| Loss std:   0.021955 |\n",
      "| Loss mean:   0.246861 |\n",
      "g_step: 26900 loss std/mean: 0.01818845048546791 0.2514958083629608\n",
      "| Loss std:   0.018188 |\n",
      "| Loss mean:   0.251496 |\n",
      "g_step: 27000 loss std/mean: 0.020939728245139122 0.24519874155521393\n",
      "| Loss std:   0.020940 |\n",
      "| Loss mean:   0.245199 |\n",
      "g_step: 27100 loss std/mean: 0.01721838116645813 0.24946831166744232\n",
      "| Loss std:   0.017218 |\n",
      "| Loss mean:   0.249468 |\n",
      "g_step: 27200 loss std/mean: 0.02222575433552265 0.2483130693435669\n",
      "| Loss std:   0.022226 |\n",
      "| Loss mean:   0.248313 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g_step: 27300 loss std/mean: 0.021013258025050163 0.24996507167816162\n",
      "| Loss std:   0.021013 |\n",
      "| Loss mean:   0.249965 |\n",
      "g_step: 27400 loss std/mean: 0.02095433697104454 0.24359552562236786\n",
      "| Loss std:   0.020954 |\n",
      "| Loss mean:   0.243596 |\n",
      "g_step: 27500 loss std/mean: 0.02079959586262703 0.25124824047088623\n",
      "| Loss std:   0.020800 |\n",
      "| Loss mean:   0.251248 |\n",
      "g_step: 27600 loss std/mean: 0.020838940516114235 0.2471330612897873\n",
      "| Loss std:   0.020839 |\n",
      "| Loss mean:   0.247133 |\n",
      "g_step: 27700 loss std/mean: 0.020577194169163704 0.24371787905693054\n",
      "| Loss std:   0.020577 |\n",
      "| Loss mean:   0.243718 |\n",
      "g_step: 27800 loss std/mean: 0.023074695840477943 0.24859628081321716\n",
      "| Loss std:   0.023075 |\n",
      "| Loss mean:   0.248596 |\n",
      "g_step: 27900 loss std/mean: 0.020735761150717735 0.24590785801410675\n",
      "| Loss std:   0.020736 |\n",
      "| Loss mean:   0.245908 |\n",
      "g_step: 28000 loss std/mean: 0.020948484539985657 0.2505885064601898\n",
      "| Loss std:   0.020948 |\n",
      "| Loss mean:   0.250589 |\n",
      "g_step: 28100 loss std/mean: 0.020582789555191994 0.24454940855503082\n",
      "| Loss std:   0.020583 |\n",
      "| Loss mean:   0.244549 |\n",
      "g_step: 28200 loss std/mean: 0.020959574729204178 0.24915815889835358\n",
      "| Loss std:   0.020960 |\n",
      "| Loss mean:   0.249158 |\n",
      "g_step: 28300 loss std/mean: 0.01727740466594696 0.2512875497341156\n",
      "| Loss std:   0.017277 |\n",
      "| Loss mean:   0.251288 |\n",
      "g_step: 28400 loss std/mean: 0.022868426516652107 0.2450144737958908\n",
      "| Loss std:   0.022868 |\n",
      "| Loss mean:   0.245014 |\n",
      "g_step: 28500 loss std/mean: 0.021914588287472725 0.2458583414554596\n",
      "| Loss std:   0.021915 |\n",
      "| Loss mean:   0.245858 |\n",
      "g_step: 28600 loss std/mean: 0.021092530339956284 0.24977807700634003\n",
      "| Loss std:   0.021093 |\n",
      "| Loss mean:   0.249778 |\n",
      "g_step: 28700 loss std/mean: 0.022034645080566406 0.24308955669403076\n",
      "| Loss std:   0.022035 |\n",
      "| Loss mean:   0.243090 |\n",
      "g_step: 28800 loss std/mean: 0.01935342513024807 0.24834181368350983\n",
      "| Loss std:   0.019353 |\n",
      "| Loss mean:   0.248342 |\n",
      "g_step: 28900 loss std/mean: 0.018035050481557846 0.2507054805755615\n",
      "| Loss std:   0.018035 |\n",
      "| Loss mean:   0.250705 |\n",
      "g_step: 29000 loss std/mean: 0.020085854455828667 0.24907641112804413\n",
      "| Loss std:   0.020086 |\n",
      "| Loss mean:   0.249076 |\n",
      "g_step: 29100 loss std/mean: 0.022170547395944595 0.2479838877916336\n",
      "| Loss std:   0.022171 |\n",
      "| Loss mean:   0.247984 |\n",
      "g_step: 29200 loss std/mean: 0.018495038151741028 0.24670682847499847\n",
      "| Loss std:   0.018495 |\n",
      "| Loss mean:   0.246707 |\n",
      "g_step: 29300 loss std/mean: 0.02259141393005848 0.2486899346113205\n",
      "| Loss std:   0.022591 |\n",
      "| Loss mean:   0.248690 |\n",
      "g_step: 29400 loss std/mean: 0.02156410925090313 0.24477583169937134\n",
      "| Loss std:   0.021564 |\n",
      "| Loss mean:   0.244776 |\n",
      "g_step: 29500 loss std/mean: 0.019818896427750587 0.24718058109283447\n",
      "| Loss std:   0.019819 |\n",
      "| Loss mean:   0.247181 |\n",
      "g_step: 29600 loss std/mean: 0.019649788737297058 0.24655303359031677\n",
      "| Loss std:   0.019650 |\n",
      "| Loss mean:   0.246553 |\n",
      "g_step: 29700 loss std/mean: 0.019198819994926453 0.2499723881483078\n",
      "| Loss std:   0.019199 |\n",
      "| Loss mean:   0.249972 |\n",
      "g_step: 29800 loss std/mean: 0.02120259217917919 0.24346093833446503\n",
      "| Loss std:   0.021203 |\n",
      "| Loss mean:   0.243461 |\n",
      "g_step: 29900 loss std/mean: 0.019832149147987366 0.24624864757061005\n",
      "| Loss std:   0.019832 |\n",
      "| Loss mean:   0.246249 |\n",
      "g_step: 30000 loss std/mean: 0.02531891129910946 0.24585600197315216\n",
      "| Loss std:   0.025319 |\n",
      "| Loss mean:   0.245856 |\n",
      "g_step: 30100 loss std/mean: 0.020490353927016258 0.24615222215652466\n",
      "| Loss std:   0.020490 |\n",
      "| Loss mean:   0.246152 |\n",
      "g_step: 30200 loss std/mean: 0.023800192400813103 0.248982772231102\n",
      "| Loss std:   0.023800 |\n",
      "| Loss mean:   0.248983 |\n",
      "g_step: 30300 loss std/mean: 0.021484969183802605 0.2431430071592331\n",
      "| Loss std:   0.021485 |\n",
      "| Loss mean:   0.243143 |\n",
      "g_step: 30400 loss std/mean: 0.023599773645401 0.2446320503950119\n",
      "| Loss std:   0.023600 |\n",
      "| Loss mean:   0.244632 |\n",
      "g_step: 30500 loss std/mean: 0.01727415807545185 0.24416451156139374\n",
      "| Loss std:   0.017274 |\n",
      "| Loss mean:   0.244165 |\n",
      "g_step: 30600 loss std/mean: 0.017569782212376595 0.2504000663757324\n",
      "| Loss std:   0.017570 |\n",
      "| Loss mean:   0.250400 |\n",
      "g_step: 30700 loss std/mean: 0.0185787882655859 0.24671675264835358\n",
      "| Loss std:   0.018579 |\n",
      "| Loss mean:   0.246717 |\n",
      "g_step: 30800 loss std/mean: 0.020717894658446312 0.24236378073692322\n",
      "| Loss std:   0.020718 |\n",
      "| Loss mean:   0.242364 |\n",
      "g_step: 30900 loss std/mean: 0.018690355122089386 0.24170829355716705\n",
      "| Loss std:   0.018690 |\n",
      "| Loss mean:   0.241708 |\n",
      "g_step: 31000 loss std/mean: 0.020355695858597755 0.24742549657821655\n",
      "| Loss std:   0.020356 |\n",
      "| Loss mean:   0.247425 |\n",
      "g_step: 31100 loss std/mean: 0.022240962833166122 0.24213725328445435\n",
      "| Loss std:   0.022241 |\n",
      "| Loss mean:   0.242137 |\n",
      "g_step: 31200 loss std/mean: 0.019135070964694023 0.24388660490512848\n",
      "| Loss std:   0.019135 |\n",
      "| Loss mean:   0.243887 |\n",
      "g_step: 31300 loss std/mean: 0.018410807475447655 0.2469182163476944\n",
      "| Loss std:   0.018411 |\n",
      "| Loss mean:   0.246918 |\n",
      "g_step: 31400 loss std/mean: 0.022097043693065643 0.24939405918121338\n",
      "| Loss std:   0.022097 |\n",
      "| Loss mean:   0.249394 |\n",
      "g_step: 31500 loss std/mean: 0.018933109939098358 0.24642080068588257\n",
      "| Loss std:   0.018933 |\n",
      "| Loss mean:   0.246421 |\n",
      "g_step: 31600 loss std/mean: 0.021740203723311424 0.24734897911548615\n",
      "| Loss std:   0.021740 |\n",
      "| Loss mean:   0.247349 |\n",
      "g_step: 31700 loss std/mean: 0.02323634922504425 0.24628108739852905\n",
      "| Loss std:   0.023236 |\n",
      "| Loss mean:   0.246281 |\n",
      "g_step: 31800 loss std/mean: 0.02111908793449402 0.2405625581741333\n",
      "| Loss std:   0.021119 |\n",
      "| Loss mean:   0.240563 |\n",
      "g_step: 31900 loss std/mean: 0.020714951679110527 0.2478421926498413\n",
      "| Loss std:   0.020715 |\n",
      "| Loss mean:   0.247842 |\n",
      "g_step: 32000 loss std/mean: 0.01970662549138069 0.24637405574321747\n",
      "| Loss std:   0.019707 |\n",
      "| Loss mean:   0.246374 |\n",
      "g_step: 32100 loss std/mean: 0.024148866534233093 0.24392709136009216\n",
      "| Loss std:   0.024149 |\n",
      "| Loss mean:   0.243927 |\n",
      "g_step: 32200 loss std/mean: 0.018923595547676086 0.24854923784732819\n",
      "| Loss std:   0.018924 |\n",
      "| Loss mean:   0.248549 |\n",
      "g_step: 32300 loss std/mean: 0.024338193237781525 0.23942077159881592\n",
      "| Loss std:   0.024338 |\n",
      "| Loss mean:   0.239421 |\n",
      "g_step: 32400 loss std/mean: 0.020202448591589928 0.24782700836658478\n",
      "| Loss std:   0.020202 |\n",
      "| Loss mean:   0.247827 |\n",
      "g_step: 32500 loss std/mean: 0.021225931122899055 0.24814189970493317\n",
      "| Loss std:   0.021226 |\n",
      "| Loss mean:   0.248142 |\n",
      "g_step: 32600 loss std/mean: 0.021671254187822342 0.2452004998922348\n",
      "| Loss std:   0.021671 |\n",
      "| Loss mean:   0.245200 |\n",
      "g_step: 32700 loss std/mean: 0.019496727734804153 0.24740295112133026\n",
      "| Loss std:   0.019497 |\n",
      "| Loss mean:   0.247403 |\n",
      "g_step: 32800 loss std/mean: 0.018003668636083603 0.24066446721553802\n",
      "| Loss std:   0.018004 |\n",
      "| Loss mean:   0.240664 |\n",
      "g_step: 32900 loss std/mean: 0.022563543170690536 0.24802103638648987\n",
      "| Loss std:   0.022564 |\n",
      "| Loss mean:   0.248021 |\n",
      "g_step: 33000 loss std/mean: 0.02145138569176197 0.24830715358257294\n",
      "| Loss std:   0.021451 |\n",
      "| Loss mean:   0.248307 |\n",
      "g_step: 33100 loss std/mean: 0.01977117359638214 0.24365173280239105\n",
      "| Loss std:   0.019771 |\n",
      "| Loss mean:   0.243652 |\n",
      "g_step: 33200 loss std/mean: 0.01780550181865692 0.24572138488292694\n",
      "| Loss std:   0.017806 |\n",
      "| Loss mean:   0.245721 |\n",
      "g_step: 33300 loss std/mean: 0.018939945846796036 0.24599622189998627\n",
      "| Loss std:   0.018940 |\n",
      "| Loss mean:   0.245996 |\n",
      "g_step: 33400 loss std/mean: 0.023403745144605637 0.24525071680545807\n",
      "| Loss std:   0.023404 |\n",
      "| Loss mean:   0.245251 |\n",
      "g_step: 33500 loss std/mean: 0.018095986917614937 0.24941124022006989\n",
      "| Loss std:   0.018096 |\n",
      "| Loss mean:   0.249411 |\n",
      "g_step: 33600 loss std/mean: 0.019229944795370102 0.24899010360240936\n",
      "| Loss std:   0.019230 |\n",
      "| Loss mean:   0.248990 |\n",
      "g_step: 33700 loss std/mean: 0.018348045647144318 0.24772076308727264\n",
      "| Loss std:   0.018348 |\n",
      "| Loss mean:   0.247721 |\n",
      "g_step: 33800 loss std/mean: 0.020002903416752815 0.24606743454933167\n",
      "| Loss std:   0.020003 |\n",
      "| Loss mean:   0.246067 |\n",
      "g_step: 33900 loss std/mean: 0.01857963390648365 0.2443198263645172\n",
      "| Loss std:   0.018580 |\n",
      "| Loss mean:   0.244320 |\n",
      "g_step: 34000 loss std/mean: 0.01817166619002819 0.24499300122261047\n",
      "| Loss std:   0.018172 |\n",
      "| Loss mean:   0.244993 |\n",
      "g_step: 34100 loss std/mean: 0.022827554494142532 0.24382373690605164\n",
      "| Loss std:   0.022828 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Loss mean:   0.243824 |\n",
      "g_step: 34200 loss std/mean: 0.020888950675725937 0.2532919943332672\n",
      "| Loss std:   0.020889 |\n",
      "| Loss mean:   0.253292 |\n",
      "g_step: 34300 loss std/mean: 0.0200667642056942 0.2490285485982895\n",
      "| Loss std:   0.020067 |\n",
      "| Loss mean:   0.249029 |\n",
      "g_step: 34400 loss std/mean: 0.01934339664876461 0.24307331442832947\n",
      "| Loss std:   0.019343 |\n",
      "| Loss mean:   0.243073 |\n",
      "g_step: 34500 loss std/mean: 0.02110474556684494 0.24453094601631165\n",
      "| Loss std:   0.021105 |\n",
      "| Loss mean:   0.244531 |\n",
      "g_step: 34600 loss std/mean: 0.01921342872083187 0.243904247879982\n",
      "| Loss std:   0.019213 |\n",
      "| Loss mean:   0.243904 |\n",
      "g_step: 34700 loss std/mean: 0.02123941108584404 0.24790643155574799\n",
      "| Loss std:   0.021239 |\n",
      "| Loss mean:   0.247906 |\n",
      "g_step: 34800 loss std/mean: 0.023813076317310333 0.2426929920911789\n",
      "| Loss std:   0.023813 |\n",
      "| Loss mean:   0.242693 |\n",
      "g_step: 34900 loss std/mean: 0.020169490948319435 0.24918556213378906\n",
      "| Loss std:   0.020169 |\n",
      "| Loss mean:   0.249186 |\n",
      "g_step: 35000 loss std/mean: 0.019840696826577187 0.24958060681819916\n",
      "| Loss std:   0.019841 |\n",
      "| Loss mean:   0.249581 |\n",
      "g_step: 35100 loss std/mean: 0.021975871175527573 0.24183285236358643\n",
      "| Loss std:   0.021976 |\n",
      "| Loss mean:   0.241833 |\n",
      "g_step: 35200 loss std/mean: 0.019118426367640495 0.24700169265270233\n",
      "| Loss std:   0.019118 |\n",
      "| Loss mean:   0.247002 |\n",
      "g_step: 35300 loss std/mean: 0.018794164061546326 0.24291680753231049\n",
      "| Loss std:   0.018794 |\n",
      "| Loss mean:   0.242917 |\n",
      "g_step: 35400 loss std/mean: 0.019638868048787117 0.2448628842830658\n",
      "| Loss std:   0.019639 |\n",
      "| Loss mean:   0.244863 |\n",
      "g_step: 35500 loss std/mean: 0.020645493641495705 0.24478167295455933\n",
      "| Loss std:   0.020645 |\n",
      "| Loss mean:   0.244782 |\n",
      "g_step: 35600 loss std/mean: 0.018364617601037025 0.2458418309688568\n",
      "| Loss std:   0.018365 |\n",
      "| Loss mean:   0.245842 |\n",
      "g_step: 35700 loss std/mean: 0.01795249804854393 0.24181097745895386\n",
      "| Loss std:   0.017952 |\n",
      "| Loss mean:   0.241811 |\n",
      "g_step: 35800 loss std/mean: 0.01864885911345482 0.24656347930431366\n",
      "| Loss std:   0.018649 |\n",
      "| Loss mean:   0.246563 |\n",
      "g_step: 35900 loss std/mean: 0.022073041647672653 0.2449594885110855\n",
      "| Loss std:   0.022073 |\n",
      "| Loss mean:   0.244959 |\n",
      "g_step: 36000 loss std/mean: 0.01871238648891449 0.24453392624855042\n",
      "| Loss std:   0.018712 |\n",
      "| Loss mean:   0.244534 |\n",
      "g_step: 36100 loss std/mean: 0.021016361191868782 0.24082933366298676\n",
      "| Loss std:   0.021016 |\n",
      "| Loss mean:   0.240829 |\n",
      "g_step: 36200 loss std/mean: 0.01818387396633625 0.2428230494260788\n",
      "| Loss std:   0.018184 |\n",
      "| Loss mean:   0.242823 |\n",
      "g_step: 36300 loss std/mean: 0.018183477222919464 0.24239975214004517\n",
      "| Loss std:   0.018183 |\n",
      "| Loss mean:   0.242400 |\n",
      "g_step: 36400 loss std/mean: 0.019588159397244453 0.24294255673885345\n",
      "| Loss std:   0.019588 |\n",
      "| Loss mean:   0.242943 |\n",
      "g_step: 36500 loss std/mean: 0.020121587440371513 0.24448920786380768\n",
      "| Loss std:   0.020122 |\n",
      "| Loss mean:   0.244489 |\n",
      "g_step: 36600 loss std/mean: 0.01995849609375 0.24772468209266663\n",
      "| Loss std:   0.019958 |\n",
      "| Loss mean:   0.247725 |\n",
      "g_step: 36700 loss std/mean: 0.021774206310510635 0.24692735075950623\n",
      "| Loss std:   0.021774 |\n",
      "| Loss mean:   0.246927 |\n",
      "g_step: 36800 loss std/mean: 0.01830085925757885 0.24862001836299896\n",
      "| Loss std:   0.018301 |\n",
      "| Loss mean:   0.248620 |\n",
      "g_step: 36900 loss std/mean: 0.02469491772353649 0.24280434846878052\n",
      "| Loss std:   0.024695 |\n",
      "| Loss mean:   0.242804 |\n",
      "g_step: 37000 loss std/mean: 0.021843664348125458 0.24297432601451874\n",
      "| Loss std:   0.021844 |\n",
      "| Loss mean:   0.242974 |\n",
      "g_step: 37100 loss std/mean: 0.019048133864998817 0.24273420870304108\n",
      "| Loss std:   0.019048 |\n",
      "| Loss mean:   0.242734 |\n",
      "g_step: 37200 loss std/mean: 0.019918769598007202 0.24576818943023682\n",
      "| Loss std:   0.019919 |\n",
      "| Loss mean:   0.245768 |\n",
      "g_step: 37300 loss std/mean: 0.019034162163734436 0.2463148683309555\n",
      "| Loss std:   0.019034 |\n",
      "| Loss mean:   0.246315 |\n",
      "g_step: 37400 loss std/mean: 0.01974816806614399 0.24674449861049652\n",
      "| Loss std:   0.019748 |\n",
      "| Loss mean:   0.246744 |\n",
      "g_step: 37500 loss std/mean: 0.018154120072722435 0.24997320771217346\n",
      "| Loss std:   0.018154 |\n",
      "| Loss mean:   0.249973 |\n",
      "g_step: 37600 loss std/mean: 0.018424827605485916 0.2432389110326767\n",
      "| Loss std:   0.018425 |\n",
      "| Loss mean:   0.243239 |\n",
      "g_step: 37700 loss std/mean: 0.01865593157708645 0.24594911932945251\n",
      "| Loss std:   0.018656 |\n",
      "| Loss mean:   0.245949 |\n",
      "g_step: 37800 loss std/mean: 0.02247149497270584 0.24192143976688385\n",
      "| Loss std:   0.022471 |\n",
      "| Loss mean:   0.241921 |\n",
      "g_step: 37900 loss std/mean: 0.017091229557991028 0.2445477992296219\n",
      "| Loss std:   0.017091 |\n",
      "| Loss mean:   0.244548 |\n",
      "g_step: 38000 loss std/mean: 0.01889920048415661 0.24323001503944397\n",
      "| Loss std:   0.018899 |\n",
      "| Loss mean:   0.243230 |\n",
      "g_step: 38100 loss std/mean: 0.019211117178201675 0.2441343069076538\n",
      "| Loss std:   0.019211 |\n",
      "| Loss mean:   0.244134 |\n",
      "g_step: 38200 loss std/mean: 0.01993376389145851 0.2448951005935669\n",
      "| Loss std:   0.019934 |\n",
      "| Loss mean:   0.244895 |\n",
      "g_step: 38300 loss std/mean: 0.020184291526675224 0.24543307721614838\n",
      "| Loss std:   0.020184 |\n",
      "| Loss mean:   0.245433 |\n",
      "g_step: 38400 loss std/mean: 0.019625337794423103 0.24613550305366516\n",
      "| Loss std:   0.019625 |\n",
      "| Loss mean:   0.246136 |\n",
      "g_step: 38500 loss std/mean: 0.018285464495420456 0.24803490936756134\n",
      "| Loss std:   0.018285 |\n",
      "| Loss mean:   0.248035 |\n",
      "g_step: 38600 loss std/mean: 0.020163724198937416 0.23927034437656403\n",
      "| Loss std:   0.020164 |\n",
      "| Loss mean:   0.239270 |\n",
      "g_step: 38700 loss std/mean: 0.020944589748978615 0.2454122006893158\n",
      "| Loss std:   0.020945 |\n",
      "| Loss mean:   0.245412 |\n",
      "g_step: 38800 loss std/mean: 0.019113026559352875 0.2442898154258728\n",
      "| Loss std:   0.019113 |\n",
      "| Loss mean:   0.244290 |\n",
      "g_step: 38900 loss std/mean: 0.01979643478989601 0.2471296489238739\n",
      "| Loss std:   0.019796 |\n",
      "| Loss mean:   0.247130 |\n",
      "g_step: 39000 loss std/mean: 0.021939082071185112 0.24207991361618042\n",
      "| Loss std:   0.021939 |\n",
      "| Loss mean:   0.242080 |\n",
      "g_step: 39100 loss std/mean: 0.02251933328807354 0.2437971830368042\n",
      "| Loss std:   0.022519 |\n",
      "| Loss mean:   0.243797 |\n",
      "g_step: 39200 loss std/mean: 0.021916359663009644 0.24798645079135895\n",
      "| Loss std:   0.021916 |\n",
      "| Loss mean:   0.247986 |\n",
      "g_step: 39300 loss std/mean: 0.02459166944026947 0.23987586796283722\n",
      "| Loss std:   0.024592 |\n",
      "| Loss mean:   0.239876 |\n",
      "g_step: 39400 loss std/mean: 0.02200252376496792 0.2466822862625122\n",
      "| Loss std:   0.022003 |\n",
      "| Loss mean:   0.246682 |\n",
      "g_step: 39500 loss std/mean: 0.018604910001158714 0.24057365953922272\n",
      "| Loss std:   0.018605 |\n",
      "| Loss mean:   0.240574 |\n",
      "g_step: 39600 loss std/mean: 0.019283391535282135 0.2450600117444992\n",
      "| Loss std:   0.019283 |\n",
      "| Loss mean:   0.245060 |\n",
      "g_step: 39700 loss std/mean: 0.020456286147236824 0.24104022979736328\n",
      "| Loss std:   0.020456 |\n",
      "| Loss mean:   0.241040 |\n",
      "g_step: 39800 loss std/mean: 0.018575821071863174 0.2446400672197342\n",
      "| Loss std:   0.018576 |\n",
      "| Loss mean:   0.244640 |\n",
      "g_step: 39900 loss std/mean: 0.022646041586995125 0.239227294921875\n",
      "| Loss std:   0.022646 |\n",
      "| Loss mean:   0.239227 |\n",
      "g_step: 40000 loss std/mean: 0.018517356365919113 0.240476593375206\n",
      "| Loss std:   0.018517 |\n",
      "| Loss mean:   0.240477 |\n",
      "g_step: 40100 loss std/mean: 0.019871992990374565 0.24488882720470428\n",
      "| Loss std:   0.019872 |\n",
      "| Loss mean:   0.244889 |\n",
      "g_step: 40200 loss std/mean: 0.01972307823598385 0.24408313632011414\n",
      "| Loss std:   0.019723 |\n",
      "| Loss mean:   0.244083 |\n",
      "g_step: 40300 loss std/mean: 0.02366839535534382 0.2418380230665207\n",
      "| Loss std:   0.023668 |\n",
      "| Loss mean:   0.241838 |\n",
      "g_step: 40400 loss std/mean: 0.01924806647002697 0.2448974996805191\n",
      "| Loss std:   0.019248 |\n",
      "| Loss mean:   0.244897 |\n",
      "g_step: 40500 loss std/mean: 0.01982865110039711 0.24577058851718903\n",
      "| Loss std:   0.019829 |\n",
      "| Loss mean:   0.245771 |\n",
      "g_step: 40600 loss std/mean: 0.020260389894247055 0.24831953644752502\n",
      "| Loss std:   0.020260 |\n",
      "| Loss mean:   0.248320 |\n",
      "g_step: 40700 loss std/mean: 0.019454441964626312 0.24758215248584747\n",
      "| Loss std:   0.019454 |\n",
      "| Loss mean:   0.247582 |\n",
      "g_step: 40800 loss std/mean: 0.016099868342280388 0.24537476897239685\n",
      "| Loss std:   0.016100 |\n",
      "| Loss mean:   0.245375 |\n",
      "g_step: 40900 loss std/mean: 0.01801181770861149 0.2453005164861679\n",
      "| Loss std:   0.018012 |\n",
      "| Loss mean:   0.245301 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g_step: 41000 loss std/mean: 0.022882582619786263 0.24029295146465302\n",
      "| Loss std:   0.022883 |\n",
      "| Loss mean:   0.240293 |\n",
      "g_step: 41100 loss std/mean: 0.021094871684908867 0.24252286553382874\n",
      "| Loss std:   0.021095 |\n",
      "| Loss mean:   0.242523 |\n",
      "g_step: 41200 loss std/mean: 0.020838120952248573 0.2448587417602539\n",
      "| Loss std:   0.020838 |\n",
      "| Loss mean:   0.244859 |\n",
      "g_step: 41300 loss std/mean: 0.02060607820749283 0.24449287354946136\n",
      "| Loss std:   0.020606 |\n",
      "| Loss mean:   0.244493 |\n",
      "g_step: 41400 loss std/mean: 0.020432842895388603 0.24242812395095825\n",
      "| Loss std:   0.020433 |\n",
      "| Loss mean:   0.242428 |\n",
      "g_step: 41500 loss std/mean: 0.022617321461439133 0.24376451969146729\n",
      "| Loss std:   0.022617 |\n",
      "| Loss mean:   0.243765 |\n",
      "g_step: 41600 loss std/mean: 0.017855998128652573 0.24455831944942474\n",
      "| Loss std:   0.017856 |\n",
      "| Loss mean:   0.244558 |\n",
      "g_step: 41700 loss std/mean: 0.0207743551582098 0.24202550947666168\n",
      "| Loss std:   0.020774 |\n",
      "| Loss mean:   0.242026 |\n",
      "g_step: 41800 loss std/mean: 0.019594648852944374 0.24024152755737305\n",
      "| Loss std:   0.019595 |\n",
      "| Loss mean:   0.240242 |\n",
      "g_step: 41900 loss std/mean: 0.018400240689516068 0.23938879370689392\n",
      "| Loss std:   0.018400 |\n",
      "| Loss mean:   0.239389 |\n",
      "g_step: 42000 loss std/mean: 0.021503368392586708 0.24156610667705536\n",
      "| Loss std:   0.021503 |\n",
      "| Loss mean:   0.241566 |\n",
      "g_step: 42100 loss std/mean: 0.017634257674217224 0.24041877686977386\n",
      "| Loss std:   0.017634 |\n",
      "| Loss mean:   0.240419 |\n",
      "g_step: 42200 loss std/mean: 0.01659093052148819 0.24594822525978088\n",
      "| Loss std:   0.016591 |\n",
      "| Loss mean:   0.245948 |\n",
      "g_step: 42300 loss std/mean: 0.018075980246067047 0.24700605869293213\n",
      "| Loss std:   0.018076 |\n",
      "| Loss mean:   0.247006 |\n",
      "g_step: 42400 loss std/mean: 0.017274999991059303 0.24109689891338348\n",
      "| Loss std:   0.017275 |\n",
      "| Loss mean:   0.241097 |\n",
      "g_step: 42500 loss std/mean: 0.019774630665779114 0.23851753771305084\n",
      "| Loss std:   0.019775 |\n",
      "| Loss mean:   0.238518 |\n",
      "g_step: 42600 loss std/mean: 0.020869996398687363 0.24108535051345825\n",
      "| Loss std:   0.020870 |\n",
      "| Loss mean:   0.241085 |\n",
      "g_step: 42700 loss std/mean: 0.020063746720552444 0.24171742796897888\n",
      "| Loss std:   0.020064 |\n",
      "| Loss mean:   0.241717 |\n",
      "g_step: 42800 loss std/mean: 0.016801906749606133 0.24467918276786804\n",
      "| Loss std:   0.016802 |\n",
      "| Loss mean:   0.244679 |\n",
      "g_step: 42900 loss std/mean: 0.01990639418363571 0.24376296997070312\n",
      "| Loss std:   0.019906 |\n",
      "| Loss mean:   0.243763 |\n",
      "g_step: 43000 loss std/mean: 0.01828601025044918 0.24440403282642365\n",
      "| Loss std:   0.018286 |\n",
      "| Loss mean:   0.244404 |\n",
      "g_step: 43100 loss std/mean: 0.01744535192847252 0.24682718515396118\n",
      "| Loss std:   0.017445 |\n",
      "| Loss mean:   0.246827 |\n",
      "g_step: 43200 loss std/mean: 0.02170620486140251 0.24366679787635803\n",
      "| Loss std:   0.021706 |\n",
      "| Loss mean:   0.243667 |\n",
      "g_step: 43300 loss std/mean: 0.0187677089124918 0.24420760571956635\n",
      "| Loss std:   0.018768 |\n",
      "| Loss mean:   0.244208 |\n",
      "g_step: 43400 loss std/mean: 0.01994680054485798 0.24459630250930786\n",
      "| Loss std:   0.019947 |\n",
      "| Loss mean:   0.244596 |\n",
      "g_step: 43500 loss std/mean: 0.01757333241403103 0.24966691434383392\n",
      "| Loss std:   0.017573 |\n",
      "| Loss mean:   0.249667 |\n",
      "g_step: 43600 loss std/mean: 0.017675647512078285 0.24271446466445923\n",
      "| Loss std:   0.017676 |\n",
      "| Loss mean:   0.242714 |\n",
      "g_step: 43700 loss std/mean: 0.019705241546034813 0.2444583773612976\n",
      "| Loss std:   0.019705 |\n",
      "| Loss mean:   0.244458 |\n",
      "g_step: 43800 loss std/mean: 0.021749189123511314 0.24430511891841888\n",
      "| Loss std:   0.021749 |\n",
      "| Loss mean:   0.244305 |\n",
      "g_step: 43900 loss std/mean: 0.019871212542057037 0.24499279260635376\n",
      "| Loss std:   0.019871 |\n",
      "| Loss mean:   0.244993 |\n",
      "g_step: 44000 loss std/mean: 0.021854262799024582 0.23961487412452698\n",
      "| Loss std:   0.021854 |\n",
      "| Loss mean:   0.239615 |\n",
      "g_step: 44100 loss std/mean: 0.020879477262496948 0.24135562777519226\n",
      "| Loss std:   0.020879 |\n",
      "| Loss mean:   0.241356 |\n",
      "g_step: 44200 loss std/mean: 0.017937710508704185 0.2384222149848938\n",
      "| Loss std:   0.017938 |\n",
      "| Loss mean:   0.238422 |\n",
      "g_step: 44300 loss std/mean: 0.019013220444321632 0.24237917363643646\n",
      "| Loss std:   0.019013 |\n",
      "| Loss mean:   0.242379 |\n",
      "g_step: 44400 loss std/mean: 0.019383417442440987 0.24081562459468842\n",
      "| Loss std:   0.019383 |\n",
      "| Loss mean:   0.240816 |\n",
      "g_step: 44500 loss std/mean: 0.020311463624238968 0.2433449923992157\n",
      "| Loss std:   0.020311 |\n",
      "| Loss mean:   0.243345 |\n",
      "g_step: 44600 loss std/mean: 0.020900949835777283 0.24542127549648285\n",
      "| Loss std:   0.020901 |\n",
      "| Loss mean:   0.245421 |\n",
      "g_step: 44700 loss std/mean: 0.018154947087168694 0.2493199110031128\n",
      "| Loss std:   0.018155 |\n",
      "| Loss mean:   0.249320 |\n",
      "g_step: 44800 loss std/mean: 0.017764724791049957 0.24311527609825134\n",
      "| Loss std:   0.017765 |\n",
      "| Loss mean:   0.243115 |\n",
      "g_step: 44900 loss std/mean: 0.01842743530869484 0.2449318766593933\n",
      "| Loss std:   0.018427 |\n",
      "| Loss mean:   0.244932 |\n",
      "g_step: 45000 loss std/mean: 0.019219597801566124 0.2452034056186676\n",
      "| Loss std:   0.019220 |\n",
      "| Loss mean:   0.245203 |\n",
      "g_step: 45100 loss std/mean: 0.01933722384274006 0.24198117852210999\n",
      "| Loss std:   0.019337 |\n",
      "| Loss mean:   0.241981 |\n",
      "g_step: 45200 loss std/mean: 0.023222927004098892 0.23727592825889587\n",
      "| Loss std:   0.023223 |\n",
      "| Loss mean:   0.237276 |\n",
      "g_step: 45300 loss std/mean: 0.020168362185359 0.24291402101516724\n",
      "| Loss std:   0.020168 |\n",
      "| Loss mean:   0.242914 |\n",
      "g_step: 45400 loss std/mean: 0.0187071580439806 0.24415813386440277\n",
      "| Loss std:   0.018707 |\n",
      "| Loss mean:   0.244158 |\n",
      "g_step: 45500 loss std/mean: 0.024096453562378883 0.24821051955223083\n",
      "| Loss std:   0.024096 |\n",
      "| Loss mean:   0.248211 |\n",
      "g_step: 45600 loss std/mean: 0.01824197545647621 0.24321773648262024\n",
      "| Loss std:   0.018242 |\n",
      "| Loss mean:   0.243218 |\n",
      "g_step: 45700 loss std/mean: 0.02244407683610916 0.24440984427928925\n",
      "| Loss std:   0.022444 |\n",
      "| Loss mean:   0.244410 |\n",
      "g_step: 45800 loss std/mean: 0.021393440663814545 0.241346538066864\n",
      "| Loss std:   0.021393 |\n",
      "| Loss mean:   0.241347 |\n",
      "g_step: 45900 loss std/mean: 0.019300051033496857 0.243939608335495\n",
      "| Loss std:   0.019300 |\n",
      "| Loss mean:   0.243940 |\n",
      "g_step: 46000 loss std/mean: 0.017125627025961876 0.24416595697402954\n",
      "| Loss std:   0.017126 |\n",
      "| Loss mean:   0.244166 |\n",
      "g_step: 46100 loss std/mean: 0.023854507133364677 0.23818472027778625\n",
      "| Loss std:   0.023855 |\n",
      "| Loss mean:   0.238185 |\n",
      "g_step: 46200 loss std/mean: 0.020410234108567238 0.2436651587486267\n",
      "| Loss std:   0.020410 |\n",
      "| Loss mean:   0.243665 |\n",
      "g_step: 46300 loss std/mean: 0.016193615272641182 0.24255691468715668\n",
      "| Loss std:   0.016194 |\n",
      "| Loss mean:   0.242557 |\n",
      "g_step: 46400 loss std/mean: 0.019898222759366035 0.2440626323223114\n",
      "| Loss std:   0.019898 |\n",
      "| Loss mean:   0.244063 |\n",
      "g_step: 46500 loss std/mean: 0.020534567534923553 0.24212142825126648\n",
      "| Loss std:   0.020535 |\n",
      "| Loss mean:   0.242121 |\n",
      "g_step: 46600 loss std/mean: 0.020340722054243088 0.24177326261997223\n",
      "| Loss std:   0.020341 |\n",
      "| Loss mean:   0.241773 |\n",
      "g_step: 46700 loss std/mean: 0.019678734242916107 0.23957933485507965\n",
      "| Loss std:   0.019679 |\n",
      "| Loss mean:   0.239579 |\n",
      "g_step: 46800 loss std/mean: 0.018750499933958054 0.24029861390590668\n",
      "| Loss std:   0.018750 |\n",
      "| Loss mean:   0.240299 |\n",
      "g_step: 46900 loss std/mean: 0.022211285308003426 0.24359647929668427\n",
      "| Loss std:   0.022211 |\n",
      "| Loss mean:   0.243596 |\n",
      "g_step: 47000 loss std/mean: 0.01663420908153057 0.24359530210494995\n",
      "| Loss std:   0.016634 |\n",
      "| Loss mean:   0.243595 |\n",
      "g_step: 47100 loss std/mean: 0.018186546862125397 0.2435823231935501\n",
      "| Loss std:   0.018187 |\n",
      "| Loss mean:   0.243582 |\n",
      "g_step: 47200 loss std/mean: 0.022703850641846657 0.237462118268013\n",
      "| Loss std:   0.022704 |\n",
      "| Loss mean:   0.237462 |\n",
      "g_step: 47300 loss std/mean: 0.01918215863406658 0.247474804520607\n",
      "| Loss std:   0.019182 |\n",
      "| Loss mean:   0.247475 |\n",
      "g_step: 47400 loss std/mean: 0.01821587234735489 0.24478434026241302\n",
      "| Loss std:   0.018216 |\n",
      "| Loss mean:   0.244784 |\n",
      "g_step: 47500 loss std/mean: 0.01914614625275135 0.23887325823307037\n",
      "| Loss std:   0.019146 |\n",
      "| Loss mean:   0.238873 |\n",
      "g_step: 47600 loss std/mean: 0.018433189019560814 0.24110090732574463\n",
      "| Loss std:   0.018433 |\n",
      "| Loss mean:   0.241101 |\n",
      "g_step: 47700 loss std/mean: 0.017857346683740616 0.24965517222881317\n",
      "| Loss std:   0.017857 |\n",
      "| Loss mean:   0.249655 |\n",
      "g_step: 47800 loss std/mean: 0.01653074100613594 0.2439817637205124\n",
      "| Loss std:   0.016531 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Loss mean:   0.243982 |\n",
      "g_step: 47900 loss std/mean: 0.021398989483714104 0.23786741495132446\n",
      "| Loss std:   0.021399 |\n",
      "| Loss mean:   0.237867 |\n",
      "g_step: 48000 loss std/mean: 0.019922861829400063 0.24559789896011353\n",
      "| Loss std:   0.019923 |\n",
      "| Loss mean:   0.245598 |\n",
      "g_step: 48100 loss std/mean: 0.01956835575401783 0.24102722108364105\n",
      "| Loss std:   0.019568 |\n",
      "| Loss mean:   0.241027 |\n",
      "g_step: 48200 loss std/mean: 0.022305158898234367 0.23841926455497742\n",
      "| Loss std:   0.022305 |\n",
      "| Loss mean:   0.238419 |\n",
      "g_step: 48300 loss std/mean: 0.02101142331957817 0.245802104473114\n",
      "| Loss std:   0.021011 |\n",
      "| Loss mean:   0.245802 |\n",
      "g_step: 48400 loss std/mean: 0.020018091425299644 0.23787905275821686\n",
      "| Loss std:   0.020018 |\n",
      "| Loss mean:   0.237879 |\n",
      "g_step: 48500 loss std/mean: 0.01905027963221073 0.23830991983413696\n",
      "| Loss std:   0.019050 |\n",
      "| Loss mean:   0.238310 |\n",
      "g_step: 48600 loss std/mean: 0.017857655882835388 0.24392284452915192\n",
      "| Loss std:   0.017858 |\n",
      "| Loss mean:   0.243923 |\n",
      "g_step: 48700 loss std/mean: 0.021187305450439453 0.2363893836736679\n",
      "| Loss std:   0.021187 |\n",
      "| Loss mean:   0.236389 |\n",
      "g_step: 48800 loss std/mean: 0.01903527043759823 0.2440546751022339\n",
      "| Loss std:   0.019035 |\n",
      "| Loss mean:   0.244055 |\n",
      "g_step: 48900 loss std/mean: 0.02053476870059967 0.23714397847652435\n",
      "| Loss std:   0.020535 |\n",
      "| Loss mean:   0.237144 |\n",
      "g_step: 49000 loss std/mean: 0.021817613393068314 0.23652613162994385\n",
      "| Loss std:   0.021818 |\n",
      "| Loss mean:   0.236526 |\n",
      "g_step: 49100 loss std/mean: 0.01795140467584133 0.242883563041687\n",
      "| Loss std:   0.017951 |\n",
      "| Loss mean:   0.242884 |\n",
      "g_step: 49200 loss std/mean: 0.019414082169532776 0.24721616506576538\n",
      "| Loss std:   0.019414 |\n",
      "| Loss mean:   0.247216 |\n",
      "g_step: 49300 loss std/mean: 0.018889544531702995 0.23971590399742126\n",
      "| Loss std:   0.018890 |\n",
      "| Loss mean:   0.239716 |\n",
      "g_step: 49400 loss std/mean: 0.019848568364977837 0.2422267347574234\n",
      "| Loss std:   0.019849 |\n",
      "| Loss mean:   0.242227 |\n",
      "g_step: 49500 loss std/mean: 0.02248973213136196 0.24613387882709503\n",
      "| Loss std:   0.022490 |\n",
      "| Loss mean:   0.246134 |\n",
      "g_step: 49600 loss std/mean: 0.020043835043907166 0.23729752004146576\n",
      "| Loss std:   0.020044 |\n",
      "| Loss mean:   0.237298 |\n",
      "g_step: 49700 loss std/mean: 0.019885381683707237 0.2392258197069168\n",
      "| Loss std:   0.019885 |\n",
      "| Loss mean:   0.239226 |\n",
      "g_step: 49800 loss std/mean: 0.021028535440564156 0.24066025018692017\n",
      "| Loss std:   0.021029 |\n",
      "| Loss mean:   0.240660 |\n",
      "g_step: 49900 loss std/mean: 0.01638217084109783 0.23906706273555756\n",
      "| Loss std:   0.016382 |\n",
      "| Loss mean:   0.239067 |\n",
      "g_step: 50000 loss std/mean: 0.017720278352499008 0.23962926864624023\n",
      "| Loss std:   0.017720 |\n",
      "| Loss mean:   0.239629 |\n",
      "g_step: 50100 loss std/mean: 0.018268682062625885 0.24753177165985107\n",
      "| Loss std:   0.018269 |\n",
      "| Loss mean:   0.247532 |\n",
      "g_step: 50200 loss std/mean: 0.020175402984023094 0.23596781492233276\n",
      "| Loss std:   0.020175 |\n",
      "| Loss mean:   0.235968 |\n",
      "g_step: 50300 loss std/mean: 0.020090021193027496 0.24434949457645416\n",
      "| Loss std:   0.020090 |\n",
      "| Loss mean:   0.244349 |\n",
      "g_step: 50400 loss std/mean: 0.018878264352679253 0.24035397171974182\n",
      "| Loss std:   0.018878 |\n",
      "| Loss mean:   0.240354 |\n",
      "g_step: 50500 loss std/mean: 0.021986525505781174 0.24346056580543518\n",
      "| Loss std:   0.021987 |\n",
      "| Loss mean:   0.243461 |\n",
      "g_step: 50600 loss std/mean: 0.016052376478910446 0.2438117414712906\n",
      "| Loss std:   0.016052 |\n",
      "| Loss mean:   0.243812 |\n",
      "g_step: 50700 loss std/mean: 0.019039392471313477 0.2481016218662262\n",
      "| Loss std:   0.019039 |\n",
      "| Loss mean:   0.248102 |\n",
      "g_step: 50800 loss std/mean: 0.018731070682406425 0.24410443007946014\n",
      "| Loss std:   0.018731 |\n",
      "| Loss mean:   0.244104 |\n",
      "g_step: 50900 loss std/mean: 0.018869539722800255 0.23954087495803833\n",
      "| Loss std:   0.018870 |\n",
      "| Loss mean:   0.239541 |\n",
      "g_step: 51000 loss std/mean: 0.02093922533094883 0.24277746677398682\n",
      "| Loss std:   0.020939 |\n",
      "| Loss mean:   0.242777 |\n",
      "g_step: 51100 loss std/mean: 0.0199393592774868 0.23950515687465668\n",
      "| Loss std:   0.019939 |\n",
      "| Loss mean:   0.239505 |\n",
      "g_step: 51200 loss std/mean: 0.017561107873916626 0.24499906599521637\n",
      "| Loss std:   0.017561 |\n",
      "| Loss mean:   0.244999 |\n",
      "g_step: 51300 loss std/mean: 0.019065208733081818 0.24247761070728302\n",
      "| Loss std:   0.019065 |\n",
      "| Loss mean:   0.242478 |\n",
      "g_step: 51400 loss std/mean: 0.016857370734214783 0.2439100444316864\n",
      "| Loss std:   0.016857 |\n",
      "| Loss mean:   0.243910 |\n",
      "g_step: 51500 loss std/mean: 0.022526754066348076 0.23804603517055511\n",
      "| Loss std:   0.022527 |\n",
      "| Loss mean:   0.238046 |\n",
      "g_step: 51600 loss std/mean: 0.018482418730854988 0.24308346211910248\n",
      "| Loss std:   0.018482 |\n",
      "| Loss mean:   0.243083 |\n",
      "g_step: 51700 loss std/mean: 0.020422806963324547 0.2382107675075531\n",
      "| Loss std:   0.020423 |\n",
      "| Loss mean:   0.238211 |\n",
      "g_step: 51800 loss std/mean: 0.018232306465506554 0.24057625234127045\n",
      "| Loss std:   0.018232 |\n",
      "| Loss mean:   0.240576 |\n",
      "g_step: 51900 loss std/mean: 0.022253047674894333 0.2401028275489807\n",
      "| Loss std:   0.022253 |\n",
      "| Loss mean:   0.240103 |\n",
      "g_step: 52000 loss std/mean: 0.02143339067697525 0.24415069818496704\n",
      "| Loss std:   0.021433 |\n",
      "| Loss mean:   0.244151 |\n",
      "g_step: 52100 loss std/mean: 0.019016457721590996 0.24457453191280365\n",
      "| Loss std:   0.019016 |\n",
      "| Loss mean:   0.244575 |\n",
      "g_step: 52200 loss std/mean: 0.02134723961353302 0.24171222746372223\n",
      "| Loss std:   0.021347 |\n",
      "| Loss mean:   0.241712 |\n",
      "g_step: 52300 loss std/mean: 0.02044467255473137 0.2435629814863205\n",
      "| Loss std:   0.020445 |\n",
      "| Loss mean:   0.243563 |\n",
      "g_step: 52400 loss std/mean: 0.017183106392621994 0.2466588020324707\n",
      "| Loss std:   0.017183 |\n",
      "| Loss mean:   0.246659 |\n",
      "g_step: 52500 loss std/mean: 0.02039465121924877 0.24265195429325104\n",
      "| Loss std:   0.020395 |\n",
      "| Loss mean:   0.242652 |\n",
      "g_step: 52600 loss std/mean: 0.022254690527915955 0.24244968593120575\n",
      "| Loss std:   0.022255 |\n",
      "| Loss mean:   0.242450 |\n",
      "g_step: 52700 loss std/mean: 0.01937294378876686 0.2407543808221817\n",
      "| Loss std:   0.019373 |\n",
      "| Loss mean:   0.240754 |\n",
      "g_step: 52800 loss std/mean: 0.021279871463775635 0.24099411070346832\n",
      "| Loss std:   0.021280 |\n",
      "| Loss mean:   0.240994 |\n",
      "g_step: 52900 loss std/mean: 0.015398094430565834 0.24101422727108002\n",
      "| Loss std:   0.015398 |\n",
      "| Loss mean:   0.241014 |\n",
      "g_step: 53000 loss std/mean: 0.02040017768740654 0.24349898099899292\n",
      "| Loss std:   0.020400 |\n",
      "| Loss mean:   0.243499 |\n",
      "g_step: 53100 loss std/mean: 0.01817205920815468 0.23948438465595245\n",
      "| Loss std:   0.018172 |\n",
      "| Loss mean:   0.239484 |\n",
      "g_step: 53200 loss std/mean: 0.01660449616611004 0.2424934059381485\n",
      "| Loss std:   0.016604 |\n",
      "| Loss mean:   0.242493 |\n",
      "g_step: 53300 loss std/mean: 0.01951315626502037 0.24274329841136932\n",
      "| Loss std:   0.019513 |\n",
      "| Loss mean:   0.242743 |\n",
      "g_step: 53400 loss std/mean: 0.019940709695219994 0.237371563911438\n",
      "| Loss std:   0.019941 |\n",
      "| Loss mean:   0.237372 |\n",
      "g_step: 53500 loss std/mean: 0.019189924001693726 0.2440449595451355\n",
      "| Loss std:   0.019190 |\n",
      "| Loss mean:   0.244045 |\n",
      "g_step: 53600 loss std/mean: 0.019767088815569878 0.24467739462852478\n",
      "| Loss std:   0.019767 |\n",
      "| Loss mean:   0.244677 |\n",
      "g_step: 53700 loss std/mean: 0.02153109945356846 0.23950360715389252\n",
      "| Loss std:   0.021531 |\n",
      "| Loss mean:   0.239504 |\n",
      "g_step: 53800 loss std/mean: 0.020894289016723633 0.23717626929283142\n",
      "| Loss std:   0.020894 |\n",
      "| Loss mean:   0.237176 |\n",
      "g_step: 53900 loss std/mean: 0.022667575627565384 0.2434806078672409\n",
      "| Loss std:   0.022668 |\n",
      "| Loss mean:   0.243481 |\n",
      "g_step: 54000 loss std/mean: 0.019570769742131233 0.2401789426803589\n",
      "| Loss std:   0.019571 |\n",
      "| Loss mean:   0.240179 |\n",
      "g_step: 54100 loss std/mean: 0.01979721151292324 0.2376743108034134\n",
      "| Loss std:   0.019797 |\n",
      "| Loss mean:   0.237674 |\n",
      "g_step: 54200 loss std/mean: 0.021456073969602585 0.23998798429965973\n",
      "| Loss std:   0.021456 |\n",
      "| Loss mean:   0.239988 |\n",
      "g_step: 54300 loss std/mean: 0.016773167997598648 0.2424434870481491\n",
      "| Loss std:   0.016773 |\n",
      "| Loss mean:   0.242443 |\n",
      "g_step: 54400 loss std/mean: 0.02065853215754032 0.2405397742986679\n",
      "| Loss std:   0.020659 |\n",
      "| Loss mean:   0.240540 |\n",
      "g_step: 54500 loss std/mean: 0.015401332639157772 0.2411501556634903\n",
      "| Loss std:   0.015401 |\n",
      "| Loss mean:   0.241150 |\n",
      "g_step: 54600 loss std/mean: 0.01880294643342495 0.23935435712337494\n",
      "| Loss std:   0.018803 |\n",
      "| Loss mean:   0.239354 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g_step: 54700 loss std/mean: 0.018409639596939087 0.24668273329734802\n",
      "| Loss std:   0.018410 |\n",
      "| Loss mean:   0.246683 |\n",
      "g_step: 54800 loss std/mean: 0.02304353006184101 0.23876237869262695\n",
      "| Loss std:   0.023044 |\n",
      "| Loss mean:   0.238762 |\n",
      "g_step: 54900 loss std/mean: 0.01710580475628376 0.23803383111953735\n",
      "| Loss std:   0.017106 |\n",
      "| Loss mean:   0.238034 |\n",
      "g_step: 55000 loss std/mean: 0.018021371215581894 0.24125432968139648\n",
      "| Loss std:   0.018021 |\n",
      "| Loss mean:   0.241254 |\n",
      "g_step: 55100 loss std/mean: 0.015948468819260597 0.24270056188106537\n",
      "| Loss std:   0.015948 |\n",
      "| Loss mean:   0.242701 |\n",
      "g_step: 55200 loss std/mean: 0.019661474972963333 0.24089232087135315\n",
      "| Loss std:   0.019661 |\n",
      "| Loss mean:   0.240892 |\n",
      "g_step: 55300 loss std/mean: 0.016232609748840332 0.24225814640522003\n",
      "| Loss std:   0.016233 |\n",
      "| Loss mean:   0.242258 |\n",
      "g_step: 55400 loss std/mean: 0.017283810302615166 0.24294671416282654\n",
      "| Loss std:   0.017284 |\n",
      "| Loss mean:   0.242947 |\n",
      "g_step: 55500 loss std/mean: 0.018917987123131752 0.2365102618932724\n",
      "| Loss std:   0.018918 |\n",
      "| Loss mean:   0.236510 |\n",
      "g_step: 55600 loss std/mean: 0.01619896851480007 0.24041791260242462\n",
      "| Loss std:   0.016199 |\n",
      "| Loss mean:   0.240418 |\n",
      "g_step: 55700 loss std/mean: 0.018475908786058426 0.2398085594177246\n",
      "| Loss std:   0.018476 |\n",
      "| Loss mean:   0.239809 |\n",
      "g_step: 55800 loss std/mean: 0.0189417265355587 0.241712287068367\n",
      "| Loss std:   0.018942 |\n",
      "| Loss mean:   0.241712 |\n",
      "g_step: 55900 loss std/mean: 0.022099316120147705 0.24481317400932312\n",
      "| Loss std:   0.022099 |\n",
      "| Loss mean:   0.244813 |\n",
      "g_step: 56000 loss std/mean: 0.020045379176735878 0.24145615100860596\n",
      "| Loss std:   0.020045 |\n",
      "| Loss mean:   0.241456 |\n",
      "g_step: 56100 loss std/mean: 0.016224466264247894 0.24239344894886017\n",
      "| Loss std:   0.016224 |\n",
      "| Loss mean:   0.242393 |\n",
      "g_step: 56200 loss std/mean: 0.021268540993332863 0.24206842482089996\n",
      "| Loss std:   0.021269 |\n",
      "| Loss mean:   0.242068 |\n",
      "g_step: 56300 loss std/mean: 0.0191415473818779 0.2404564470052719\n",
      "| Loss std:   0.019142 |\n",
      "| Loss mean:   0.240456 |\n",
      "g_step: 56400 loss std/mean: 0.018329888582229614 0.2410735934972763\n",
      "| Loss std:   0.018330 |\n",
      "| Loss mean:   0.241074 |\n",
      "g_step: 56500 loss std/mean: 0.020038453862071037 0.23603257536888123\n",
      "| Loss std:   0.020038 |\n",
      "| Loss mean:   0.236033 |\n",
      "g_step: 56600 loss std/mean: 0.018837416544556618 0.23924177885055542\n",
      "| Loss std:   0.018837 |\n",
      "| Loss mean:   0.239242 |\n",
      "g_step: 56700 loss std/mean: 0.019639620557427406 0.24640357494354248\n",
      "| Loss std:   0.019640 |\n",
      "| Loss mean:   0.246404 |\n",
      "g_step: 56800 loss std/mean: 0.021222691982984543 0.2396266907453537\n",
      "| Loss std:   0.021223 |\n",
      "| Loss mean:   0.239627 |\n",
      "g_step: 56900 loss std/mean: 0.0170359555631876 0.24599504470825195\n",
      "| Loss std:   0.017036 |\n",
      "| Loss mean:   0.245995 |\n",
      "g_step: 57000 loss std/mean: 0.0203318502753973 0.2425890564918518\n",
      "| Loss std:   0.020332 |\n",
      "| Loss mean:   0.242589 |\n",
      "g_step: 57100 loss std/mean: 0.017375633120536804 0.24201412498950958\n",
      "| Loss std:   0.017376 |\n",
      "| Loss mean:   0.242014 |\n",
      "g_step: 57200 loss std/mean: 0.016055984422564507 0.2419559359550476\n",
      "| Loss std:   0.016056 |\n",
      "| Loss mean:   0.241956 |\n",
      "g_step: 57300 loss std/mean: 0.020062914118170738 0.2392008900642395\n",
      "| Loss std:   0.020063 |\n",
      "| Loss mean:   0.239201 |\n",
      "g_step: 57400 loss std/mean: 0.01909165270626545 0.2408737689256668\n",
      "| Loss std:   0.019092 |\n",
      "| Loss mean:   0.240874 |\n",
      "g_step: 57500 loss std/mean: 0.0220150388777256 0.2414558082818985\n",
      "| Loss std:   0.022015 |\n",
      "| Loss mean:   0.241456 |\n",
      "g_step: 57600 loss std/mean: 0.01899070478975773 0.24544408917427063\n",
      "| Loss std:   0.018991 |\n",
      "| Loss mean:   0.245444 |\n",
      "g_step: 57700 loss std/mean: 0.021409781649708748 0.23719029128551483\n",
      "| Loss std:   0.021410 |\n",
      "| Loss mean:   0.237190 |\n",
      "g_step: 57800 loss std/mean: 0.022004900500178337 0.24021345376968384\n",
      "| Loss std:   0.022005 |\n",
      "| Loss mean:   0.240213 |\n",
      "g_step: 57900 loss std/mean: 0.017970861867070198 0.24418464303016663\n",
      "| Loss std:   0.017971 |\n",
      "| Loss mean:   0.244185 |\n",
      "g_step: 58000 loss std/mean: 0.020473631098866463 0.24330541491508484\n",
      "| Loss std:   0.020474 |\n",
      "| Loss mean:   0.243305 |\n",
      "g_step: 58100 loss std/mean: 0.018721969798207283 0.2400999218225479\n",
      "| Loss std:   0.018722 |\n",
      "| Loss mean:   0.240100 |\n",
      "g_step: 58200 loss std/mean: 0.018298419192433357 0.24278700351715088\n",
      "| Loss std:   0.018298 |\n",
      "| Loss mean:   0.242787 |\n",
      "g_step: 58300 loss std/mean: 0.019054125994443893 0.23593752086162567\n",
      "| Loss std:   0.019054 |\n",
      "| Loss mean:   0.235938 |\n",
      "g_step: 58400 loss std/mean: 0.02218855358660221 0.238590806722641\n",
      "| Loss std:   0.022189 |\n",
      "| Loss mean:   0.238591 |\n",
      "g_step: 58500 loss std/mean: 0.01874891296029091 0.24135421216487885\n",
      "| Loss std:   0.018749 |\n",
      "| Loss mean:   0.241354 |\n",
      "g_step: 58600 loss std/mean: 0.019351454451680183 0.23731864988803864\n",
      "| Loss std:   0.019351 |\n",
      "| Loss mean:   0.237319 |\n",
      "g_step: 58700 loss std/mean: 0.020334457978606224 0.2406691163778305\n",
      "| Loss std:   0.020334 |\n",
      "| Loss mean:   0.240669 |\n",
      "g_step: 58800 loss std/mean: 0.0219986941665411 0.2431858628988266\n",
      "| Loss std:   0.021999 |\n",
      "| Loss mean:   0.243186 |\n",
      "g_step: 58900 loss std/mean: 0.02119196020066738 0.24470797181129456\n",
      "| Loss std:   0.021192 |\n",
      "| Loss mean:   0.244708 |\n",
      "g_step: 59000 loss std/mean: 0.021102670580148697 0.23593856394290924\n",
      "| Loss std:   0.021103 |\n",
      "| Loss mean:   0.235939 |\n",
      "g_step: 59100 loss std/mean: 0.01930016092956066 0.24185246229171753\n",
      "| Loss std:   0.019300 |\n",
      "| Loss mean:   0.241852 |\n",
      "g_step: 59200 loss std/mean: 0.017721964046359062 0.24382442235946655\n",
      "| Loss std:   0.017722 |\n",
      "| Loss mean:   0.243824 |\n",
      "g_step: 59300 loss std/mean: 0.016890954226255417 0.2428848296403885\n",
      "| Loss std:   0.016891 |\n",
      "| Loss mean:   0.242885 |\n",
      "g_step: 59400 loss std/mean: 0.01913009025156498 0.24444690346717834\n",
      "| Loss std:   0.019130 |\n",
      "| Loss mean:   0.244447 |\n",
      "g_step: 59500 loss std/mean: 0.01912357658147812 0.24367636442184448\n",
      "| Loss std:   0.019124 |\n",
      "| Loss mean:   0.243676 |\n",
      "g_step: 59600 loss std/mean: 0.015624746680259705 0.2448141872882843\n",
      "| Loss std:   0.015625 |\n",
      "| Loss mean:   0.244814 |\n",
      "g_step: 59700 loss std/mean: 0.019696353003382683 0.24109184741973877\n",
      "| Loss std:   0.019696 |\n",
      "| Loss mean:   0.241092 |\n",
      "g_step: 59800 loss std/mean: 0.016772400587797165 0.24549083411693573\n",
      "| Loss std:   0.016772 |\n",
      "| Loss mean:   0.245491 |\n",
      "g_step: 59900 loss std/mean: 0.01813482865691185 0.24106769263744354\n",
      "| Loss std:   0.018135 |\n",
      "| Loss mean:   0.241068 |\n",
      "g_step: 60000 loss std/mean: 0.018751204013824463 0.24026986956596375\n",
      "| Loss std:   0.018751 |\n",
      "| Loss mean:   0.240270 |\n",
      "g_step: 60100 loss std/mean: 0.01961517333984375 0.24094222486019135\n",
      "| Loss std:   0.019615 |\n",
      "| Loss mean:   0.240942 |\n",
      "g_step: 60200 loss std/mean: 0.016815299168229103 0.24028906226158142\n",
      "| Loss std:   0.016815 |\n",
      "| Loss mean:   0.240289 |\n",
      "g_step: 60300 loss std/mean: 0.015269285067915916 0.2433524876832962\n",
      "| Loss std:   0.015269 |\n",
      "| Loss mean:   0.243352 |\n",
      "g_step: 60400 loss std/mean: 0.019538722932338715 0.2363356649875641\n",
      "| Loss std:   0.019539 |\n",
      "| Loss mean:   0.236336 |\n",
      "g_step: 60500 loss std/mean: 0.018592819571495056 0.24323779344558716\n",
      "| Loss std:   0.018593 |\n",
      "| Loss mean:   0.243238 |\n",
      "g_step: 60600 loss std/mean: 0.018410105258226395 0.2397657036781311\n",
      "| Loss std:   0.018410 |\n",
      "| Loss mean:   0.239766 |\n",
      "g_step: 60700 loss std/mean: 0.020937006920576096 0.23878328502178192\n",
      "| Loss std:   0.020937 |\n",
      "| Loss mean:   0.238783 |\n",
      "g_step: 60800 loss std/mean: 0.01679808273911476 0.24312180280685425\n",
      "| Loss std:   0.016798 |\n",
      "| Loss mean:   0.243122 |\n",
      "g_step: 60900 loss std/mean: 0.02093653753399849 0.2345818132162094\n",
      "| Loss std:   0.020937 |\n",
      "| Loss mean:   0.234582 |\n",
      "g_step: 61000 loss std/mean: 0.017619801685214043 0.24078863859176636\n",
      "| Loss std:   0.017620 |\n",
      "| Loss mean:   0.240789 |\n",
      "g_step: 61100 loss std/mean: 0.021710097789764404 0.23381832242012024\n",
      "| Loss std:   0.021710 |\n",
      "| Loss mean:   0.233818 |\n",
      "g_step: 61200 loss std/mean: 0.018124397844076157 0.2396373599767685\n",
      "| Loss std:   0.018124 |\n",
      "| Loss mean:   0.239637 |\n",
      "g_step: 61300 loss std/mean: 0.017128491774201393 0.23916392028331757\n",
      "| Loss std:   0.017128 |\n",
      "| Loss mean:   0.239164 |\n",
      "g_step: 61400 loss std/mean: 0.019787242636084557 0.24132026731967926\n",
      "| Loss std:   0.019787 |\n",
      "| Loss mean:   0.241320 |\n",
      "g_step: 61500 loss std/mean: 0.014345381408929825 0.24139375984668732\n",
      "| Loss std:   0.014345 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Loss mean:   0.241394 |\n",
      "g_step: 61600 loss std/mean: 0.019095346331596375 0.23566395044326782\n",
      "| Loss std:   0.019095 |\n",
      "| Loss mean:   0.235664 |\n",
      "g_step: 61700 loss std/mean: 0.022170254960656166 0.23883208632469177\n",
      "| Loss std:   0.022170 |\n",
      "| Loss mean:   0.238832 |\n",
      "g_step: 61800 loss std/mean: 0.017777008935809135 0.24206170439720154\n",
      "| Loss std:   0.017777 |\n",
      "| Loss mean:   0.242062 |\n",
      "g_step: 61900 loss std/mean: 0.01921192929148674 0.237731471657753\n",
      "| Loss std:   0.019212 |\n",
      "| Loss mean:   0.237731 |\n",
      "g_step: 62000 loss std/mean: 0.019168753176927567 0.24293066561222076\n",
      "| Loss std:   0.019169 |\n",
      "| Loss mean:   0.242931 |\n",
      "g_step: 62100 loss std/mean: 0.021955296397209167 0.2363656610250473\n",
      "| Loss std:   0.021955 |\n",
      "| Loss mean:   0.236366 |\n",
      "g_step: 62200 loss std/mean: 0.021849840879440308 0.24035340547561646\n",
      "| Loss std:   0.021850 |\n",
      "| Loss mean:   0.240353 |\n",
      "g_step: 62300 loss std/mean: 0.018431531265378 0.2404937744140625\n",
      "| Loss std:   0.018432 |\n",
      "| Loss mean:   0.240494 |\n",
      "g_step: 62400 loss std/mean: 0.018877949565649033 0.246465802192688\n",
      "| Loss std:   0.018878 |\n",
      "| Loss mean:   0.246466 |\n",
      "g_step: 62500 loss std/mean: 0.018153876066207886 0.24360160529613495\n",
      "| Loss std:   0.018154 |\n",
      "| Loss mean:   0.243602 |\n",
      "g_step: 62600 loss std/mean: 0.01749683916568756 0.241842120885849\n",
      "| Loss std:   0.017497 |\n",
      "| Loss mean:   0.241842 |\n",
      "g_step: 62700 loss std/mean: 0.017408030107617378 0.23943056166172028\n",
      "| Loss std:   0.017408 |\n",
      "| Loss mean:   0.239431 |\n",
      "g_step: 62800 loss std/mean: 0.019888386130332947 0.24262340366840363\n",
      "| Loss std:   0.019888 |\n",
      "| Loss mean:   0.242623 |\n",
      "g_step: 62900 loss std/mean: 0.022873269394040108 0.23888392746448517\n",
      "| Loss std:   0.022873 |\n",
      "| Loss mean:   0.238884 |\n",
      "g_step: 63000 loss std/mean: 0.018955979496240616 0.2378208190202713\n",
      "| Loss std:   0.018956 |\n",
      "| Loss mean:   0.237821 |\n",
      "g_step: 63100 loss std/mean: 0.020285289734601974 0.24300134181976318\n",
      "| Loss std:   0.020285 |\n",
      "| Loss mean:   0.243001 |\n",
      "g_step: 63200 loss std/mean: 0.01848381757736206 0.2380293905735016\n",
      "| Loss std:   0.018484 |\n",
      "| Loss mean:   0.238029 |\n",
      "g_step: 63300 loss std/mean: 0.020024865865707397 0.2391948252916336\n",
      "| Loss std:   0.020025 |\n",
      "| Loss mean:   0.239195 |\n",
      "g_step: 63400 loss std/mean: 0.020207151770591736 0.23677608370780945\n",
      "| Loss std:   0.020207 |\n",
      "| Loss mean:   0.236776 |\n",
      "g_step: 63500 loss std/mean: 0.020218105986714363 0.23799726366996765\n",
      "| Loss std:   0.020218 |\n",
      "| Loss mean:   0.237997 |\n",
      "g_step: 63600 loss std/mean: 0.020896179601550102 0.23731768131256104\n",
      "| Loss std:   0.020896 |\n",
      "| Loss mean:   0.237318 |\n",
      "g_step: 63700 loss std/mean: 0.019826767966151237 0.23901380598545074\n",
      "| Loss std:   0.019827 |\n",
      "| Loss mean:   0.239014 |\n",
      "g_step: 63800 loss std/mean: 0.019920779392123222 0.23622997105121613\n",
      "| Loss std:   0.019921 |\n",
      "| Loss mean:   0.236230 |\n",
      "g_step: 63900 loss std/mean: 0.020348289981484413 0.24019236862659454\n",
      "| Loss std:   0.020348 |\n",
      "| Loss mean:   0.240192 |\n",
      "g_step: 64000 loss std/mean: 0.022150592878460884 0.23449917137622833\n",
      "| Loss std:   0.022151 |\n",
      "| Loss mean:   0.234499 |\n",
      "g_step: 64100 loss std/mean: 0.018227022141218185 0.2370920181274414\n",
      "| Loss std:   0.018227 |\n",
      "| Loss mean:   0.237092 |\n",
      "g_step: 64200 loss std/mean: 0.019379403442144394 0.23904775083065033\n",
      "| Loss std:   0.019379 |\n",
      "| Loss mean:   0.239048 |\n",
      "g_step: 64300 loss std/mean: 0.015900325030088425 0.23697973787784576\n",
      "| Loss std:   0.015900 |\n",
      "| Loss mean:   0.236980 |\n",
      "g_step: 64400 loss std/mean: 0.018878581002354622 0.24114960432052612\n",
      "| Loss std:   0.018879 |\n",
      "| Loss mean:   0.241150 |\n",
      "g_step: 64500 loss std/mean: 0.020974785089492798 0.23900966346263885\n",
      "| Loss std:   0.020975 |\n",
      "| Loss mean:   0.239010 |\n",
      "g_step: 64600 loss std/mean: 0.01695968210697174 0.24025651812553406\n",
      "| Loss std:   0.016960 |\n",
      "| Loss mean:   0.240257 |\n",
      "g_step: 64700 loss std/mean: 0.02139987051486969 0.2390661984682083\n",
      "| Loss std:   0.021400 |\n",
      "| Loss mean:   0.239066 |\n",
      "g_step: 64800 loss std/mean: 0.019459476694464684 0.23619693517684937\n",
      "| Loss std:   0.019459 |\n",
      "| Loss mean:   0.236197 |\n",
      "g_step: 64900 loss std/mean: 0.017775150015950203 0.24110938608646393\n",
      "| Loss std:   0.017775 |\n",
      "| Loss mean:   0.241109 |\n",
      "g_step: 65000 loss std/mean: 0.018691832199692726 0.2404341697692871\n",
      "| Loss std:   0.018692 |\n",
      "| Loss mean:   0.240434 |\n",
      "g_step: 65100 loss std/mean: 0.01734042726457119 0.23991960287094116\n",
      "| Loss std:   0.017340 |\n",
      "| Loss mean:   0.239920 |\n",
      "g_step: 65200 loss std/mean: 0.021413687616586685 0.23533914983272552\n",
      "| Loss std:   0.021414 |\n",
      "| Loss mean:   0.235339 |\n",
      "g_step: 65300 loss std/mean: 0.01944493129849434 0.23303574323654175\n",
      "| Loss std:   0.019445 |\n",
      "| Loss mean:   0.233036 |\n",
      "g_step: 65400 loss std/mean: 0.01837548054754734 0.23950625956058502\n",
      "| Loss std:   0.018375 |\n",
      "| Loss mean:   0.239506 |\n",
      "g_step: 65500 loss std/mean: 0.01689603365957737 0.2368887960910797\n",
      "| Loss std:   0.016896 |\n",
      "| Loss mean:   0.236889 |\n",
      "g_step: 65600 loss std/mean: 0.019755011424422264 0.2398337870836258\n",
      "| Loss std:   0.019755 |\n",
      "| Loss mean:   0.239834 |\n",
      "g_step: 65700 loss std/mean: 0.016970930621027946 0.23763607442378998\n",
      "| Loss std:   0.016971 |\n",
      "| Loss mean:   0.237636 |\n",
      "g_step: 65800 loss std/mean: 0.0171470008790493 0.24061647057533264\n",
      "| Loss std:   0.017147 |\n",
      "| Loss mean:   0.240616 |\n",
      "g_step: 65900 loss std/mean: 0.019737381488084793 0.24052393436431885\n",
      "| Loss std:   0.019737 |\n",
      "| Loss mean:   0.240524 |\n",
      "g_step: 66000 loss std/mean: 0.020066989585757256 0.2382633537054062\n",
      "| Loss std:   0.020067 |\n",
      "| Loss mean:   0.238263 |\n",
      "g_step: 66100 loss std/mean: 0.02034648507833481 0.2417692244052887\n",
      "| Loss std:   0.020346 |\n",
      "| Loss mean:   0.241769 |\n",
      "g_step: 66200 loss std/mean: 0.020289001986384392 0.23768508434295654\n",
      "| Loss std:   0.020289 |\n",
      "| Loss mean:   0.237685 |\n",
      "g_step: 66300 loss std/mean: 0.017190854996442795 0.2416989654302597\n",
      "| Loss std:   0.017191 |\n",
      "| Loss mean:   0.241699 |\n",
      "g_step: 66400 loss std/mean: 0.020932402461767197 0.24182531237602234\n",
      "| Loss std:   0.020932 |\n",
      "| Loss mean:   0.241825 |\n",
      "g_step: 66500 loss std/mean: 0.0207606740295887 0.23954933881759644\n",
      "| Loss std:   0.020761 |\n",
      "| Loss mean:   0.239549 |\n",
      "g_step: 66600 loss std/mean: 0.02012595348060131 0.2380916178226471\n",
      "| Loss std:   0.020126 |\n",
      "| Loss mean:   0.238092 |\n",
      "g_step: 66700 loss std/mean: 0.02159317210316658 0.23732340335845947\n",
      "| Loss std:   0.021593 |\n",
      "| Loss mean:   0.237323 |\n",
      "g_step: 66800 loss std/mean: 0.021028803661465645 0.23754331469535828\n",
      "| Loss std:   0.021029 |\n",
      "| Loss mean:   0.237543 |\n",
      "g_step: 66900 loss std/mean: 0.01908705197274685 0.23774917423725128\n",
      "| Loss std:   0.019087 |\n",
      "| Loss mean:   0.237749 |\n",
      "g_step: 67000 loss std/mean: 0.018406111747026443 0.2393970489501953\n",
      "| Loss std:   0.018406 |\n",
      "| Loss mean:   0.239397 |\n",
      "g_step: 67100 loss std/mean: 0.019120698794722557 0.23904205858707428\n",
      "| Loss std:   0.019121 |\n",
      "| Loss mean:   0.239042 |\n",
      "g_step: 67200 loss std/mean: 0.01979571208357811 0.23914234340190887\n",
      "| Loss std:   0.019796 |\n",
      "| Loss mean:   0.239142 |\n",
      "g_step: 67300 loss std/mean: 0.020916758105158806 0.24323631823062897\n",
      "| Loss std:   0.020917 |\n",
      "| Loss mean:   0.243236 |\n",
      "g_step: 67400 loss std/mean: 0.01912285014986992 0.2407875955104828\n",
      "| Loss std:   0.019123 |\n",
      "| Loss mean:   0.240788 |\n",
      "g_step: 67500 loss std/mean: 0.01847655139863491 0.24226310849189758\n",
      "| Loss std:   0.018477 |\n",
      "| Loss mean:   0.242263 |\n",
      "g_step: 67600 loss std/mean: 0.017453161999583244 0.24345234036445618\n",
      "| Loss std:   0.017453 |\n",
      "| Loss mean:   0.243452 |\n",
      "g_step: 67700 loss std/mean: 0.021035315468907356 0.23590435087680817\n",
      "| Loss std:   0.021035 |\n",
      "| Loss mean:   0.235904 |\n",
      "g_step: 67800 loss std/mean: 0.018644409254193306 0.24025703966617584\n",
      "| Loss std:   0.018644 |\n",
      "| Loss mean:   0.240257 |\n",
      "g_step: 67900 loss std/mean: 0.0188704002648592 0.2433285117149353\n",
      "| Loss std:   0.018870 |\n",
      "| Loss mean:   0.243329 |\n",
      "g_step: 68000 loss std/mean: 0.01970549486577511 0.24024160206317902\n",
      "| Loss std:   0.019705 |\n",
      "| Loss mean:   0.240242 |\n",
      "g_step: 68100 loss std/mean: 0.018468674272298813 0.23934295773506165\n",
      "| Loss std:   0.018469 |\n",
      "| Loss mean:   0.239343 |\n",
      "g_step: 68200 loss std/mean: 0.020392348989844322 0.23516255617141724\n",
      "| Loss std:   0.020392 |\n",
      "| Loss mean:   0.235163 |\n",
      "g_step: 68300 loss std/mean: 0.01619493030011654 0.24101576209068298\n",
      "| Loss std:   0.016195 |\n",
      "| Loss mean:   0.241016 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g_step: 68400 loss std/mean: 0.019524183124303818 0.23956885933876038\n",
      "| Loss std:   0.019524 |\n",
      "| Loss mean:   0.239569 |\n",
      "g_step: 68500 loss std/mean: 0.017589813098311424 0.24793244898319244\n",
      "| Loss std:   0.017590 |\n",
      "| Loss mean:   0.247932 |\n",
      "g_step: 68600 loss std/mean: 0.01762019842863083 0.24055646359920502\n",
      "| Loss std:   0.017620 |\n",
      "| Loss mean:   0.240556 |\n",
      "g_step: 68700 loss std/mean: 0.019211385399103165 0.24149371683597565\n",
      "| Loss std:   0.019211 |\n",
      "| Loss mean:   0.241494 |\n",
      "g_step: 68800 loss std/mean: 0.019352905452251434 0.2380613386631012\n",
      "| Loss std:   0.019353 |\n",
      "| Loss mean:   0.238061 |\n",
      "g_step: 68900 loss std/mean: 0.017934778705239296 0.23968228697776794\n",
      "| Loss std:   0.017935 |\n",
      "| Loss mean:   0.239682 |\n",
      "g_step: 69000 loss std/mean: 0.01675976812839508 0.2411826252937317\n",
      "| Loss std:   0.016760 |\n",
      "| Loss mean:   0.241183 |\n",
      "g_step: 69100 loss std/mean: 0.019268905743956566 0.24044765532016754\n",
      "| Loss std:   0.019269 |\n",
      "| Loss mean:   0.240448 |\n",
      "g_step: 69200 loss std/mean: 0.019770650193095207 0.2397506684064865\n",
      "| Loss std:   0.019771 |\n",
      "| Loss mean:   0.239751 |\n",
      "g_step: 69300 loss std/mean: 0.02000950649380684 0.23945440351963043\n",
      "| Loss std:   0.020010 |\n",
      "| Loss mean:   0.239454 |\n",
      "g_step: 69400 loss std/mean: 0.018471088260412216 0.2370654046535492\n",
      "| Loss std:   0.018471 |\n",
      "| Loss mean:   0.237065 |\n",
      "g_step: 69500 loss std/mean: 0.019755899906158447 0.2388514131307602\n",
      "| Loss std:   0.019756 |\n",
      "| Loss mean:   0.238851 |\n",
      "g_step: 69600 loss std/mean: 0.018683064728975296 0.23703666031360626\n",
      "| Loss std:   0.018683 |\n",
      "| Loss mean:   0.237037 |\n",
      "g_step: 69700 loss std/mean: 0.018511658534407616 0.2365509271621704\n",
      "| Loss std:   0.018512 |\n",
      "| Loss mean:   0.236551 |\n",
      "g_step: 69800 loss std/mean: 0.01806296780705452 0.24056512117385864\n",
      "| Loss std:   0.018063 |\n",
      "| Loss mean:   0.240565 |\n",
      "g_step: 69900 loss std/mean: 0.022910207509994507 0.23897932469844818\n",
      "| Loss std:   0.022910 |\n",
      "| Loss mean:   0.238979 |\n",
      "g_step: 70000 loss std/mean: 0.017575586214661598 0.2371780276298523\n",
      "| Loss std:   0.017576 |\n",
      "| Loss mean:   0.237178 |\n",
      "g_step: 70100 loss std/mean: 0.01922254078090191 0.23770374059677124\n",
      "| Loss std:   0.019223 |\n",
      "| Loss mean:   0.237704 |\n",
      "g_step: 70200 loss std/mean: 0.018631387501955032 0.23570449650287628\n",
      "| Loss std:   0.018631 |\n",
      "| Loss mean:   0.235704 |\n",
      "g_step: 70300 loss std/mean: 0.019112225621938705 0.2349240630865097\n",
      "| Loss std:   0.019112 |\n",
      "| Loss mean:   0.234924 |\n",
      "g_step: 70400 loss std/mean: 0.020388783887028694 0.24009038507938385\n",
      "| Loss std:   0.020389 |\n",
      "| Loss mean:   0.240090 |\n",
      "g_step: 70500 loss std/mean: 0.020805949345231056 0.23744648694992065\n",
      "| Loss std:   0.020806 |\n",
      "| Loss mean:   0.237446 |\n",
      "g_step: 70600 loss std/mean: 0.019953839480876923 0.24505452811717987\n",
      "| Loss std:   0.019954 |\n",
      "| Loss mean:   0.245055 |\n",
      "g_step: 70700 loss std/mean: 0.019280988723039627 0.24150671064853668\n",
      "| Loss std:   0.019281 |\n",
      "| Loss mean:   0.241507 |\n",
      "g_step: 70800 loss std/mean: 0.019242476671934128 0.24213829636573792\n",
      "| Loss std:   0.019242 |\n",
      "| Loss mean:   0.242138 |\n",
      "g_step: 70900 loss std/mean: 0.018920646980404854 0.2398911863565445\n",
      "| Loss std:   0.018921 |\n",
      "| Loss mean:   0.239891 |\n",
      "g_step: 71000 loss std/mean: 0.018570775166153908 0.2348589450120926\n",
      "| Loss std:   0.018571 |\n",
      "| Loss mean:   0.234859 |\n",
      "g_step: 71100 loss std/mean: 0.0208396315574646 0.23578780889511108\n",
      "| Loss std:   0.020840 |\n",
      "| Loss mean:   0.235788 |\n",
      "g_step: 71200 loss std/mean: 0.01947893388569355 0.23759441077709198\n",
      "| Loss std:   0.019479 |\n",
      "| Loss mean:   0.237594 |\n",
      "g_step: 71300 loss std/mean: 0.02082343026995659 0.23573753237724304\n",
      "| Loss std:   0.020823 |\n",
      "| Loss mean:   0.235738 |\n",
      "g_step: 71400 loss std/mean: 0.018659448251128197 0.23984737694263458\n",
      "| Loss std:   0.018659 |\n",
      "| Loss mean:   0.239847 |\n",
      "g_step: 71500 loss std/mean: 0.016722558066248894 0.23875999450683594\n",
      "| Loss std:   0.016723 |\n",
      "| Loss mean:   0.238760 |\n",
      "g_step: 71600 loss std/mean: 0.018609032034873962 0.2383776158094406\n",
      "| Loss std:   0.018609 |\n",
      "| Loss mean:   0.238378 |\n",
      "g_step: 71700 loss std/mean: 0.020662592723965645 0.23677806556224823\n",
      "| Loss std:   0.020663 |\n",
      "| Loss mean:   0.236778 |\n",
      "g_step: 71800 loss std/mean: 0.020205272361636162 0.2383572906255722\n",
      "| Loss std:   0.020205 |\n",
      "| Loss mean:   0.238357 |\n",
      "g_step: 71900 loss std/mean: 0.01718159206211567 0.2407190501689911\n",
      "| Loss std:   0.017182 |\n",
      "| Loss mean:   0.240719 |\n",
      "g_step: 72000 loss std/mean: 0.01976768672466278 0.23720745742321014\n",
      "| Loss std:   0.019768 |\n",
      "| Loss mean:   0.237207 |\n",
      "g_step: 72100 loss std/mean: 0.019718119874596596 0.23631919920444489\n",
      "| Loss std:   0.019718 |\n",
      "| Loss mean:   0.236319 |\n",
      "g_step: 72200 loss std/mean: 0.02097170241177082 0.23069068789482117\n",
      "| Loss std:   0.020972 |\n",
      "| Loss mean:   0.230691 |\n",
      "g_step: 72300 loss std/mean: 0.021005209535360336 0.2367655634880066\n",
      "| Loss std:   0.021005 |\n",
      "| Loss mean:   0.236766 |\n",
      "g_step: 72400 loss std/mean: 0.017566241323947906 0.2376977950334549\n",
      "| Loss std:   0.017566 |\n",
      "| Loss mean:   0.237698 |\n",
      "g_step: 72500 loss std/mean: 0.019548796117305756 0.238317608833313\n",
      "| Loss std:   0.019549 |\n",
      "| Loss mean:   0.238318 |\n",
      "g_step: 72600 loss std/mean: 0.018572870641946793 0.23748554289340973\n",
      "| Loss std:   0.018573 |\n",
      "| Loss mean:   0.237486 |\n",
      "g_step: 72700 loss std/mean: 0.022483158856630325 0.23751813173294067\n",
      "| Loss std:   0.022483 |\n",
      "| Loss mean:   0.237518 |\n",
      "g_step: 72800 loss std/mean: 0.018115032464265823 0.24154235422611237\n",
      "| Loss std:   0.018115 |\n",
      "| Loss mean:   0.241542 |\n",
      "g_step: 72900 loss std/mean: 0.018214693292975426 0.2378414273262024\n",
      "| Loss std:   0.018215 |\n",
      "| Loss mean:   0.237841 |\n",
      "g_step: 73000 loss std/mean: 0.01679139770567417 0.2378641664981842\n",
      "| Loss std:   0.016791 |\n",
      "| Loss mean:   0.237864 |\n",
      "g_step: 73100 loss std/mean: 0.019127527251839638 0.2409282624721527\n",
      "| Loss std:   0.019128 |\n",
      "| Loss mean:   0.240928 |\n",
      "g_step: 73200 loss std/mean: 0.02054247446358204 0.24082839488983154\n",
      "| Loss std:   0.020542 |\n",
      "| Loss mean:   0.240828 |\n",
      "g_step: 73300 loss std/mean: 0.018884679302573204 0.23873098194599152\n",
      "| Loss std:   0.018885 |\n",
      "| Loss mean:   0.238731 |\n",
      "g_step: 73400 loss std/mean: 0.019511358812451363 0.23587292432785034\n",
      "| Loss std:   0.019511 |\n",
      "| Loss mean:   0.235873 |\n",
      "g_step: 73500 loss std/mean: 0.017086556181311607 0.24072504043579102\n",
      "| Loss std:   0.017087 |\n",
      "| Loss mean:   0.240725 |\n",
      "g_step: 73600 loss std/mean: 0.019860202446579933 0.2383285015821457\n",
      "| Loss std:   0.019860 |\n",
      "| Loss mean:   0.238329 |\n",
      "g_step: 73700 loss std/mean: 0.020361782982945442 0.23719070851802826\n",
      "| Loss std:   0.020362 |\n",
      "| Loss mean:   0.237191 |\n",
      "g_step: 73800 loss std/mean: 0.019530564546585083 0.2327699214220047\n",
      "| Loss std:   0.019531 |\n",
      "| Loss mean:   0.232770 |\n",
      "g_step: 73900 loss std/mean: 0.016189094632864 0.2382587045431137\n",
      "| Loss std:   0.016189 |\n",
      "| Loss mean:   0.238259 |\n",
      "g_step: 74000 loss std/mean: 0.01875201426446438 0.2416243702173233\n",
      "| Loss std:   0.018752 |\n",
      "| Loss mean:   0.241624 |\n",
      "g_step: 74100 loss std/mean: 0.021666022017598152 0.2377067357301712\n",
      "| Loss std:   0.021666 |\n",
      "| Loss mean:   0.237707 |\n",
      "g_step: 74200 loss std/mean: 0.017171218991279602 0.24050188064575195\n",
      "| Loss std:   0.017171 |\n",
      "| Loss mean:   0.240502 |\n",
      "g_step: 74300 loss std/mean: 0.0199014563113451 0.23769564926624298\n",
      "| Loss std:   0.019901 |\n",
      "| Loss mean:   0.237696 |\n",
      "g_step: 74400 loss std/mean: 0.019560014829039574 0.23603861033916473\n",
      "| Loss std:   0.019560 |\n",
      "| Loss mean:   0.236039 |\n",
      "g_step: 74500 loss std/mean: 0.01512953545898199 0.2426847666501999\n",
      "| Loss std:   0.015130 |\n",
      "| Loss mean:   0.242685 |\n",
      "g_step: 74600 loss std/mean: 0.01706935279071331 0.24085094034671783\n",
      "| Loss std:   0.017069 |\n",
      "| Loss mean:   0.240851 |\n",
      "g_step: 74700 loss std/mean: 0.01662728562951088 0.23496438562870026\n",
      "| Loss std:   0.016627 |\n",
      "| Loss mean:   0.234964 |\n",
      "g_step: 74800 loss std/mean: 0.022381985560059547 0.23934853076934814\n",
      "| Loss std:   0.022382 |\n",
      "| Loss mean:   0.239349 |\n",
      "g_step: 74900 loss std/mean: 0.02002403698861599 0.23711009323596954\n",
      "| Loss std:   0.020024 |\n",
      "| Loss mean:   0.237110 |\n",
      "g_step: 75000 loss std/mean: 0.01690443977713585 0.24439294636249542\n",
      "| Loss std:   0.016904 |\n",
      "| Loss mean:   0.244393 |\n",
      "g_step: 75100 loss std/mean: 0.020354803651571274 0.23950521647930145\n",
      "| Loss std:   0.020355 |\n",
      "| Loss mean:   0.239505 |\n",
      "g_step: 75200 loss std/mean: 0.018475402146577835 0.2401074916124344\n",
      "| Loss std:   0.018475 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Loss mean:   0.240107 |\n",
      "g_step: 75300 loss std/mean: 0.01665009744465351 0.23883360624313354\n",
      "| Loss std:   0.016650 |\n",
      "| Loss mean:   0.238834 |\n",
      "g_step: 75400 loss std/mean: 0.018576916307210922 0.23830169439315796\n",
      "| Loss std:   0.018577 |\n",
      "| Loss mean:   0.238302 |\n",
      "g_step: 75500 loss std/mean: 0.019859066233038902 0.23603178560733795\n",
      "| Loss std:   0.019859 |\n",
      "| Loss mean:   0.236032 |\n",
      "g_step: 75600 loss std/mean: 0.018993064761161804 0.23388724029064178\n",
      "| Loss std:   0.018993 |\n",
      "| Loss mean:   0.233887 |\n",
      "g_step: 75700 loss std/mean: 0.016337474808096886 0.23864158987998962\n",
      "| Loss std:   0.016337 |\n",
      "| Loss mean:   0.238642 |\n",
      "g_step: 75800 loss std/mean: 0.01853819005191326 0.23682868480682373\n",
      "| Loss std:   0.018538 |\n",
      "| Loss mean:   0.236829 |\n",
      "g_step: 75900 loss std/mean: 0.01997363567352295 0.23762378096580505\n",
      "| Loss std:   0.019974 |\n",
      "| Loss mean:   0.237624 |\n",
      "g_step: 76000 loss std/mean: 0.017238015308976173 0.23699510097503662\n",
      "| Loss std:   0.017238 |\n",
      "| Loss mean:   0.236995 |\n",
      "g_step: 76100 loss std/mean: 0.018884215503931046 0.23855379223823547\n",
      "| Loss std:   0.018884 |\n",
      "| Loss mean:   0.238554 |\n",
      "g_step: 76200 loss std/mean: 0.019343499094247818 0.2334519922733307\n",
      "| Loss std:   0.019343 |\n",
      "| Loss mean:   0.233452 |\n",
      "g_step: 76300 loss std/mean: 0.02327725850045681 0.2384040653705597\n",
      "| Loss std:   0.023277 |\n",
      "| Loss mean:   0.238404 |\n",
      "g_step: 76400 loss std/mean: 0.019321022555232048 0.23482494056224823\n",
      "| Loss std:   0.019321 |\n",
      "| Loss mean:   0.234825 |\n",
      "g_step: 76500 loss std/mean: 0.020445063710212708 0.23680591583251953\n",
      "| Loss std:   0.020445 |\n",
      "| Loss mean:   0.236806 |\n",
      "g_step: 76600 loss std/mean: 0.019843420013785362 0.23565973341464996\n",
      "| Loss std:   0.019843 |\n",
      "| Loss mean:   0.235660 |\n",
      "g_step: 76700 loss std/mean: 0.017130021005868912 0.23747384548187256\n",
      "| Loss std:   0.017130 |\n",
      "| Loss mean:   0.237474 |\n",
      "g_step: 76800 loss std/mean: 0.021090496331453323 0.23994402587413788\n",
      "| Loss std:   0.021090 |\n",
      "| Loss mean:   0.239944 |\n",
      "g_step: 76900 loss std/mean: 0.021549338474869728 0.2330930083990097\n",
      "| Loss std:   0.021549 |\n",
      "| Loss mean:   0.233093 |\n",
      "g_step: 77000 loss std/mean: 0.014956667087972164 0.23886889219284058\n",
      "| Loss std:   0.014957 |\n",
      "| Loss mean:   0.238869 |\n",
      "g_step: 77100 loss std/mean: 0.021423589438199997 0.2386651486158371\n",
      "| Loss std:   0.021424 |\n",
      "| Loss mean:   0.238665 |\n",
      "g_step: 77200 loss std/mean: 0.020391026511788368 0.23498758673667908\n",
      "| Loss std:   0.020391 |\n",
      "| Loss mean:   0.234988 |\n",
      "g_step: 77300 loss std/mean: 0.019215604290366173 0.23856687545776367\n",
      "| Loss std:   0.019216 |\n",
      "| Loss mean:   0.238567 |\n",
      "g_step: 77400 loss std/mean: 0.01830068789422512 0.23730412125587463\n",
      "| Loss std:   0.018301 |\n",
      "| Loss mean:   0.237304 |\n"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "\n",
    "history = 400\n",
    "time_to_predict = 16\n",
    "freq=1\n",
    "\n",
    "last_day_train = '2017-07-30'\n",
    "window=300\n",
    "epochs = 100\n",
    "validation_day = pd.to_datetime(last_day_train) #+ pd.Timedelta('{} days'.format(time_to_predict))\n",
    "batch_size = 1000\n",
    "sum_W = 3574368.0/16\n",
    "skip=0\n",
    "\n",
    "print(validation_day)\n",
    "\n",
    "batch_gen = get_random_train_test(\n",
    "    df_pivot,\n",
    "    last_day_train,\n",
    "    window=window,\n",
    "    history=history,\n",
    "    size=batch_size,\n",
    "    predict_days=time_to_predict,\n",
    "    epochs=epochs,\n",
    "    skip=skip,\n",
    "    freq=freq\n",
    ")\n",
    "\n",
    "val_set = get_validation(df_pivot, validation_day, history=history,\n",
    "                        predict_days=time_to_predict, skip=skip)\n",
    "\n",
    "from model import RNNModel\n",
    "\n",
    "m = RNNModel(\n",
    "    history=history,\n",
    "    n_days_predict=time_to_predict,\n",
    "    clip_gradients=1.,\n",
    "    starter_learning_rate=0.0001,\n",
    "    #starter_learning_rate=0.0005,\n",
    "    n_layers_rnn=1,\n",
    "    rnn_size_encoder=400,\n",
    "    rnn_size_decoder=400,\n",
    "    #output_droupouts_kp=[.9, .9, .9, .95, 1.]\n",
    ")\n",
    "print(1)\n",
    "m.build_graph(batch_gen)\n",
    "\n",
    "\n",
    "try:\n",
    "    hd_exp.end()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "hd_exp = hd.Experiment('RNN fav 3')\n",
    "\n",
    "m.train(val_set, coef=unit_std, sum_W=sum_W,\n",
    "        report_every=100, validate_every=0,\n",
    "        hd_exp=hd_exp, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales = df[\n",
    "    (df['item_nbr'] == 1503844) &\n",
    "    (df['store_nbr'] == 44) \n",
    "    \n",
    "]['unit_sales_scaled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEICAYAAABWJCMKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2aaf2878cdd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sm.graphics.tsa.plot_pacf(sales, lags=450)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEICAYAAABWJCMKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2aad0004ceb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sales2, tmp = stats.boxcox((sales+unit_mean))\n",
    "sm.graphics.tsa.plot_pacf(sales2, lags=450)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
