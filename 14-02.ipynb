{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gc\n",
    "import hyperdash as hd\n",
    "\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#'store_nbr', 'n_city', 'n_state', 'n_type', 'cluster', 'item_nbr', 'n_family', 'class', 'perishable'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unit_mean, unit_std = pd.read_csv('data/mean_std.csv', index_col=0).T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_items['class'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4100 entries, 0 to 4099\n",
      "Data columns (total 4 columns):\n",
      "item_nbr      4100 non-null int32\n",
      "n_family      4100 non-null uint32\n",
      "class         4100 non-null int32\n",
      "perishable    4100 non-null int8\n",
      "dtypes: int32(2), int8(1), uint32(1)\n",
      "memory usage: 52.1 KB\n"
     ]
    }
   ],
   "source": [
    "df_items.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_stores = pd.read_csv(\n",
    "    'data/num_stores.csv.gz',\n",
    "     dtype={\n",
    "         'store_nbr': np.uint8,\n",
    "         'n_city': np.uint32,\n",
    "         'n_state': np.uint32,\n",
    "         'n_type': np.uint32,\n",
    "         'cluster': np.uint32\n",
    "     }\n",
    "\n",
    ")\n",
    "df_items = pd.read_csv(\n",
    "    'data/num_items.csv.gz',\n",
    "    dtype={\n",
    "        'item_nbr': np.int32,\n",
    "        'n_family': np.int32,\n",
    "        'class': np.int32,\n",
    "        'perishable': np.int8,\n",
    "    }\n",
    ")\n",
    "for stores_col in ['n_city', 'n_state', 'n_type', 'cluster']:\n",
    "    df_stores[stores_col] = df_stores[stores_col] - df_stores[stores_col].min()\n",
    "    \n",
    "for items_col in ['n_family', 'class', 'perishable']:\n",
    "    df_items[items_col] = df_items[items_col] - df_items[items_col].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 34s, sys: 18.3 s, total: 5min 52s\n",
      "Wall time: 5min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_csv(\n",
    "    'data/ts.csv.gz',\n",
    "    parse_dates=[0],\n",
    "    #nrows=1000000,\n",
    "    dtype={\n",
    "        'item_nbr': np.int32,\n",
    "        'store_nbr': np.int8,\n",
    "        'unit_sales': np.float32,\n",
    "        'onpromotion': np.int8,\n",
    "        'holiday': np.int8,\n",
    "        'weekend': np.int8,\n",
    "        'waged_day': np.int8,\n",
    "        'dow_0': np.int8,\n",
    "        'dow_1': np.int8,\n",
    "        'dow_2': np.int8,\n",
    "        'dow_3': np.int8,\n",
    "        'dow_4': np.int8,\n",
    "        'dow_5': np.int8,\n",
    "        'dow_6': np.int8,\n",
    "    }\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmitry/miniconda3/envs/tf_intel/lib/python3.5/site-packages/pandas/core/reshape/merge.py:551: UserWarning: merging between different levels can give an unintended result (2 levels on the left, 1 on the right)\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 49s, sys: 2min 43s, total: 15min 32s\n",
      "Wall time: 12min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ts_columns = df.columns[3:]\n",
    "      \n",
    "attr_cols = [\n",
    "    'store_nbr', 'n_city', 'n_state', 'n_type', 'cluster',\n",
    "    'item_nbr', 'n_family', 'class',\n",
    "    #'perishable'\n",
    "]\n",
    "\n",
    "df_pivot = df.pivot_table(\n",
    "    index=['store_nbr', 'item_nbr'],\n",
    "    columns=['date'],\n",
    "    values=ts_columns\n",
    ").reset_index()\n",
    "\n",
    "df_pivot = df_pivot.merge(df_items, on='item_nbr')\n",
    "df_pivot['store_nbr'] = df_pivot[('store_nbr', '')]\n",
    "df_pivot = df_pivot.merge(df_stores, on='store_nbr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 210654 entries, 0 to 210653\n",
      "Columns: 9887 entries, item_nbr to cluster\n",
      "dtypes: float64(823), int32(1), int64(4), int8(9054), uint32(5)\n",
      "memory usage: 3.1 GB\n"
     ]
    }
   ],
   "source": [
    "df_pivot.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(onpromotion, 2017-07-31 00:00:00)</th>\n",
       "      <th>(holiday, 2017-07-31 00:00:00)</th>\n",
       "      <th>(weekend, 2017-07-31 00:00:00)</th>\n",
       "      <th>(waged_day, 2017-07-31 00:00:00)</th>\n",
       "      <th>(dow_0, 2017-07-31 00:00:00)</th>\n",
       "      <th>(dow_1, 2017-07-31 00:00:00)</th>\n",
       "      <th>(dow_2, 2017-07-31 00:00:00)</th>\n",
       "      <th>(dow_3, 2017-07-31 00:00:00)</th>\n",
       "      <th>(dow_4, 2017-07-31 00:00:00)</th>\n",
       "      <th>(dow_5, 2017-07-31 00:00:00)</th>\n",
       "      <th>...</th>\n",
       "      <th>(holiday, 2017-08-15 00:00:00)</th>\n",
       "      <th>(weekend, 2017-08-15 00:00:00)</th>\n",
       "      <th>(waged_day, 2017-08-15 00:00:00)</th>\n",
       "      <th>(dow_0, 2017-08-15 00:00:00)</th>\n",
       "      <th>(dow_1, 2017-08-15 00:00:00)</th>\n",
       "      <th>(dow_2, 2017-08-15 00:00:00)</th>\n",
       "      <th>(dow_3, 2017-08-15 00:00:00)</th>\n",
       "      <th>(dow_4, 2017-08-15 00:00:00)</th>\n",
       "      <th>(dow_5, 2017-08-15 00:00:00)</th>\n",
       "      <th>(dow_6, 2017-08-15 00:00:00)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 176 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   (onpromotion, 2017-07-31 00:00:00)  (holiday, 2017-07-31 00:00:00)  \\\n",
       "0                                   0                               0   \n",
       "1                                   0                               0   \n",
       "2                                   0                               0   \n",
       "3                                   0                               0   \n",
       "4                                   0                               0   \n",
       "\n",
       "   (weekend, 2017-07-31 00:00:00)  (waged_day, 2017-07-31 00:00:00)  \\\n",
       "0                               0                                 1   \n",
       "1                               0                                 1   \n",
       "2                               0                                 1   \n",
       "3                               0                                 1   \n",
       "4                               0                                 1   \n",
       "\n",
       "   (dow_0, 2017-07-31 00:00:00)  (dow_1, 2017-07-31 00:00:00)  \\\n",
       "0                             1                             0   \n",
       "1                             1                             0   \n",
       "2                             1                             0   \n",
       "3                             1                             0   \n",
       "4                             1                             0   \n",
       "\n",
       "   (dow_2, 2017-07-31 00:00:00)  (dow_3, 2017-07-31 00:00:00)  \\\n",
       "0                             0                             0   \n",
       "1                             0                             0   \n",
       "2                             0                             0   \n",
       "3                             0                             0   \n",
       "4                             0                             0   \n",
       "\n",
       "   (dow_4, 2017-07-31 00:00:00)  (dow_5, 2017-07-31 00:00:00)  \\\n",
       "0                             0                             0   \n",
       "1                             0                             0   \n",
       "2                             0                             0   \n",
       "3                             0                             0   \n",
       "4                             0                             0   \n",
       "\n",
       "               ...               (holiday, 2017-08-15 00:00:00)  \\\n",
       "0              ...                                            0   \n",
       "1              ...                                            0   \n",
       "2              ...                                            0   \n",
       "3              ...                                            0   \n",
       "4              ...                                            0   \n",
       "\n",
       "   (weekend, 2017-08-15 00:00:00)  (waged_day, 2017-08-15 00:00:00)  \\\n",
       "0                               0                                 1   \n",
       "1                               0                                 1   \n",
       "2                               0                                 1   \n",
       "3                               0                                 1   \n",
       "4                               0                                 1   \n",
       "\n",
       "   (dow_0, 2017-08-15 00:00:00)  (dow_1, 2017-08-15 00:00:00)  \\\n",
       "0                             0                             1   \n",
       "1                             0                             1   \n",
       "2                             0                             1   \n",
       "3                             0                             1   \n",
       "4                             0                             1   \n",
       "\n",
       "   (dow_2, 2017-08-15 00:00:00)  (dow_3, 2017-08-15 00:00:00)  \\\n",
       "0                             0                             0   \n",
       "1                             0                             0   \n",
       "2                             0                             0   \n",
       "3                             0                             0   \n",
       "4                             0                             0   \n",
       "\n",
       "   (dow_4, 2017-08-15 00:00:00)  (dow_5, 2017-08-15 00:00:00)  \\\n",
       "0                             0                             0   \n",
       "1                             0                             0   \n",
       "2                             0                             0   \n",
       "3                             0                             0   \n",
       "4                             0                             0   \n",
       "\n",
       "   (dow_6, 2017-08-15 00:00:00)  \n",
       "0                             0  \n",
       "1                             0  \n",
       "2                             0  \n",
       "3                             0  \n",
       "4                             0  \n",
       "\n",
       "[5 rows x 176 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_date_cols(date, history=20, predict_days=16, ts_columns=ts_columns, skip=0):\n",
    "                  #date, days=1, attr_cols=attr_columns_wo_means, ts_cols=ts_columns, attr=True):\n",
    "    \n",
    "    if type(date) != pd.Timestamp:\n",
    "        date = pd.to_datetime(date)\n",
    "        \n",
    "    X_start_date = date - pd.Timedelta('{} days'.format(history-1))\n",
    "    #X_end_date = date\n",
    "    y_start_date = date + pd.Timedelta('{} days'.format(skip+1))\n",
    "    #y_end_date = date + pd.Timedelta('{} days'.format(predict_days))\n",
    "\n",
    "    X_cols, y_cols, y_day_attr_cols = [], [], []\n",
    "    \n",
    "    for d in pd.date_range(X_start_date, periods=history, freq='D'):\n",
    "        for elem in ts_columns:\n",
    "            X_cols.append((elem, d))\n",
    "            \n",
    "    for d in pd.date_range(y_start_date, periods=predict_days, freq='D'):\n",
    "        y_cols.append(('unit_sales_scaled', d))\n",
    "        for elem in ts_columns[1:]:\n",
    "            y_day_attr_cols.append((elem, d))\n",
    "            \n",
    "    return X_cols, y_cols, y_day_attr_cols\n",
    "\n",
    "\n",
    "\n",
    "X_cols, y_cols, y_day_attr_cols = get_date_cols('2017-07-30', predict_days=16)\n",
    "    \n",
    "df_pivot.head().loc[:, y_day_attr_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2017-07-01', '2017-07-08', '2017-07-15'], dtype='datetime64[ns]', freq='7D')\n",
      "(90, 12) (5, 11) (5,) ()\n"
     ]
    }
   ],
   "source": [
    "def get_random_train_test(df_pivot,\n",
    "        date, window=21, freq=7, size=2000, history=1, predict_days=16, epochs=2, \n",
    "        shuffle_dates=True, shuffle_indexes=True, attr_cols=attr_cols, ts_columns=ts_columns, skip=0):\n",
    "    \n",
    "    \n",
    "    window = freq * (window//freq)\n",
    "    num_items = df_pivot.shape[0]\n",
    "    \n",
    "    date = pd.to_datetime(date)\n",
    "    start_window =  date - pd.Timedelta('{} days'.format(window))\n",
    "    end_date = date\n",
    "    \n",
    "    dates = pd.date_range(start=start_window, end=end_date)\n",
    "    dates = dates[::freq]\n",
    "    \n",
    "    print(dates)\n",
    "    \n",
    "    patches = []\n",
    "    #end_X_date = end_date - pd.Timedelta('{} days'.format(label_dates))\n",
    "    if shuffle_dates and shuffle_indexes:\n",
    "        permutated_dates = np.random.permutation(dates)\n",
    "        permutated_indx = np.random.permutation(num_items)   \n",
    "        for epoch in range(epochs):\n",
    "            for i in range(num_items//size+1):\n",
    "                s = size * i\n",
    "                e = size * (i+1)\n",
    "                indexes = permutated_indx[s:e]\n",
    "\n",
    "                for date in permutated_dates:\n",
    "                    patches.append([indexes, date])\n",
    "\n",
    "        patches = np.random.permutation(patches)\n",
    "        \n",
    "    elif not shuffle_dates and shuffle_indexes:\n",
    "        permutated_indx = np.random.permutation(num_items)\n",
    "        for date in dates:\n",
    "            for epoch in range(epochs):\n",
    "                for i in range(num_items//size+1):\n",
    "                    s = size * i\n",
    "                    e = size * (i+1)\n",
    "                    indexes = permutated_indx[s:e]\n",
    "                    patches.append([indexes, date])\n",
    "\n",
    "    for indexes, date in patches:\n",
    "        df_pivot_slice = df_pivot.iloc[indexes]\n",
    "        X_cols, y_cols, y_day_attr_cols = get_date_cols(\n",
    "            date, history=history, predict_days=predict_days, ts_columns=ts_columns, skip=skip\n",
    "        )\n",
    "\n",
    "        X = np.array(\n",
    "            df_pivot_slice.loc[:, X_cols]\n",
    "        ).reshape([-1, history, len(ts_columns)])\n",
    "\n",
    "        y_day_attr = np.array(\n",
    "            df_pivot_slice.loc[:, y_day_attr_cols]\n",
    "        ).reshape([-1, predict_days, len(ts_columns)-1])\n",
    "        \n",
    "        y = np.array(df_pivot_slice.loc[:, y_cols])\n",
    "        features = [X, y_day_attr, y]\n",
    "        for feature in attr_cols:\n",
    "            features.append(\n",
    "                np.array(df_pivot_slice.loc[:, feature])\n",
    "            )\n",
    "        for i in range(len(indexes)):\n",
    "            yield tuple([elem[i] for elem in features])\n",
    "\n",
    "tmp = get_random_train_test(df_pivot, '2017-07-15', window=20, history=90, predict_days=5)\n",
    "tmp1 = next(tmp)\n",
    "print(tmp1[0].shape, tmp1[1].shape, tmp1[2].shape, tmp1[3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(2000):\n",
    "    next(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.71689729,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [-0.71689729,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [-0.71689729,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        ..., \n",
       "        [-0.71689729,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [-0.71689729,  0.        ,  0.        , ...,  1.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [-0.71689729,  0.        ,  0.        , ...,  0.        ,\n",
       "          1.        ,  0.        ]]), array([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]], dtype=int8), array([-0.71689729, -0.71689729, -0.71689729, -0.71689729, -0.71689729]), 36, 11, 6, 4, 9, 1960806, 30, 106)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210654, 90, 12) (210654, 5, 11) (210654, 5) (210654,)\n"
     ]
    }
   ],
   "source": [
    "def get_validation(df_pivot,\n",
    "        date, history=1, predict_days=16, attr_cols=attr_cols, ts_columns=ts_columns, skip=0):\n",
    "    \n",
    "    X_cols, y_cols, y_day_attr_cols = get_date_cols(\n",
    "        date, history=history, predict_days=predict_days, ts_columns=ts_columns, skip=skip\n",
    "    )\n",
    "\n",
    "    X = np.array(\n",
    "        df_pivot.loc[:, X_cols]\n",
    "    ).reshape([-1, history, len(ts_columns)])\n",
    "    \n",
    "    y_day_attr = np.array(\n",
    "        df_pivot.loc[:, y_day_attr_cols]\n",
    "    ).reshape([-1, predict_days, len(ts_columns)-1])\n",
    "\n",
    "    y = np.array(df_pivot.loc[:, y_cols])\n",
    "    features = [X, y_day_attr, y]\n",
    "    for feature in attr_cols:\n",
    "        features.append(\n",
    "            np.array(df_pivot.loc[:, feature])\n",
    "        )\n",
    "\n",
    "    return features\n",
    "\n",
    "tmp = get_validation(df_pivot, '2017-07-15', history=90, predict_days=5)\n",
    "print(tmp[0].shape, tmp[1].shape, tmp[2].shape, tmp[3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-30 00:00:00\n",
      "1\n",
      "DatetimeIndex(['2016-09-17', '2016-09-18', '2016-09-19', '2016-09-20',\n",
      "               '2016-09-21', '2016-09-22', '2016-09-23', '2016-09-24',\n",
      "               '2016-09-25', '2016-09-26',\n",
      "               ...\n",
      "               '2017-07-05', '2017-07-06', '2017-07-07', '2017-07-08',\n",
      "               '2017-07-09', '2017-07-10', '2017-07-11', '2017-07-12',\n",
      "               '2017-07-13', '2017-07-14'],\n",
      "              dtype='datetime64[ns]', length=301, freq='D')\n",
      "g_step: 100 loss std/mean: 0.1469144970178604 0.3842019736766815\n",
      "| Loss std:   0.146914 |\n",
      "| Loss mean:   0.384202 |\n",
      "g_step: 200 loss std/mean: 0.042616572231054306 0.29940861463546753\n",
      "| Loss std:   0.042617 |\n",
      "| Loss mean:   0.299409 |\n",
      "g_step: 300 loss std/mean: 0.0687897652387619 0.3116743266582489\n",
      "| Loss std:   0.068790 |\n",
      "| Loss mean:   0.311674 |\n",
      "g_step: 400 loss std/mean: 0.038225989788770676 0.2979506850242615\n",
      "| Loss std:   0.038226 |\n",
      "| Loss mean:   0.297951 |\n",
      "g_step: 500 loss std/mean: 0.03937314450740814 0.28980013728141785\n",
      "| Loss std:   0.039373 |\n",
      "| Loss mean:   0.289800 |\n",
      "g_step: 600 loss std/mean: 0.060945309698581696 0.30092155933380127\n",
      "| Loss std:   0.060945 |\n",
      "| Loss mean:   0.300922 |\n",
      "g_step: 700 loss std/mean: 0.04330550134181976 0.2863258421421051\n",
      "| Loss std:   0.043306 |\n",
      "| Loss mean:   0.286326 |\n",
      "g_step: 800 loss std/mean: 0.05594080686569214 0.2971396744251251\n",
      "| Loss std:   0.055941 |\n",
      "| Loss mean:   0.297140 |\n",
      "g_step: 900 loss std/mean: 0.046479761600494385 0.29040879011154175\n",
      "| Loss std:   0.046480 |\n",
      "| Loss mean:   0.290409 |\n",
      "g_step: 1000 loss std/mean: 0.04196969419717789 0.28817903995513916\n",
      "| Loss std:   0.041970 |\n",
      "| Loss mean:   0.288179 |\n",
      "g_step: 1100 loss std/mean: 0.0584346279501915 0.2974523603916168\n",
      "| Loss std:   0.058435 |\n",
      "| Loss mean:   0.297452 |\n",
      "g_step: 1200 loss std/mean: 0.03479153290390968 0.27945417165756226\n",
      "| Loss std:   0.034792 |\n",
      "| Loss mean:   0.279454 |\n",
      "g_step: 1300 loss std/mean: 0.03972821682691574 0.2913665473461151\n",
      "| Loss std:   0.039728 |\n",
      "| Loss mean:   0.291367 |\n",
      "g_step: 1400 loss std/mean: 0.047493111342191696 0.28293657302856445\n",
      "| Loss std:   0.047493 |\n",
      "| Loss mean:   0.282937 |\n",
      "g_step: 1500 loss std/mean: 0.03347612917423248 0.2732749879360199\n",
      "| Loss std:   0.033476 |\n",
      "| Loss mean:   0.273275 |\n",
      "g_step: 1600 loss std/mean: 0.060390375554561615 0.28789591789245605\n",
      "| Loss std:   0.060390 |\n",
      "| Loss mean:   0.287896 |\n",
      "g_step: 1700 loss std/mean: 0.031189454719424248 0.27767518162727356\n",
      "| Loss std:   0.031189 |\n",
      "| Loss mean:   0.277675 |\n",
      "g_step: 1800 loss std/mean: 0.037050578743219376 0.2786236107349396\n",
      "| Loss std:   0.037051 |\n",
      "| Loss mean:   0.278624 |\n",
      "g_step: 1900 loss std/mean: 0.05664084106683731 0.29242730140686035\n",
      "| Loss std:   0.056641 |\n",
      "| Loss mean:   0.292427 |\n",
      "g_step: 2000 loss std/mean: 0.044352754950523376 0.27976906299591064\n",
      "| Loss std:   0.044353 |\n",
      "| Loss mean:   0.279769 |\n",
      "g_step: 2100 loss std/mean: 0.04704376682639122 0.28600892424583435\n",
      "| Loss std:   0.047044 |\n",
      "| Loss mean:   0.286009 |\n",
      "g_step: 2200 loss std/mean: 0.046430543065071106 0.278435617685318\n",
      "| Loss std:   0.046431 |\n",
      "| Loss mean:   0.278436 |\n",
      "g_step: 2300 loss std/mean: 0.06070780009031296 0.28578951954841614\n",
      "| Loss std:   0.060708 |\n",
      "| Loss mean:   0.285790 |\n",
      "g_step: 2400 loss std/mean: 0.040522344410419464 0.2760657072067261\n",
      "| Loss std:   0.040522 |\n",
      "| Loss mean:   0.276066 |\n",
      "g_step: 2500 loss std/mean: 0.04633700102567673 0.277151495218277\n",
      "| Loss std:   0.046337 |\n",
      "| Loss mean:   0.277151 |\n",
      "g_step: 2600 loss std/mean: 0.04244580864906311 0.27738144993782043\n",
      "| Loss std:   0.042446 |\n",
      "| Loss mean:   0.277381 |\n",
      "g_step: 2700 loss std/mean: 0.045136164873838425 0.2723700702190399\n",
      "| Loss std:   0.045136 |\n",
      "| Loss mean:   0.272370 |\n",
      "g_step: 2800 loss std/mean: 0.04532327130436897 0.2906782925128937\n",
      "| Loss std:   0.045323 |\n",
      "| Loss mean:   0.290678 |\n",
      "g_step: 2900 loss std/mean: 0.04659126698970795 0.2827262580394745\n",
      "| Loss std:   0.046591 |\n",
      "| Loss mean:   0.282726 |\n",
      "g_step: 3000 loss std/mean: 0.05593860521912575 0.2873481512069702\n",
      "| Loss std:   0.055939 |\n",
      "| Loss mean:   0.287348 |\n",
      "g_step: 3100 loss std/mean: 0.047958627343177795 0.2892802953720093\n",
      "| Loss std:   0.047959 |\n",
      "| Loss mean:   0.289280 |\n",
      "g_step: 3200 loss std/mean: 0.0417233482003212 0.27963995933532715\n",
      "| Loss std:   0.041723 |\n",
      "| Loss mean:   0.279640 |\n",
      "g_step: 3300 loss std/mean: 0.052835047245025635 0.2808893024921417\n",
      "| Loss std:   0.052835 |\n",
      "| Loss mean:   0.280889 |\n",
      "g_step: 3400 loss std/mean: 0.038098741322755814 0.2769646644592285\n",
      "| Loss std:   0.038099 |\n",
      "| Loss mean:   0.276965 |\n",
      "g_step: 3500 loss std/mean: 0.04377370327711105 0.2736639976501465\n",
      "| Loss std:   0.043774 |\n",
      "| Loss mean:   0.273664 |\n",
      "g_step: 3600 loss std/mean: 0.04380999878048897 0.27572429180145264\n",
      "| Loss std:   0.043810 |\n",
      "| Loss mean:   0.275724 |\n",
      "g_step: 3700 loss std/mean: 0.0427677258849144 0.27813977003097534\n",
      "| Loss std:   0.042768 |\n",
      "| Loss mean:   0.278140 |\n",
      "g_step: 3800 loss std/mean: 0.03720312565565109 0.2676403522491455\n",
      "| Loss std:   0.037203 |\n",
      "| Loss mean:   0.267640 |\n",
      "g_step: 3900 loss std/mean: 0.0488504022359848 0.2829357087612152\n",
      "| Loss std:   0.048850 |\n",
      "| Loss mean:   0.282936 |\n",
      "g_step: 4000 loss std/mean: 0.032308805733919144 0.263064980506897\n",
      "| Loss std:   0.032309 |\n",
      "| Loss mean:   0.263065 |\n",
      "g_step: 4100 loss std/mean: 0.04575178772211075 0.28295600414276123\n",
      "| Loss std:   0.045752 |\n",
      "| Loss mean:   0.282956 |\n",
      "g_step: 4200 loss std/mean: 0.034027375280857086 0.2624339163303375\n",
      "| Loss std:   0.034027 |\n",
      "| Loss mean:   0.262434 |\n",
      "g_step: 4300 loss std/mean: 0.035319309681653976 0.27920153737068176\n",
      "| Loss std:   0.035319 |\n",
      "| Loss mean:   0.279202 |\n",
      "g_step: 4400 loss std/mean: 0.038872960954904556 0.27360999584198\n",
      "| Loss std:   0.038873 |\n",
      "| Loss mean:   0.273610 |\n",
      "g_step: 4500 loss std/mean: 0.03307749703526497 0.2628744840621948\n",
      "| Loss std:   0.033077 |\n",
      "| Loss mean:   0.262874 |\n",
      "g_step: 4600 loss std/mean: 0.029079969972372055 0.26824551820755005\n",
      "| Loss std:   0.029080 |\n",
      "| Loss mean:   0.268246 |\n",
      "g_step: 4700 loss std/mean: 0.0319281704723835 0.27092134952545166\n",
      "| Loss std:   0.031928 |\n",
      "| Loss mean:   0.270921 |\n",
      "g_step: 4800 loss std/mean: 0.03566955775022507 0.27344951033592224\n",
      "| Loss std:   0.035670 |\n",
      "| Loss mean:   0.273450 |\n",
      "g_step: 4900 loss std/mean: 0.033593375235795975 0.27912983298301697\n",
      "| Loss std:   0.033593 |\n",
      "| Loss mean:   0.279130 |\n",
      "g_step: 5000 loss std/mean: 0.035350386053323746 0.2784016728401184\n",
      "| Loss std:   0.035350 |\n",
      "| Loss mean:   0.278402 |\n",
      "\tValidation NWRMSLE  : 0.54719220058\n",
      "| Validation NWRMSLE:   0.547192 |\n",
      "\tValidation NWRMSLE_5: 0.534267618645\n",
      "| Validation NWRMSLE_5:   0.534268 |\n",
      "g_step: 5100 loss std/mean: 0.0286224577575922 0.2694111764431\n",
      "| Loss std:   0.028622 |\n",
      "| Loss mean:   0.269411 |\n",
      "g_step: 5200 loss std/mean: 0.031604647636413574 0.2634815275669098\n",
      "| Loss std:   0.031605 |\n",
      "| Loss mean:   0.263482 |\n",
      "g_step: 5300 loss std/mean: 0.034992050379514694 0.26467642188072205\n",
      "| Loss std:   0.034992 |\n",
      "| Loss mean:   0.264676 |\n",
      "g_step: 5400 loss std/mean: 0.027309473603963852 0.2724296450614929\n",
      "| Loss std:   0.027309 |\n",
      "| Loss mean:   0.272430 |\n",
      "g_step: 5500 loss std/mean: 0.03403211385011673 0.265394002199173\n",
      "| Loss std:   0.034032 |\n",
      "| Loss mean:   0.265394 |\n",
      "g_step: 5600 loss std/mean: 0.026007629930973053 0.26563429832458496\n",
      "| Loss std:   0.026008 |\n",
      "| Loss mean:   0.265634 |\n",
      "g_step: 5700 loss std/mean: 0.038167230784893036 0.2712254524230957\n",
      "| Loss std:   0.038167 |\n",
      "| Loss mean:   0.271225 |\n",
      "g_step: 5800 loss std/mean: 0.030940745025873184 0.2685774862766266\n",
      "| Loss std:   0.030941 |\n",
      "| Loss mean:   0.268577 |\n",
      "g_step: 5900 loss std/mean: 0.03046700730919838 0.263004332780838\n",
      "| Loss std:   0.030467 |\n",
      "| Loss mean:   0.263004 |\n",
      "g_step: 6000 loss std/mean: 0.03292497247457504 0.2762693464756012\n",
      "| Loss std:   0.032925 |\n",
      "| Loss mean:   0.276269 |\n",
      "g_step: 6100 loss std/mean: 0.026432301849126816 0.26963135600090027\n",
      "| Loss std:   0.026432 |\n",
      "| Loss mean:   0.269631 |\n",
      "g_step: 6200 loss std/mean: 0.03086753375828266 0.26417219638824463\n",
      "| Loss std:   0.030868 |\n",
      "| Loss mean:   0.264172 |\n",
      "g_step: 6300 loss std/mean: 0.027822358533740044 0.26376470923423767\n",
      "| Loss std:   0.027822 |\n",
      "| Loss mean:   0.263765 |\n",
      "g_step: 6400 loss std/mean: 0.026060575619339943 0.26367244124412537\n",
      "| Loss std:   0.026061 |\n",
      "| Loss mean:   0.263672 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g_step: 6500 loss std/mean: 0.025067254900932312 0.26214441657066345\n",
      "| Loss std:   0.025067 |\n",
      "| Loss mean:   0.262144 |\n",
      "g_step: 6600 loss std/mean: 0.027244551107287407 0.2599848508834839\n",
      "| Loss std:   0.027245 |\n",
      "| Loss mean:   0.259985 |\n",
      "g_step: 6700 loss std/mean: 0.02627219818532467 0.25933071970939636\n",
      "| Loss std:   0.026272 |\n",
      "| Loss mean:   0.259331 |\n",
      "g_step: 6800 loss std/mean: 0.028893791139125824 0.2682610750198364\n",
      "| Loss std:   0.028894 |\n",
      "| Loss mean:   0.268261 |\n",
      "g_step: 6900 loss std/mean: 0.02811018005013466 0.2645304799079895\n",
      "| Loss std:   0.028110 |\n",
      "| Loss mean:   0.264530 |\n",
      "g_step: 7000 loss std/mean: 0.027167130261659622 0.2653965651988983\n",
      "| Loss std:   0.027167 |\n",
      "| Loss mean:   0.265397 |\n",
      "g_step: 7100 loss std/mean: 0.02941996417939663 0.26368725299835205\n",
      "| Loss std:   0.029420 |\n",
      "| Loss mean:   0.263687 |\n",
      "g_step: 7200 loss std/mean: 0.0272139273583889 0.2509707510471344\n",
      "| Loss std:   0.027214 |\n",
      "| Loss mean:   0.250971 |\n",
      "g_step: 7300 loss std/mean: 0.026602275669574738 0.2561468183994293\n",
      "| Loss std:   0.026602 |\n",
      "| Loss mean:   0.256147 |\n",
      "g_step: 7400 loss std/mean: 0.025665558874607086 0.2633495032787323\n",
      "| Loss std:   0.025666 |\n",
      "| Loss mean:   0.263350 |\n",
      "g_step: 7500 loss std/mean: 0.023522846400737762 0.26196423172950745\n",
      "| Loss std:   0.023523 |\n",
      "| Loss mean:   0.261964 |\n",
      "g_step: 7600 loss std/mean: 0.02704824134707451 0.27252131700515747\n",
      "| Loss std:   0.027048 |\n",
      "| Loss mean:   0.272521 |\n",
      "g_step: 7700 loss std/mean: 0.032431475818157196 0.2676140069961548\n",
      "| Loss std:   0.032431 |\n",
      "| Loss mean:   0.267614 |\n",
      "g_step: 7800 loss std/mean: 0.0292091965675354 0.2616802453994751\n",
      "| Loss std:   0.029209 |\n",
      "| Loss mean:   0.261680 |\n",
      "g_step: 7900 loss std/mean: 0.023133685812354088 0.26143917441368103\n",
      "| Loss std:   0.023134 |\n",
      "| Loss mean:   0.261439 |\n",
      "g_step: 8000 loss std/mean: 0.025786809623241425 0.2616952657699585\n",
      "| Loss std:   0.025787 |\n",
      "| Loss mean:   0.261695 |\n",
      "g_step: 8100 loss std/mean: 0.026381995528936386 0.2605593204498291\n",
      "| Loss std:   0.026382 |\n",
      "| Loss mean:   0.260559 |\n",
      "g_step: 8200 loss std/mean: 0.02553664520382881 0.2608773410320282\n",
      "| Loss std:   0.025537 |\n",
      "| Loss mean:   0.260877 |\n",
      "g_step: 8300 loss std/mean: 0.027423733845353127 0.25905469059944153\n",
      "| Loss std:   0.027424 |\n",
      "| Loss mean:   0.259055 |\n",
      "g_step: 8400 loss std/mean: 0.023858679458498955 0.2657221853733063\n",
      "| Loss std:   0.023859 |\n",
      "| Loss mean:   0.265722 |\n",
      "g_step: 8500 loss std/mean: 0.0322444848716259 0.26001498103141785\n",
      "| Loss std:   0.032244 |\n",
      "| Loss mean:   0.260015 |\n",
      "g_step: 8600 loss std/mean: 0.030344342812895775 0.266296923160553\n",
      "| Loss std:   0.030344 |\n",
      "| Loss mean:   0.266297 |\n",
      "g_step: 8700 loss std/mean: 0.026159750297665596 0.2571088373661041\n",
      "| Loss std:   0.026160 |\n",
      "| Loss mean:   0.257109 |\n",
      "g_step: 8800 loss std/mean: 0.032623909413814545 0.2600362300872803\n",
      "| Loss std:   0.032624 |\n",
      "| Loss mean:   0.260036 |\n",
      "g_step: 8900 loss std/mean: 0.027514886111021042 0.2554442882537842\n",
      "| Loss std:   0.027515 |\n",
      "| Loss mean:   0.255444 |\n",
      "g_step: 9000 loss std/mean: 0.02722259610891342 0.2539081871509552\n",
      "| Loss std:   0.027223 |\n",
      "| Loss mean:   0.253908 |\n",
      "g_step: 9100 loss std/mean: 0.02082240581512451 0.2611369788646698\n",
      "| Loss std:   0.020822 |\n",
      "| Loss mean:   0.261137 |\n",
      "g_step: 9200 loss std/mean: 0.0308192390948534 0.26161956787109375\n",
      "| Loss std:   0.030819 |\n",
      "| Loss mean:   0.261620 |\n",
      "g_step: 9300 loss std/mean: 0.027333315461874008 0.26319724321365356\n",
      "| Loss std:   0.027333 |\n",
      "| Loss mean:   0.263197 |\n",
      "g_step: 9400 loss std/mean: 0.02656436152756214 0.2557697296142578\n",
      "| Loss std:   0.026564 |\n",
      "| Loss mean:   0.255770 |\n",
      "g_step: 9500 loss std/mean: 0.02335621602833271 0.2654634416103363\n",
      "| Loss std:   0.023356 |\n",
      "| Loss mean:   0.265463 |\n",
      "g_step: 9600 loss std/mean: 0.024711189791560173 0.25853556394577026\n",
      "| Loss std:   0.024711 |\n",
      "| Loss mean:   0.258536 |\n",
      "g_step: 9700 loss std/mean: 0.026754235848784447 0.2613544166088104\n",
      "| Loss std:   0.026754 |\n",
      "| Loss mean:   0.261354 |\n",
      "g_step: 9800 loss std/mean: 0.02695895917713642 0.25254082679748535\n",
      "| Loss std:   0.026959 |\n",
      "| Loss mean:   0.252541 |\n",
      "g_step: 9900 loss std/mean: 0.027691613882780075 0.2558227479457855\n",
      "| Loss std:   0.027692 |\n",
      "| Loss mean:   0.255823 |\n",
      "g_step: 10000 loss std/mean: 0.025569399818778038 0.2652842104434967\n",
      "| Loss std:   0.025569 |\n",
      "| Loss mean:   0.265284 |\n",
      "\tValidation NWRMSLE  : 0.531013585284\n",
      "| Validation NWRMSLE:   0.531014 |\n",
      "\tValidation NWRMSLE_5: 0.522337348936\n",
      "| Validation NWRMSLE_5:   0.522337 |\n",
      "g_step: 10100 loss std/mean: 0.019635185599327087 0.2571485936641693\n",
      "| Loss std:   0.019635 |\n",
      "| Loss mean:   0.257149 |\n",
      "g_step: 10200 loss std/mean: 0.025689901784062386 0.25738847255706787\n",
      "| Loss std:   0.025690 |\n",
      "| Loss mean:   0.257388 |\n",
      "g_step: 10300 loss std/mean: 0.02601027861237526 0.2653365433216095\n",
      "| Loss std:   0.026010 |\n",
      "| Loss mean:   0.265337 |\n",
      "g_step: 10400 loss std/mean: 0.026887672021985054 0.2571757137775421\n",
      "| Loss std:   0.026888 |\n",
      "| Loss mean:   0.257176 |\n",
      "g_step: 10500 loss std/mean: 0.026324404403567314 0.2536579370498657\n",
      "| Loss std:   0.026324 |\n",
      "| Loss mean:   0.253658 |\n",
      "g_step: 10600 loss std/mean: 0.02768898941576481 0.2482966184616089\n",
      "| Loss std:   0.027689 |\n",
      "| Loss mean:   0.248297 |\n",
      "g_step: 10700 loss std/mean: 0.029018385335803032 0.2495632767677307\n",
      "| Loss std:   0.029018 |\n",
      "| Loss mean:   0.249563 |\n",
      "g_step: 10800 loss std/mean: 0.030015558004379272 0.2581072747707367\n",
      "| Loss std:   0.030016 |\n",
      "| Loss mean:   0.258107 |\n",
      "g_step: 10900 loss std/mean: 0.022686345502734184 0.2599003314971924\n",
      "| Loss std:   0.022686 |\n",
      "| Loss mean:   0.259900 |\n",
      "g_step: 11000 loss std/mean: 0.023725267499685287 0.25933268666267395\n",
      "| Loss std:   0.023725 |\n",
      "| Loss mean:   0.259333 |\n",
      "g_step: 11100 loss std/mean: 0.02816673368215561 0.2608422040939331\n",
      "| Loss std:   0.028167 |\n",
      "| Loss mean:   0.260842 |\n",
      "g_step: 11200 loss std/mean: 0.029668033123016357 0.2598087191581726\n",
      "| Loss std:   0.029668 |\n",
      "| Loss mean:   0.259809 |\n",
      "g_step: 11300 loss std/mean: 0.029433473944664 0.2578292191028595\n",
      "| Loss std:   0.029433 |\n",
      "| Loss mean:   0.257829 |\n",
      "g_step: 11400 loss std/mean: 0.024017155170440674 0.26026496291160583\n",
      "| Loss std:   0.024017 |\n",
      "| Loss mean:   0.260265 |\n",
      "g_step: 11500 loss std/mean: 0.02193344756960869 0.25301897525787354\n",
      "| Loss std:   0.021933 |\n",
      "| Loss mean:   0.253019 |\n",
      "g_step: 11600 loss std/mean: 0.020349768921732903 0.2583853006362915\n",
      "| Loss std:   0.020350 |\n",
      "| Loss mean:   0.258385 |\n",
      "g_step: 11700 loss std/mean: 0.027254456654191017 0.2568386197090149\n",
      "| Loss std:   0.027254 |\n",
      "| Loss mean:   0.256839 |\n",
      "g_step: 11800 loss std/mean: 0.02249240316450596 0.2620875835418701\n",
      "| Loss std:   0.022492 |\n",
      "| Loss mean:   0.262088 |\n",
      "g_step: 11900 loss std/mean: 0.027105478569865227 0.25674664974212646\n",
      "| Loss std:   0.027105 |\n",
      "| Loss mean:   0.256747 |\n",
      "g_step: 12000 loss std/mean: 0.026569178327918053 0.2524641454219818\n",
      "| Loss std:   0.026569 |\n",
      "| Loss mean:   0.252464 |\n",
      "g_step: 12100 loss std/mean: 0.022285768762230873 0.25619789958000183\n",
      "| Loss std:   0.022286 |\n",
      "| Loss mean:   0.256198 |\n",
      "g_step: 12200 loss std/mean: 0.024865049868822098 0.2540287673473358\n",
      "| Loss std:   0.024865 |\n",
      "| Loss mean:   0.254029 |\n",
      "g_step: 12300 loss std/mean: 0.02243969216942787 0.25831279158592224\n",
      "| Loss std:   0.022440 |\n",
      "| Loss mean:   0.258313 |\n",
      "g_step: 12400 loss std/mean: 0.030086705461144447 0.24692346155643463\n",
      "| Loss std:   0.030087 |\n",
      "| Loss mean:   0.246923 |\n",
      "g_step: 12500 loss std/mean: 0.024613024666905403 0.2503524720668793\n",
      "| Loss std:   0.024613 |\n",
      "| Loss mean:   0.250352 |\n",
      "g_step: 12600 loss std/mean: 0.02524528093636036 0.2522395849227905\n",
      "| Loss std:   0.025245 |\n",
      "| Loss mean:   0.252240 |\n",
      "g_step: 12700 loss std/mean: 0.023953473195433617 0.25591009855270386\n",
      "| Loss std:   0.023953 |\n",
      "| Loss mean:   0.255910 |\n",
      "g_step: 12800 loss std/mean: 0.024668753147125244 0.24796976149082184\n",
      "| Loss std:   0.024669 |\n",
      "| Loss mean:   0.247970 |\n",
      "g_step: 12900 loss std/mean: 0.021546635776758194 0.25570210814476013\n",
      "| Loss std:   0.021547 |\n",
      "| Loss mean:   0.255702 |\n",
      "g_step: 13000 loss std/mean: 0.02755073457956314 0.25027987360954285\n",
      "| Loss std:   0.027551 |\n",
      "| Loss mean:   0.250280 |\n",
      "g_step: 13100 loss std/mean: 0.024247854948043823 0.2523409128189087\n",
      "| Loss std:   0.024248 |\n",
      "| Loss mean:   0.252341 |\n",
      "g_step: 13200 loss std/mean: 0.023141726851463318 0.2567504644393921\n",
      "| Loss std:   0.023142 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Loss mean:   0.256750 |\n",
      "g_step: 13300 loss std/mean: 0.027544749900698662 0.25215432047843933\n",
      "| Loss std:   0.027545 |\n",
      "| Loss mean:   0.252154 |\n",
      "g_step: 13400 loss std/mean: 0.021215159446001053 0.2512039244174957\n",
      "| Loss std:   0.021215 |\n",
      "| Loss mean:   0.251204 |\n",
      "g_step: 13500 loss std/mean: 0.027414502575993538 0.2552782893180847\n",
      "| Loss std:   0.027415 |\n",
      "| Loss mean:   0.255278 |\n",
      "g_step: 13600 loss std/mean: 0.02534320391714573 0.2542404532432556\n",
      "| Loss std:   0.025343 |\n",
      "| Loss mean:   0.254240 |\n",
      "g_step: 13700 loss std/mean: 0.02364587038755417 0.2565014064311981\n",
      "| Loss std:   0.023646 |\n",
      "| Loss mean:   0.256501 |\n",
      "g_step: 13800 loss std/mean: 0.02410855144262314 0.250861793756485\n",
      "| Loss std:   0.024109 |\n",
      "| Loss mean:   0.250862 |\n",
      "g_step: 13900 loss std/mean: 0.024232111871242523 0.24893340468406677\n",
      "| Loss std:   0.024232 |\n",
      "| Loss mean:   0.248933 |\n",
      "g_step: 14000 loss std/mean: 0.027179306373000145 0.2537468373775482\n",
      "| Loss std:   0.027179 |\n",
      "| Loss mean:   0.253747 |\n",
      "g_step: 14100 loss std/mean: 0.02456703782081604 0.24963438510894775\n",
      "| Loss std:   0.024567 |\n",
      "| Loss mean:   0.249634 |\n",
      "g_step: 14200 loss std/mean: 0.026621177792549133 0.24657541513442993\n",
      "| Loss std:   0.026621 |\n",
      "| Loss mean:   0.246575 |\n",
      "g_step: 14300 loss std/mean: 0.024061216041445732 0.2515856623649597\n",
      "| Loss std:   0.024061 |\n",
      "| Loss mean:   0.251586 |\n",
      "g_step: 14400 loss std/mean: 0.024990346282720566 0.249308243393898\n",
      "| Loss std:   0.024990 |\n",
      "| Loss mean:   0.249308 |\n",
      "g_step: 14500 loss std/mean: 0.021616926416754723 0.2497486174106598\n",
      "| Loss std:   0.021617 |\n",
      "| Loss mean:   0.249749 |\n",
      "g_step: 14600 loss std/mean: 0.024107014760375023 0.2530653476715088\n",
      "| Loss std:   0.024107 |\n",
      "| Loss mean:   0.253065 |\n",
      "g_step: 14700 loss std/mean: 0.027164435014128685 0.24797430634498596\n",
      "| Loss std:   0.027164 |\n",
      "| Loss mean:   0.247974 |\n",
      "g_step: 14800 loss std/mean: 0.022438766434788704 0.25144442915916443\n",
      "| Loss std:   0.022439 |\n",
      "| Loss mean:   0.251444 |\n",
      "g_step: 14900 loss std/mean: 0.025538815185427666 0.2536904513835907\n",
      "| Loss std:   0.025539 |\n",
      "| Loss mean:   0.253690 |\n",
      "g_step: 15000 loss std/mean: 0.024701131507754326 0.2554492652416229\n",
      "| Loss std:   0.024701 |\n",
      "| Loss mean:   0.255449 |\n",
      "\tValidation NWRMSLE  : 0.527264400129\n",
      "| Validation NWRMSLE:   0.527264 |\n",
      "\tValidation NWRMSLE_5: 0.516496587858\n",
      "| Validation NWRMSLE_5:   0.516497 |\n",
      "g_step: 15100 loss std/mean: 0.026815950870513916 0.24837130308151245\n",
      "| Loss std:   0.026816 |\n",
      "| Loss mean:   0.248371 |\n",
      "g_step: 15200 loss std/mean: 0.02145823836326599 0.2508164346218109\n",
      "| Loss std:   0.021458 |\n",
      "| Loss mean:   0.250816 |\n",
      "g_step: 15300 loss std/mean: 0.02486215904355049 0.2402290552854538\n",
      "| Loss std:   0.024862 |\n",
      "| Loss mean:   0.240229 |\n",
      "g_step: 15400 loss std/mean: 0.027079256251454353 0.254912793636322\n",
      "| Loss std:   0.027079 |\n",
      "| Loss mean:   0.254913 |\n",
      "g_step: 15500 loss std/mean: 0.026231126859784126 0.2538365423679352\n",
      "| Loss std:   0.026231 |\n",
      "| Loss mean:   0.253837 |\n",
      "g_step: 15600 loss std/mean: 0.024062106385827065 0.2527318298816681\n",
      "| Loss std:   0.024062 |\n",
      "| Loss mean:   0.252732 |\n",
      "g_step: 15700 loss std/mean: 0.01941227726638317 0.25379711389541626\n",
      "| Loss std:   0.019412 |\n",
      "| Loss mean:   0.253797 |\n",
      "g_step: 15800 loss std/mean: 0.0240611769258976 0.2445591390132904\n",
      "| Loss std:   0.024061 |\n",
      "| Loss mean:   0.244559 |\n",
      "g_step: 15900 loss std/mean: 0.024320172145962715 0.24925853312015533\n",
      "| Loss std:   0.024320 |\n",
      "| Loss mean:   0.249259 |\n",
      "g_step: 16000 loss std/mean: 0.023409437388181686 0.2531370222568512\n",
      "| Loss std:   0.023409 |\n",
      "| Loss mean:   0.253137 |\n",
      "g_step: 16100 loss std/mean: 0.022509105503559113 0.24587106704711914\n",
      "| Loss std:   0.022509 |\n",
      "| Loss mean:   0.245871 |\n",
      "g_step: 16200 loss std/mean: 0.021984504535794258 0.2531600296497345\n",
      "| Loss std:   0.021985 |\n",
      "| Loss mean:   0.253160 |\n",
      "g_step: 16300 loss std/mean: 0.025774074718356133 0.25840598344802856\n",
      "| Loss std:   0.025774 |\n",
      "| Loss mean:   0.258406 |\n",
      "g_step: 16400 loss std/mean: 0.02352999709546566 0.24623727798461914\n",
      "| Loss std:   0.023530 |\n",
      "| Loss mean:   0.246237 |\n",
      "g_step: 16500 loss std/mean: 0.023270053789019585 0.24571123719215393\n",
      "| Loss std:   0.023270 |\n",
      "| Loss mean:   0.245711 |\n",
      "g_step: 16600 loss std/mean: 0.02994995191693306 0.2473258227109909\n",
      "| Loss std:   0.029950 |\n",
      "| Loss mean:   0.247326 |\n",
      "g_step: 16700 loss std/mean: 0.023364923894405365 0.24950198829174042\n",
      "| Loss std:   0.023365 |\n",
      "| Loss mean:   0.249502 |\n",
      "g_step: 16800 loss std/mean: 0.02227029763162136 0.24860897660255432\n",
      "| Loss std:   0.022270 |\n",
      "| Loss mean:   0.248609 |\n",
      "g_step: 16900 loss std/mean: 0.026410775259137154 0.2510136365890503\n",
      "| Loss std:   0.026411 |\n",
      "| Loss mean:   0.251014 |\n",
      "g_step: 17000 loss std/mean: 0.024254782125353813 0.2455960512161255\n",
      "| Loss std:   0.024255 |\n",
      "| Loss mean:   0.245596 |\n",
      "g_step: 17100 loss std/mean: 0.022958870977163315 0.24944093823432922\n",
      "| Loss std:   0.022959 |\n",
      "| Loss mean:   0.249441 |\n",
      "g_step: 17200 loss std/mean: 0.028068115934729576 0.2532259523868561\n",
      "| Loss std:   0.028068 |\n",
      "| Loss mean:   0.253226 |\n",
      "g_step: 17300 loss std/mean: 0.025695646181702614 0.24461188912391663\n",
      "| Loss std:   0.025696 |\n",
      "| Loss mean:   0.244612 |\n",
      "g_step: 17400 loss std/mean: 0.021811313927173615 0.25131794810295105\n",
      "| Loss std:   0.021811 |\n",
      "| Loss mean:   0.251318 |\n",
      "g_step: 17500 loss std/mean: 0.024410083889961243 0.24400018155574799\n",
      "| Loss std:   0.024410 |\n",
      "| Loss mean:   0.244000 |\n",
      "g_step: 17600 loss std/mean: 0.02771105244755745 0.25004446506500244\n",
      "| Loss std:   0.027711 |\n",
      "| Loss mean:   0.250044 |\n",
      "g_step: 17700 loss std/mean: 0.02748759090900421 0.25001662969589233\n",
      "| Loss std:   0.027488 |\n",
      "| Loss mean:   0.250017 |\n",
      "g_step: 17800 loss std/mean: 0.025588765740394592 0.2495766282081604\n",
      "| Loss std:   0.025589 |\n",
      "| Loss mean:   0.249577 |\n",
      "g_step: 17900 loss std/mean: 0.029417412355542183 0.24933074414730072\n",
      "| Loss std:   0.029417 |\n",
      "| Loss mean:   0.249331 |\n",
      "g_step: 18000 loss std/mean: 0.026119939982891083 0.24508175253868103\n",
      "| Loss std:   0.026120 |\n",
      "| Loss mean:   0.245082 |\n",
      "g_step: 18100 loss std/mean: 0.023423081263899803 0.2521406412124634\n",
      "| Loss std:   0.023423 |\n",
      "| Loss mean:   0.252141 |\n",
      "g_step: 18200 loss std/mean: 0.022168822586536407 0.2519999146461487\n",
      "| Loss std:   0.022169 |\n",
      "| Loss mean:   0.252000 |\n",
      "g_step: 18300 loss std/mean: 0.022825567051768303 0.2514665722846985\n",
      "| Loss std:   0.022826 |\n",
      "| Loss mean:   0.251467 |\n",
      "g_step: 18400 loss std/mean: 0.024453619495034218 0.2524796426296234\n",
      "| Loss std:   0.024454 |\n",
      "| Loss mean:   0.252480 |\n",
      "g_step: 18500 loss std/mean: 0.021747969090938568 0.24579735100269318\n",
      "| Loss std:   0.021748 |\n",
      "| Loss mean:   0.245797 |\n",
      "g_step: 18600 loss std/mean: 0.024831144139170647 0.2540229856967926\n",
      "| Loss std:   0.024831 |\n",
      "| Loss mean:   0.254023 |\n",
      "g_step: 18700 loss std/mean: 0.023976925760507584 0.254706472158432\n",
      "| Loss std:   0.023977 |\n",
      "| Loss mean:   0.254706 |\n",
      "g_step: 18800 loss std/mean: 0.02248166687786579 0.25110703706741333\n",
      "| Loss std:   0.022482 |\n",
      "| Loss mean:   0.251107 |\n",
      "g_step: 18900 loss std/mean: 0.026400117203593254 0.2482084333896637\n",
      "| Loss std:   0.026400 |\n",
      "| Loss mean:   0.248208 |\n",
      "g_step: 19000 loss std/mean: 0.023615006357431412 0.2507871091365814\n",
      "| Loss std:   0.023615 |\n",
      "| Loss mean:   0.250787 |\n",
      "g_step: 19100 loss std/mean: 0.024267010390758514 0.24366526305675507\n",
      "| Loss std:   0.024267 |\n",
      "| Loss mean:   0.243665 |\n",
      "g_step: 19200 loss std/mean: 0.021024595946073532 0.24854068458080292\n",
      "| Loss std:   0.021025 |\n",
      "| Loss mean:   0.248541 |\n",
      "g_step: 19300 loss std/mean: 0.02405433915555477 0.24839457869529724\n",
      "| Loss std:   0.024054 |\n",
      "| Loss mean:   0.248395 |\n",
      "g_step: 19400 loss std/mean: 0.023529985919594765 0.24550923705101013\n",
      "| Loss std:   0.023530 |\n",
      "| Loss mean:   0.245509 |\n",
      "g_step: 19500 loss std/mean: 0.020883018150925636 0.2434033900499344\n",
      "| Loss std:   0.020883 |\n",
      "| Loss mean:   0.243403 |\n",
      "g_step: 19600 loss std/mean: 0.025857988744974136 0.24971193075180054\n",
      "| Loss std:   0.025858 |\n",
      "| Loss mean:   0.249712 |\n",
      "g_step: 19700 loss std/mean: 0.024353312328457832 0.25203976035118103\n",
      "| Loss std:   0.024353 |\n",
      "| Loss mean:   0.252040 |\n",
      "g_step: 19800 loss std/mean: 0.025666171684861183 0.24984827637672424\n",
      "| Loss std:   0.025666 |\n",
      "| Loss mean:   0.249848 |\n",
      "g_step: 19900 loss std/mean: 0.025161491706967354 0.24345576763153076\n",
      "| Loss std:   0.025161 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Loss mean:   0.243456 |\n",
      "g_step: 20000 loss std/mean: 0.022439900785684586 0.2522207498550415\n",
      "| Loss std:   0.022440 |\n",
      "| Loss mean:   0.252221 |\n",
      "\tValidation NWRMSLE  : 0.523939475808\n",
      "| Validation NWRMSLE:   0.523939 |\n",
      "\tValidation NWRMSLE_5: 0.515369246757\n",
      "| Validation NWRMSLE_5:   0.515369 |\n",
      "g_step: 20100 loss std/mean: 0.025669431313872337 0.2513468861579895\n",
      "| Loss std:   0.025669 |\n",
      "| Loss mean:   0.251347 |\n",
      "g_step: 20200 loss std/mean: 0.026155240833759308 0.24153493344783783\n",
      "| Loss std:   0.026155 |\n",
      "| Loss mean:   0.241535 |\n",
      "g_step: 20300 loss std/mean: 0.022408422082662582 0.2507874071598053\n",
      "| Loss std:   0.022408 |\n",
      "| Loss mean:   0.250787 |\n",
      "g_step: 20400 loss std/mean: 0.0222614873200655 0.24539253115653992\n",
      "| Loss std:   0.022261 |\n",
      "| Loss mean:   0.245393 |\n",
      "g_step: 20500 loss std/mean: 0.021970491856336594 0.2529306709766388\n",
      "| Loss std:   0.021970 |\n",
      "| Loss mean:   0.252931 |\n",
      "g_step: 20600 loss std/mean: 0.02370951697230339 0.24536117911338806\n",
      "| Loss std:   0.023710 |\n",
      "| Loss mean:   0.245361 |\n",
      "g_step: 20700 loss std/mean: 0.031203512102365494 0.2500041425228119\n",
      "| Loss std:   0.031204 |\n",
      "| Loss mean:   0.250004 |\n",
      "g_step: 20800 loss std/mean: 0.022333763539791107 0.2488899677991867\n",
      "| Loss std:   0.022334 |\n",
      "| Loss mean:   0.248890 |\n",
      "g_step: 20900 loss std/mean: 0.027263127267360687 0.24527542293071747\n",
      "| Loss std:   0.027263 |\n",
      "| Loss mean:   0.245275 |\n",
      "g_step: 21000 loss std/mean: 0.023794377222657204 0.2524525821208954\n",
      "| Loss std:   0.023794 |\n",
      "| Loss mean:   0.252453 |\n",
      "g_step: 21100 loss std/mean: 0.022748155519366264 0.24775360524654388\n",
      "| Loss std:   0.022748 |\n",
      "| Loss mean:   0.247754 |\n",
      "g_step: 21200 loss std/mean: 0.024936728179454803 0.24735622107982635\n",
      "| Loss std:   0.024937 |\n",
      "| Loss mean:   0.247356 |\n",
      "g_step: 21300 loss std/mean: 0.026528263464570045 0.24618269503116608\n",
      "| Loss std:   0.026528 |\n",
      "| Loss mean:   0.246183 |\n",
      "g_step: 21400 loss std/mean: 0.022727007046341896 0.25038257241249084\n",
      "| Loss std:   0.022727 |\n",
      "| Loss mean:   0.250383 |\n",
      "g_step: 21500 loss std/mean: 0.0247916579246521 0.24999597668647766\n",
      "| Loss std:   0.024792 |\n",
      "| Loss mean:   0.249996 |\n",
      "g_step: 21600 loss std/mean: 0.02269904315471649 0.24423108994960785\n",
      "| Loss std:   0.022699 |\n",
      "| Loss mean:   0.244231 |\n",
      "g_step: 21700 loss std/mean: 0.02413034252822399 0.2422320395708084\n",
      "| Loss std:   0.024130 |\n",
      "| Loss mean:   0.242232 |\n",
      "g_step: 21800 loss std/mean: 0.020820654928684235 0.25249823927879333\n",
      "| Loss std:   0.020821 |\n",
      "| Loss mean:   0.252498 |\n",
      "g_step: 21900 loss std/mean: 0.023370154201984406 0.23982347548007965\n",
      "| Loss std:   0.023370 |\n",
      "| Loss mean:   0.239823 |\n",
      "g_step: 22000 loss std/mean: 0.023357009515166283 0.2438126504421234\n",
      "| Loss std:   0.023357 |\n",
      "| Loss mean:   0.243813 |\n",
      "g_step: 22100 loss std/mean: 0.023669682443141937 0.25131306052207947\n",
      "| Loss std:   0.023670 |\n",
      "| Loss mean:   0.251313 |\n",
      "g_step: 22200 loss std/mean: 0.02475755475461483 0.2450641393661499\n",
      "| Loss std:   0.024758 |\n",
      "| Loss mean:   0.245064 |\n",
      "g_step: 22300 loss std/mean: 0.020599454641342163 0.24724777042865753\n",
      "| Loss std:   0.020599 |\n",
      "| Loss mean:   0.247248 |\n",
      "g_step: 22400 loss std/mean: 0.024473879486322403 0.24772101640701294\n",
      "| Loss std:   0.024474 |\n",
      "| Loss mean:   0.247721 |\n",
      "g_step: 22500 loss std/mean: 0.026468155905604362 0.24751724302768707\n",
      "| Loss std:   0.026468 |\n",
      "| Loss mean:   0.247517 |\n",
      "g_step: 22600 loss std/mean: 0.023104369640350342 0.24400290846824646\n",
      "| Loss std:   0.023104 |\n",
      "| Loss mean:   0.244003 |\n",
      "g_step: 22700 loss std/mean: 0.018795201554894447 0.25499483942985535\n",
      "| Loss std:   0.018795 |\n",
      "| Loss mean:   0.254995 |\n",
      "g_step: 22800 loss std/mean: 0.023415694013237953 0.24643197655677795\n",
      "| Loss std:   0.023416 |\n",
      "| Loss mean:   0.246432 |\n",
      "g_step: 22900 loss std/mean: 0.018680613487958908 0.24412816762924194\n",
      "| Loss std:   0.018681 |\n",
      "| Loss mean:   0.244128 |\n",
      "g_step: 23000 loss std/mean: 0.021936213597655296 0.24412374198436737\n",
      "| Loss std:   0.021936 |\n",
      "| Loss mean:   0.244124 |\n",
      "g_step: 23100 loss std/mean: 0.022966712713241577 0.24660047888755798\n",
      "| Loss std:   0.022967 |\n",
      "| Loss mean:   0.246600 |\n",
      "g_step: 23200 loss std/mean: 0.020122574642300606 0.250515341758728\n",
      "| Loss std:   0.020123 |\n",
      "| Loss mean:   0.250515 |\n",
      "g_step: 23300 loss std/mean: 0.024908030405640602 0.24307610094547272\n",
      "| Loss std:   0.024908 |\n",
      "| Loss mean:   0.243076 |\n",
      "g_step: 23400 loss std/mean: 0.020737016573548317 0.2455650269985199\n",
      "| Loss std:   0.020737 |\n",
      "| Loss mean:   0.245565 |\n",
      "g_step: 23500 loss std/mean: 0.024667683988809586 0.2475394308567047\n",
      "| Loss std:   0.024668 |\n",
      "| Loss mean:   0.247539 |\n",
      "g_step: 23600 loss std/mean: 0.024737194180488586 0.24900352954864502\n",
      "| Loss std:   0.024737 |\n",
      "| Loss mean:   0.249004 |\n",
      "g_step: 23700 loss std/mean: 0.021312113851308823 0.24768856167793274\n",
      "| Loss std:   0.021312 |\n",
      "| Loss mean:   0.247689 |\n",
      "g_step: 23800 loss std/mean: 0.021096551790833473 0.24960345029830933\n",
      "| Loss std:   0.021097 |\n",
      "| Loss mean:   0.249603 |\n",
      "g_step: 23900 loss std/mean: 0.02637648582458496 0.24869701266288757\n",
      "| Loss std:   0.026376 |\n",
      "| Loss mean:   0.248697 |\n",
      "g_step: 24000 loss std/mean: 0.020873021334409714 0.24196836352348328\n",
      "| Loss std:   0.020873 |\n",
      "| Loss mean:   0.241968 |\n",
      "g_step: 24100 loss std/mean: 0.01803925260901451 0.2507222294807434\n",
      "| Loss std:   0.018039 |\n",
      "| Loss mean:   0.250722 |\n",
      "g_step: 24200 loss std/mean: 0.02362445555627346 0.24391482770442963\n",
      "| Loss std:   0.023624 |\n",
      "| Loss mean:   0.243915 |\n",
      "g_step: 24300 loss std/mean: 0.024335291236639023 0.24759304523468018\n",
      "| Loss std:   0.024335 |\n",
      "| Loss mean:   0.247593 |\n",
      "g_step: 24400 loss std/mean: 0.02264326438307762 0.24681252241134644\n",
      "| Loss std:   0.022643 |\n",
      "| Loss mean:   0.246813 |\n",
      "g_step: 24500 loss std/mean: 0.02188468538224697 0.23946435749530792\n",
      "| Loss std:   0.021885 |\n",
      "| Loss mean:   0.239464 |\n",
      "g_step: 24600 loss std/mean: 0.02317848987877369 0.243332639336586\n",
      "| Loss std:   0.023178 |\n",
      "| Loss mean:   0.243333 |\n",
      "g_step: 24700 loss std/mean: 0.02146059088408947 0.24391169846057892\n",
      "| Loss std:   0.021461 |\n",
      "| Loss mean:   0.243912 |\n",
      "g_step: 24800 loss std/mean: 0.024054938927292824 0.24554257094860077\n",
      "| Loss std:   0.024055 |\n",
      "| Loss mean:   0.245543 |\n",
      "g_step: 24900 loss std/mean: 0.024810373783111572 0.24802358448505402\n",
      "| Loss std:   0.024810 |\n",
      "| Loss mean:   0.248024 |\n",
      "g_step: 25000 loss std/mean: 0.01892601139843464 0.24358133971691132\n",
      "| Loss std:   0.018926 |\n",
      "| Loss mean:   0.243581 |\n",
      "\tValidation NWRMSLE  : 0.522954520125\n",
      "| Validation NWRMSLE:   0.522955 |\n",
      "\tValidation NWRMSLE_5: 0.513703514013\n",
      "| Validation NWRMSLE_5:   0.513704 |\n",
      "g_step: 25100 loss std/mean: 0.02273530326783657 0.24779477715492249\n",
      "| Loss std:   0.022735 |\n",
      "| Loss mean:   0.247795 |\n",
      "g_step: 25200 loss std/mean: 0.02486908994615078 0.2501043975353241\n",
      "| Loss std:   0.024869 |\n",
      "| Loss mean:   0.250104 |\n",
      "g_step: 25300 loss std/mean: 0.026456840336322784 0.24861976504325867\n",
      "| Loss std:   0.026457 |\n",
      "| Loss mean:   0.248620 |\n",
      "g_step: 25400 loss std/mean: 0.022494278848171234 0.23974239826202393\n",
      "| Loss std:   0.022494 |\n",
      "| Loss mean:   0.239742 |\n",
      "g_step: 25500 loss std/mean: 0.022862229496240616 0.24327422678470612\n",
      "| Loss std:   0.022862 |\n",
      "| Loss mean:   0.243274 |\n",
      "g_step: 25600 loss std/mean: 0.019333481788635254 0.2474648803472519\n",
      "| Loss std:   0.019333 |\n",
      "| Loss mean:   0.247465 |\n",
      "g_step: 25700 loss std/mean: 0.022412598133087158 0.24359741806983948\n",
      "| Loss std:   0.022413 |\n",
      "| Loss mean:   0.243597 |\n",
      "g_step: 25800 loss std/mean: 0.020531034097075462 0.24305696785449982\n",
      "| Loss std:   0.020531 |\n",
      "| Loss mean:   0.243057 |\n",
      "g_step: 25900 loss std/mean: 0.025489743798971176 0.2459082454442978\n",
      "| Loss std:   0.025490 |\n",
      "| Loss mean:   0.245908 |\n",
      "g_step: 26000 loss std/mean: 0.023097902536392212 0.2443784475326538\n",
      "| Loss std:   0.023098 |\n",
      "| Loss mean:   0.244378 |\n",
      "g_step: 26100 loss std/mean: 0.02303069643676281 0.2444133758544922\n",
      "| Loss std:   0.023031 |\n",
      "| Loss mean:   0.244413 |\n",
      "g_step: 26200 loss std/mean: 0.023601550608873367 0.24791504442691803\n",
      "| Loss std:   0.023602 |\n",
      "| Loss mean:   0.247915 |\n",
      "g_step: 26300 loss std/mean: 0.024692565202713013 0.24433088302612305\n",
      "| Loss std:   0.024693 |\n",
      "| Loss mean:   0.244331 |\n",
      "g_step: 26400 loss std/mean: 0.02637891098856926 0.24208779633045197\n",
      "| Loss std:   0.026379 |\n",
      "| Loss mean:   0.242088 |\n",
      "g_step: 26500 loss std/mean: 0.022552013397216797 0.24371294677257538\n",
      "| Loss std:   0.022552 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Loss mean:   0.243713 |\n",
      "g_step: 26600 loss std/mean: 0.02323411963880062 0.2415548861026764\n",
      "| Loss std:   0.023234 |\n",
      "| Loss mean:   0.241555 |\n",
      "g_step: 26700 loss std/mean: 0.02267555706202984 0.24267007410526276\n",
      "| Loss std:   0.022676 |\n",
      "| Loss mean:   0.242670 |\n",
      "g_step: 26800 loss std/mean: 0.025943875312805176 0.2490420937538147\n",
      "| Loss std:   0.025944 |\n",
      "| Loss mean:   0.249042 |\n",
      "g_step: 26900 loss std/mean: 0.02264736406505108 0.25083667039871216\n",
      "| Loss std:   0.022647 |\n",
      "| Loss mean:   0.250837 |\n",
      "g_step: 27000 loss std/mean: 0.020102938637137413 0.2471204251050949\n",
      "| Loss std:   0.020103 |\n",
      "| Loss mean:   0.247120 |\n",
      "g_step: 27100 loss std/mean: 0.0191416684538126 0.24793130159378052\n",
      "| Loss std:   0.019142 |\n",
      "| Loss mean:   0.247931 |\n",
      "g_step: 27200 loss std/mean: 0.020718693733215332 0.24984313547611237\n",
      "| Loss std:   0.020719 |\n",
      "| Loss mean:   0.249843 |\n",
      "g_step: 27300 loss std/mean: 0.020589999854564667 0.24601806700229645\n",
      "| Loss std:   0.020590 |\n",
      "| Loss mean:   0.246018 |\n",
      "g_step: 27400 loss std/mean: 0.019374903291463852 0.24741880595684052\n",
      "| Loss std:   0.019375 |\n",
      "| Loss mean:   0.247419 |\n",
      "g_step: 27500 loss std/mean: 0.024409325793385506 0.2410864233970642\n",
      "| Loss std:   0.024409 |\n",
      "| Loss mean:   0.241086 |\n",
      "g_step: 27600 loss std/mean: 0.02189118228852749 0.23957307636737823\n",
      "| Loss std:   0.021891 |\n",
      "| Loss mean:   0.239573 |\n",
      "g_step: 27700 loss std/mean: 0.0226421020925045 0.2412334382534027\n",
      "| Loss std:   0.022642 |\n",
      "| Loss mean:   0.241233 |\n",
      "g_step: 27800 loss std/mean: 0.02266494370996952 0.24156036972999573\n",
      "| Loss std:   0.022665 |\n",
      "| Loss mean:   0.241560 |\n",
      "g_step: 27900 loss std/mean: 0.02341308817267418 0.24272538721561432\n",
      "| Loss std:   0.023413 |\n",
      "| Loss mean:   0.242725 |\n",
      "g_step: 28000 loss std/mean: 0.023612959310412407 0.24211150407791138\n",
      "| Loss std:   0.023613 |\n",
      "| Loss mean:   0.242112 |\n",
      "g_step: 28100 loss std/mean: 0.022114628925919533 0.24871975183486938\n",
      "| Loss std:   0.022115 |\n",
      "| Loss mean:   0.248720 |\n",
      "g_step: 28200 loss std/mean: 0.023320326581597328 0.24026280641555786\n",
      "| Loss std:   0.023320 |\n",
      "| Loss mean:   0.240263 |\n",
      "g_step: 28300 loss std/mean: 0.024463362991809845 0.2471780776977539\n",
      "| Loss std:   0.024463 |\n",
      "| Loss mean:   0.247178 |\n",
      "g_step: 28400 loss std/mean: 0.021821249276399612 0.2486182004213333\n",
      "| Loss std:   0.021821 |\n",
      "| Loss mean:   0.248618 |\n",
      "g_step: 28500 loss std/mean: 0.019010869786143303 0.2519114017486572\n",
      "| Loss std:   0.019011 |\n",
      "| Loss mean:   0.251911 |\n",
      "g_step: 28600 loss std/mean: 0.018673475831747055 0.24881334602832794\n",
      "| Loss std:   0.018673 |\n",
      "| Loss mean:   0.248813 |\n",
      "g_step: 28700 loss std/mean: 0.022382184863090515 0.24624881148338318\n",
      "| Loss std:   0.022382 |\n",
      "| Loss mean:   0.246249 |\n",
      "g_step: 28800 loss std/mean: 0.02002130262553692 0.24423931539058685\n",
      "| Loss std:   0.020021 |\n",
      "| Loss mean:   0.244239 |\n",
      "g_step: 28900 loss std/mean: 0.020268449559807777 0.24282878637313843\n",
      "| Loss std:   0.020268 |\n",
      "| Loss mean:   0.242829 |\n",
      "g_step: 29000 loss std/mean: 0.025026286020874977 0.24340419471263885\n",
      "| Loss std:   0.025026 |\n",
      "| Loss mean:   0.243404 |\n",
      "g_step: 29100 loss std/mean: 0.023583272472023964 0.24265161156654358\n",
      "| Loss std:   0.023583 |\n",
      "| Loss mean:   0.242652 |\n",
      "g_step: 29200 loss std/mean: 0.021070700138807297 0.2440294474363327\n",
      "| Loss std:   0.021071 |\n",
      "| Loss mean:   0.244029 |\n",
      "g_step: 29300 loss std/mean: 0.021649252623319626 0.23773907124996185\n",
      "| Loss std:   0.021649 |\n",
      "| Loss mean:   0.237739 |\n",
      "g_step: 29400 loss std/mean: 0.02249930612742901 0.24376161396503448\n",
      "| Loss std:   0.022499 |\n",
      "| Loss mean:   0.243762 |\n",
      "g_step: 29500 loss std/mean: 0.021472979336977005 0.24383479356765747\n",
      "| Loss std:   0.021473 |\n",
      "| Loss mean:   0.243835 |\n",
      "g_step: 29600 loss std/mean: 0.018887387588620186 0.24245744943618774\n",
      "| Loss std:   0.018887 |\n",
      "| Loss mean:   0.242457 |\n",
      "g_step: 29700 loss std/mean: 0.02579033561050892 0.24628262221813202\n",
      "| Loss std:   0.025790 |\n",
      "| Loss mean:   0.246283 |\n",
      "g_step: 29800 loss std/mean: 0.019436918199062347 0.24217095971107483\n",
      "| Loss std:   0.019437 |\n",
      "| Loss mean:   0.242171 |\n",
      "g_step: 29900 loss std/mean: 0.024918371811509132 0.24444936215877533\n",
      "| Loss std:   0.024918 |\n",
      "| Loss mean:   0.244449 |\n",
      "g_step: 30000 loss std/mean: 0.01967720501124859 0.24184437096118927\n",
      "| Loss std:   0.019677 |\n",
      "| Loss mean:   0.241844 |\n",
      "\tValidation NWRMSLE  : 0.524390297745\n",
      "| Validation NWRMSLE:   0.524390 |\n",
      "\tValidation NWRMSLE_5: 0.512249440688\n",
      "| Validation NWRMSLE_5:   0.512249 |\n",
      "g_step: 30100 loss std/mean: 0.022923249751329422 0.23984648287296295\n",
      "| Loss std:   0.022923 |\n",
      "| Loss mean:   0.239846 |\n",
      "g_step: 30200 loss std/mean: 0.025993837043642998 0.24137195944786072\n",
      "| Loss std:   0.025994 |\n",
      "| Loss mean:   0.241372 |\n",
      "g_step: 30300 loss std/mean: 0.022665362805128098 0.24609312415122986\n",
      "| Loss std:   0.022665 |\n",
      "| Loss mean:   0.246093 |\n",
      "g_step: 30400 loss std/mean: 0.023136695846915245 0.2440580576658249\n",
      "| Loss std:   0.023137 |\n",
      "| Loss mean:   0.244058 |\n",
      "g_step: 30500 loss std/mean: 0.020984206348657608 0.24405989050865173\n",
      "| Loss std:   0.020984 |\n",
      "| Loss mean:   0.244060 |\n",
      "g_step: 30600 loss std/mean: 0.02380218915641308 0.2422528862953186\n",
      "| Loss std:   0.023802 |\n",
      "| Loss mean:   0.242253 |\n",
      "g_step: 30700 loss std/mean: 0.0222332663834095 0.24393680691719055\n",
      "| Loss std:   0.022233 |\n",
      "| Loss mean:   0.243937 |\n",
      "g_step: 30800 loss std/mean: 0.023378603160381317 0.2478572130203247\n",
      "| Loss std:   0.023379 |\n",
      "| Loss mean:   0.247857 |\n",
      "g_step: 30900 loss std/mean: 0.02163049392402172 0.24269632995128632\n",
      "| Loss std:   0.021630 |\n",
      "| Loss mean:   0.242696 |\n",
      "g_step: 31000 loss std/mean: 0.020203907042741776 0.24567289650440216\n",
      "| Loss std:   0.020204 |\n",
      "| Loss mean:   0.245673 |\n",
      "g_step: 31100 loss std/mean: 0.022189466282725334 0.24212555587291718\n",
      "| Loss std:   0.022189 |\n",
      "| Loss mean:   0.242126 |\n",
      "g_step: 31200 loss std/mean: 0.02343996986746788 0.24303245544433594\n",
      "| Loss std:   0.023440 |\n",
      "| Loss mean:   0.243032 |\n",
      "g_step: 31300 loss std/mean: 0.020934004336595535 0.2407628446817398\n",
      "| Loss std:   0.020934 |\n",
      "| Loss mean:   0.240763 |\n",
      "g_step: 31400 loss std/mean: 0.018670296296477318 0.24616290628910065\n",
      "| Loss std:   0.018670 |\n",
      "| Loss mean:   0.246163 |\n",
      "g_step: 31500 loss std/mean: 0.02163863554596901 0.2408469319343567\n",
      "| Loss std:   0.021639 |\n",
      "| Loss mean:   0.240847 |\n",
      "g_step: 31600 loss std/mean: 0.022899264469742775 0.2470541149377823\n",
      "| Loss std:   0.022899 |\n",
      "| Loss mean:   0.247054 |\n",
      "g_step: 31700 loss std/mean: 0.023388344794511795 0.2394963502883911\n",
      "| Loss std:   0.023388 |\n",
      "| Loss mean:   0.239496 |\n",
      "g_step: 31800 loss std/mean: 0.022084316238760948 0.24141855537891388\n",
      "| Loss std:   0.022084 |\n",
      "| Loss mean:   0.241419 |\n",
      "g_step: 31900 loss std/mean: 0.024201378226280212 0.24145832657814026\n",
      "| Loss std:   0.024201 |\n",
      "| Loss mean:   0.241458 |\n",
      "g_step: 32000 loss std/mean: 0.0199716929346323 0.24629560112953186\n",
      "| Loss std:   0.019972 |\n",
      "| Loss mean:   0.246296 |\n",
      "g_step: 32100 loss std/mean: 0.019864946603775024 0.24503129720687866\n",
      "| Loss std:   0.019865 |\n",
      "| Loss mean:   0.245031 |\n",
      "g_step: 32200 loss std/mean: 0.02334604784846306 0.23633675277233124\n",
      "| Loss std:   0.023346 |\n",
      "| Loss mean:   0.236337 |\n",
      "g_step: 32300 loss std/mean: 0.02236875705420971 0.23981252312660217\n",
      "| Loss std:   0.022369 |\n",
      "| Loss mean:   0.239813 |\n",
      "g_step: 32400 loss std/mean: 0.025818999856710434 0.24107906222343445\n",
      "| Loss std:   0.025819 |\n",
      "| Loss mean:   0.241079 |\n",
      "g_step: 32500 loss std/mean: 0.02068381756544113 0.24389173090457916\n",
      "| Loss std:   0.020684 |\n",
      "| Loss mean:   0.243892 |\n",
      "g_step: 32600 loss std/mean: 0.019936228170990944 0.23758995532989502\n",
      "| Loss std:   0.019936 |\n",
      "| Loss mean:   0.237590 |\n",
      "g_step: 32700 loss std/mean: 0.020076148211956024 0.24568668007850647\n",
      "| Loss std:   0.020076 |\n",
      "| Loss mean:   0.245687 |\n",
      "g_step: 32800 loss std/mean: 0.022060181945562363 0.24424949288368225\n",
      "| Loss std:   0.022060 |\n",
      "| Loss mean:   0.244249 |\n",
      "g_step: 32900 loss std/mean: 0.02161659114062786 0.2410574108362198\n",
      "| Loss std:   0.021617 |\n",
      "| Loss mean:   0.241057 |\n",
      "g_step: 33000 loss std/mean: 0.022150516510009766 0.2406974732875824\n",
      "| Loss std:   0.022151 |\n",
      "| Loss mean:   0.240697 |\n",
      "g_step: 33100 loss std/mean: 0.021427690982818604 0.24341735243797302\n",
      "| Loss std:   0.021428 |\n",
      "| Loss mean:   0.243417 |\n",
      "g_step: 33200 loss std/mean: 0.020869333297014236 0.2418060302734375\n",
      "| Loss std:   0.020869 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Loss mean:   0.241806 |\n",
      "g_step: 33300 loss std/mean: 0.02342156693339348 0.2408418208360672\n",
      "| Loss std:   0.023422 |\n",
      "| Loss mean:   0.240842 |\n",
      "g_step: 33400 loss std/mean: 0.019969625398516655 0.24097339808940887\n",
      "| Loss std:   0.019970 |\n",
      "| Loss mean:   0.240973 |\n",
      "g_step: 33500 loss std/mean: 0.02212660387158394 0.23766092956066132\n",
      "| Loss std:   0.022127 |\n",
      "| Loss mean:   0.237661 |\n",
      "g_step: 33600 loss std/mean: 0.023009952157735825 0.23929814994335175\n",
      "| Loss std:   0.023010 |\n",
      "| Loss mean:   0.239298 |\n",
      "g_step: 33700 loss std/mean: 0.019538670778274536 0.251552939414978\n",
      "| Loss std:   0.019539 |\n",
      "| Loss mean:   0.251553 |\n",
      "g_step: 33800 loss std/mean: 0.025808464735746384 0.24293901026248932\n",
      "| Loss std:   0.025808 |\n",
      "| Loss mean:   0.242939 |\n",
      "g_step: 33900 loss std/mean: 0.023506250232458115 0.23970282077789307\n",
      "| Loss std:   0.023506 |\n",
      "| Loss mean:   0.239703 |\n",
      "g_step: 34000 loss std/mean: 0.02199053205549717 0.24223662912845612\n",
      "| Loss std:   0.021991 |\n",
      "| Loss mean:   0.242237 |\n",
      "g_step: 34100 loss std/mean: 0.022327758371829987 0.239750474691391\n",
      "| Loss std:   0.022328 |\n",
      "| Loss mean:   0.239750 |\n",
      "g_step: 34200 loss std/mean: 0.021112121641635895 0.24606600403785706\n",
      "| Loss std:   0.021112 |\n",
      "| Loss mean:   0.246066 |\n",
      "g_step: 34300 loss std/mean: 0.02196839451789856 0.2429015189409256\n",
      "| Loss std:   0.021968 |\n",
      "| Loss mean:   0.242902 |\n",
      "g_step: 34400 loss std/mean: 0.022233638912439346 0.2413109540939331\n",
      "| Loss std:   0.022234 |\n",
      "| Loss mean:   0.241311 |\n",
      "g_step: 34500 loss std/mean: 0.02148774079978466 0.24337346851825714\n",
      "| Loss std:   0.021488 |\n",
      "| Loss mean:   0.243373 |\n",
      "g_step: 34600 loss std/mean: 0.0206168033182621 0.24357055127620697\n",
      "| Loss std:   0.020617 |\n",
      "| Loss mean:   0.243571 |\n",
      "g_step: 34700 loss std/mean: 0.017650485038757324 0.24634519219398499\n",
      "| Loss std:   0.017650 |\n",
      "| Loss mean:   0.246345 |\n",
      "g_step: 34800 loss std/mean: 0.022885851562023163 0.23970168828964233\n",
      "| Loss std:   0.022886 |\n",
      "| Loss mean:   0.239702 |\n",
      "g_step: 34900 loss std/mean: 0.021381156519055367 0.24374191462993622\n",
      "| Loss std:   0.021381 |\n",
      "| Loss mean:   0.243742 |\n",
      "g_step: 35000 loss std/mean: 0.019546858966350555 0.2380995750427246\n",
      "| Loss std:   0.019547 |\n",
      "| Loss mean:   0.238100 |\n",
      "\tValidation NWRMSLE  : 0.523416316936\n",
      "| Validation NWRMSLE:   0.523416 |\n",
      "\tValidation NWRMSLE_5: 0.512567497645\n",
      "| Validation NWRMSLE_5:   0.512567 |\n",
      "g_step: 35100 loss std/mean: 0.021346861496567726 0.24312150478363037\n",
      "| Loss std:   0.021347 |\n",
      "| Loss mean:   0.243122 |\n",
      "g_step: 35200 loss std/mean: 0.01984044350683689 0.24548648297786713\n",
      "| Loss std:   0.019840 |\n",
      "| Loss mean:   0.245486 |\n",
      "g_step: 35300 loss std/mean: 0.02105339802801609 0.23620028793811798\n",
      "| Loss std:   0.021053 |\n",
      "| Loss mean:   0.236200 |\n",
      "g_step: 35400 loss std/mean: 0.019727589562535286 0.24514088034629822\n",
      "| Loss std:   0.019728 |\n",
      "| Loss mean:   0.245141 |\n",
      "g_step: 35500 loss std/mean: 0.020902883261442184 0.24059538543224335\n",
      "| Loss std:   0.020903 |\n",
      "| Loss mean:   0.240595 |\n",
      "g_step: 35600 loss std/mean: 0.021040113642811775 0.2426683008670807\n",
      "| Loss std:   0.021040 |\n",
      "| Loss mean:   0.242668 |\n",
      "g_step: 35700 loss std/mean: 0.022735724225640297 0.244432732462883\n",
      "| Loss std:   0.022736 |\n",
      "| Loss mean:   0.244433 |\n",
      "g_step: 35800 loss std/mean: 0.018512239679694176 0.24294430017471313\n",
      "| Loss std:   0.018512 |\n",
      "| Loss mean:   0.242944 |\n",
      "g_step: 35900 loss std/mean: 0.02151251770555973 0.2433396279811859\n",
      "| Loss std:   0.021513 |\n",
      "| Loss mean:   0.243340 |\n",
      "g_step: 36000 loss std/mean: 0.019624287262558937 0.24171993136405945\n",
      "| Loss std:   0.019624 |\n",
      "| Loss mean:   0.241720 |\n",
      "g_step: 36100 loss std/mean: 0.022007165476679802 0.23812158405780792\n",
      "| Loss std:   0.022007 |\n",
      "| Loss mean:   0.238122 |\n",
      "g_step: 36200 loss std/mean: 0.02241620421409607 0.24197980761528015\n",
      "| Loss std:   0.022416 |\n",
      "| Loss mean:   0.241980 |\n",
      "g_step: 36300 loss std/mean: 0.024480411782860756 0.2414488047361374\n",
      "| Loss std:   0.024480 |\n",
      "| Loss mean:   0.241449 |\n",
      "g_step: 36400 loss std/mean: 0.02053343877196312 0.2409188449382782\n",
      "| Loss std:   0.020533 |\n",
      "| Loss mean:   0.240919 |\n",
      "g_step: 36500 loss std/mean: 0.01993391662836075 0.242295041680336\n",
      "| Loss std:   0.019934 |\n",
      "| Loss mean:   0.242295 |\n",
      "g_step: 36600 loss std/mean: 0.017469193786382675 0.24457840621471405\n",
      "| Loss std:   0.017469 |\n",
      "| Loss mean:   0.244578 |\n",
      "g_step: 36700 loss std/mean: 0.016885219141840935 0.23844385147094727\n",
      "| Loss std:   0.016885 |\n",
      "| Loss mean:   0.238444 |\n",
      "g_step: 36800 loss std/mean: 0.022459182888269424 0.24171054363250732\n",
      "| Loss std:   0.022459 |\n",
      "| Loss mean:   0.241711 |\n",
      "g_step: 36900 loss std/mean: 0.02261490374803543 0.2417202740907669\n",
      "| Loss std:   0.022615 |\n",
      "| Loss mean:   0.241720 |\n",
      "g_step: 37000 loss std/mean: 0.01896229386329651 0.23972393572330475\n",
      "| Loss std:   0.018962 |\n",
      "| Loss mean:   0.239724 |\n",
      "g_step: 37100 loss std/mean: 0.02120172791182995 0.2421247810125351\n",
      "| Loss std:   0.021202 |\n",
      "| Loss mean:   0.242125 |\n",
      "g_step: 37200 loss std/mean: 0.021269727498292923 0.24297016859054565\n",
      "| Loss std:   0.021270 |\n",
      "| Loss mean:   0.242970 |\n",
      "g_step: 37300 loss std/mean: 0.022661708295345306 0.24274758994579315\n",
      "| Loss std:   0.022662 |\n",
      "| Loss mean:   0.242748 |\n",
      "g_step: 37400 loss std/mean: 0.02410462312400341 0.23878827691078186\n",
      "| Loss std:   0.024105 |\n",
      "| Loss mean:   0.238788 |\n",
      "g_step: 37500 loss std/mean: 0.02127690054476261 0.2428596168756485\n",
      "| Loss std:   0.021277 |\n",
      "| Loss mean:   0.242860 |\n",
      "g_step: 37600 loss std/mean: 0.017046775668859482 0.24539026618003845\n",
      "| Loss std:   0.017047 |\n",
      "| Loss mean:   0.245390 |\n",
      "g_step: 37700 loss std/mean: 0.021546082571148872 0.24069954454898834\n",
      "| Loss std:   0.021546 |\n",
      "| Loss mean:   0.240700 |\n",
      "g_step: 37800 loss std/mean: 0.021215302869677544 0.2347996085882187\n",
      "| Loss std:   0.021215 |\n",
      "| Loss mean:   0.234800 |\n",
      "g_step: 37900 loss std/mean: 0.023768629878759384 0.24311962723731995\n",
      "| Loss std:   0.023769 |\n",
      "| Loss mean:   0.243120 |\n",
      "g_step: 38000 loss std/mean: 0.021323835477232933 0.24119633436203003\n",
      "| Loss std:   0.021324 |\n",
      "| Loss mean:   0.241196 |\n",
      "g_step: 38100 loss std/mean: 0.023695826530456543 0.23749324679374695\n",
      "| Loss std:   0.023696 |\n",
      "| Loss mean:   0.237493 |\n",
      "g_step: 38200 loss std/mean: 0.022589560598134995 0.2427947074174881\n",
      "| Loss std:   0.022590 |\n",
      "| Loss mean:   0.242795 |\n",
      "g_step: 38300 loss std/mean: 0.022455835714936256 0.23760680854320526\n",
      "| Loss std:   0.022456 |\n",
      "| Loss mean:   0.237607 |\n",
      "g_step: 38400 loss std/mean: 0.01986355148255825 0.24451623857021332\n",
      "| Loss std:   0.019864 |\n",
      "| Loss mean:   0.244516 |\n",
      "g_step: 38500 loss std/mean: 0.02280084416270256 0.2391376495361328\n",
      "| Loss std:   0.022801 |\n",
      "| Loss mean:   0.239138 |\n",
      "g_step: 38600 loss std/mean: 0.02075219340622425 0.24022170901298523\n",
      "| Loss std:   0.020752 |\n",
      "| Loss mean:   0.240222 |\n",
      "g_step: 38700 loss std/mean: 0.020259423181414604 0.24396438896656036\n",
      "| Loss std:   0.020259 |\n",
      "| Loss mean:   0.243964 |\n",
      "g_step: 38800 loss std/mean: 0.021930446848273277 0.2411126345396042\n",
      "| Loss std:   0.021930 |\n",
      "| Loss mean:   0.241113 |\n",
      "g_step: 38900 loss std/mean: 0.02464006282389164 0.24222004413604736\n",
      "| Loss std:   0.024640 |\n",
      "| Loss mean:   0.242220 |\n",
      "g_step: 39000 loss std/mean: 0.022182192653417587 0.23991183936595917\n",
      "| Loss std:   0.022182 |\n",
      "| Loss mean:   0.239912 |\n",
      "g_step: 39100 loss std/mean: 0.023805394768714905 0.24250248074531555\n",
      "| Loss std:   0.023805 |\n",
      "| Loss mean:   0.242502 |\n",
      "g_step: 39200 loss std/mean: 0.023760780692100525 0.24292920529842377\n",
      "| Loss std:   0.023761 |\n",
      "| Loss mean:   0.242929 |\n",
      "g_step: 39300 loss std/mean: 0.024900194257497787 0.23534566164016724\n",
      "| Loss std:   0.024900 |\n",
      "| Loss mean:   0.235346 |\n",
      "g_step: 39400 loss std/mean: 0.024611763656139374 0.2430819272994995\n",
      "| Loss std:   0.024612 |\n",
      "| Loss mean:   0.243082 |\n",
      "g_step: 39500 loss std/mean: 0.022820167243480682 0.24099399149417877\n",
      "| Loss std:   0.022820 |\n",
      "| Loss mean:   0.240994 |\n",
      "g_step: 39600 loss std/mean: 0.021258311346173286 0.2398330271244049\n",
      "| Loss std:   0.021258 |\n",
      "| Loss mean:   0.239833 |\n",
      "g_step: 39700 loss std/mean: 0.022535869851708412 0.24409280717372894\n",
      "| Loss std:   0.022536 |\n",
      "| Loss mean:   0.244093 |\n",
      "g_step: 39800 loss std/mean: 0.020605867728590965 0.23925252258777618\n",
      "| Loss std:   0.020606 |\n",
      "| Loss mean:   0.239253 |\n",
      "g_step: 39900 loss std/mean: 0.021250324323773384 0.2446793019771576\n",
      "| Loss std:   0.021250 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Loss mean:   0.244679 |\n",
      "g_step: 40000 loss std/mean: 0.02248038351535797 0.23936109244823456\n",
      "| Loss std:   0.022480 |\n",
      "| Loss mean:   0.239361 |\n",
      "\tValidation NWRMSLE  : 0.522901735979\n",
      "| Validation NWRMSLE:   0.522902 |\n",
      "\tValidation NWRMSLE_5: 0.51416520149\n",
      "| Validation NWRMSLE_5:   0.514165 |\n",
      "g_step: 40100 loss std/mean: 0.01924535632133484 0.2361779808998108\n",
      "| Loss std:   0.019245 |\n",
      "| Loss mean:   0.236178 |\n",
      "g_step: 40200 loss std/mean: 0.02071082964539528 0.24094229936599731\n",
      "| Loss std:   0.020711 |\n",
      "| Loss mean:   0.240942 |\n",
      "g_step: 40300 loss std/mean: 0.022969383746385574 0.23857839405536652\n",
      "| Loss std:   0.022969 |\n",
      "| Loss mean:   0.238578 |\n",
      "g_step: 40400 loss std/mean: 0.022234326228499413 0.23990826308727264\n",
      "| Loss std:   0.022234 |\n",
      "| Loss mean:   0.239908 |\n",
      "g_step: 40500 loss std/mean: 0.02490818127989769 0.24130859971046448\n",
      "| Loss std:   0.024908 |\n",
      "| Loss mean:   0.241309 |\n",
      "g_step: 40600 loss std/mean: 0.021843520924448967 0.2392946481704712\n",
      "| Loss std:   0.021844 |\n",
      "| Loss mean:   0.239295 |\n",
      "g_step: 40700 loss std/mean: 0.021406035870313644 0.24602057039737701\n",
      "| Loss std:   0.021406 |\n",
      "| Loss mean:   0.246021 |\n",
      "g_step: 40800 loss std/mean: 0.022773217409849167 0.2409854680299759\n",
      "| Loss std:   0.022773 |\n",
      "| Loss mean:   0.240985 |\n",
      "g_step: 40900 loss std/mean: 0.019396614283323288 0.23810720443725586\n",
      "| Loss std:   0.019397 |\n",
      "| Loss mean:   0.238107 |\n",
      "g_step: 41000 loss std/mean: 0.020975055173039436 0.24353626370429993\n",
      "| Loss std:   0.020975 |\n",
      "| Loss mean:   0.243536 |\n",
      "g_step: 41100 loss std/mean: 0.01981898583471775 0.2409752458333969\n",
      "| Loss std:   0.019819 |\n",
      "| Loss mean:   0.240975 |\n",
      "g_step: 41200 loss std/mean: 0.020663714036345482 0.23682646453380585\n",
      "| Loss std:   0.020664 |\n",
      "| Loss mean:   0.236826 |\n",
      "g_step: 41300 loss std/mean: 0.02294452302157879 0.2425130307674408\n",
      "| Loss std:   0.022945 |\n",
      "| Loss mean:   0.242513 |\n",
      "g_step: 41400 loss std/mean: 0.019289536401629448 0.2401348054409027\n",
      "| Loss std:   0.019290 |\n",
      "| Loss mean:   0.240135 |\n",
      "g_step: 41500 loss std/mean: 0.020021697506308556 0.24264946579933167\n",
      "| Loss std:   0.020022 |\n",
      "| Loss mean:   0.242649 |\n",
      "g_step: 41600 loss std/mean: 0.02460363134741783 0.2387647181749344\n",
      "| Loss std:   0.024604 |\n",
      "| Loss mean:   0.238765 |\n",
      "g_step: 41700 loss std/mean: 0.017150914296507835 0.24361808598041534\n",
      "| Loss std:   0.017151 |\n",
      "| Loss mean:   0.243618 |\n",
      "g_step: 41800 loss std/mean: 0.02413027547299862 0.23751713335514069\n",
      "| Loss std:   0.024130 |\n",
      "| Loss mean:   0.237517 |\n",
      "g_step: 41900 loss std/mean: 0.02221888303756714 0.23599064350128174\n",
      "| Loss std:   0.022219 |\n",
      "| Loss mean:   0.235991 |\n",
      "g_step: 42000 loss std/mean: 0.023284777998924255 0.24253888428211212\n",
      "| Loss std:   0.023285 |\n",
      "| Loss mean:   0.242539 |\n",
      "g_step: 42100 loss std/mean: 0.023112637922167778 0.2386677861213684\n",
      "| Loss std:   0.023113 |\n",
      "| Loss mean:   0.238668 |\n",
      "g_step: 42200 loss std/mean: 0.022072836756706238 0.2435513138771057\n",
      "| Loss std:   0.022073 |\n",
      "| Loss mean:   0.243551 |\n",
      "g_step: 42300 loss std/mean: 0.022539330646395683 0.23828831315040588\n",
      "| Loss std:   0.022539 |\n",
      "| Loss mean:   0.238288 |\n",
      "g_step: 42400 loss std/mean: 0.0202297605574131 0.24368204176425934\n",
      "| Loss std:   0.020230 |\n",
      "| Loss mean:   0.243682 |\n",
      "g_step: 42500 loss std/mean: 0.02172895148396492 0.24410468339920044\n",
      "| Loss std:   0.021729 |\n",
      "| Loss mean:   0.244105 |\n",
      "g_step: 42600 loss std/mean: 0.02622460201382637 0.23741953074932098\n",
      "| Loss std:   0.026225 |\n",
      "| Loss mean:   0.237420 |\n",
      "g_step: 42700 loss std/mean: 0.018302999436855316 0.2467104196548462\n",
      "| Loss std:   0.018303 |\n",
      "| Loss mean:   0.246710 |\n",
      "g_step: 42800 loss std/mean: 0.020038234069943428 0.2389344424009323\n",
      "| Loss std:   0.020038 |\n",
      "| Loss mean:   0.238934 |\n",
      "g_step: 42900 loss std/mean: 0.024013260379433632 0.23581519722938538\n",
      "| Loss std:   0.024013 |\n",
      "| Loss mean:   0.235815 |\n",
      "g_step: 43000 loss std/mean: 0.02247096598148346 0.23375529050827026\n",
      "| Loss std:   0.022471 |\n",
      "| Loss mean:   0.233755 |\n",
      "g_step: 43100 loss std/mean: 0.021906252950429916 0.23440372943878174\n",
      "| Loss std:   0.021906 |\n",
      "| Loss mean:   0.234404 |\n",
      "g_step: 43200 loss std/mean: 0.020633991807699203 0.2385726422071457\n",
      "| Loss std:   0.020634 |\n",
      "| Loss mean:   0.238573 |\n",
      "g_step: 43300 loss std/mean: 0.022165143862366676 0.23838725686073303\n",
      "| Loss std:   0.022165 |\n",
      "| Loss mean:   0.238387 |\n",
      "g_step: 43400 loss std/mean: 0.020668985322117805 0.23659758269786835\n",
      "| Loss std:   0.020669 |\n",
      "| Loss mean:   0.236598 |\n",
      "g_step: 43500 loss std/mean: 0.02314400114119053 0.23732714354991913\n",
      "| Loss std:   0.023144 |\n",
      "| Loss mean:   0.237327 |\n",
      "g_step: 43600 loss std/mean: 0.02313859574496746 0.24611906707286835\n",
      "| Loss std:   0.023139 |\n",
      "| Loss mean:   0.246119 |\n",
      "g_step: 43700 loss std/mean: 0.0201592817902565 0.24122436344623566\n",
      "| Loss std:   0.020159 |\n",
      "| Loss mean:   0.241224 |\n",
      "g_step: 43800 loss std/mean: 0.02081470750272274 0.23931488394737244\n",
      "| Loss std:   0.020815 |\n",
      "| Loss mean:   0.239315 |\n",
      "g_step: 43900 loss std/mean: 0.02246136963367462 0.239973247051239\n",
      "| Loss std:   0.022461 |\n",
      "| Loss mean:   0.239973 |\n",
      "g_step: 44000 loss std/mean: 0.02425376884639263 0.24039167165756226\n",
      "| Loss std:   0.024254 |\n",
      "| Loss mean:   0.240392 |\n",
      "g_step: 44100 loss std/mean: 0.02077493444085121 0.2373475581407547\n",
      "| Loss std:   0.020775 |\n",
      "| Loss mean:   0.237348 |\n",
      "g_step: 44200 loss std/mean: 0.01937500759959221 0.2383393496274948\n",
      "| Loss std:   0.019375 |\n",
      "| Loss mean:   0.238339 |\n",
      "g_step: 44300 loss std/mean: 0.021787427365779877 0.2382708340883255\n",
      "| Loss std:   0.021787 |\n",
      "| Loss mean:   0.238271 |\n",
      "g_step: 44400 loss std/mean: 0.019336724653840065 0.2434140145778656\n",
      "| Loss std:   0.019337 |\n",
      "| Loss mean:   0.243414 |\n",
      "g_step: 44500 loss std/mean: 0.022449098527431488 0.23733676970005035\n",
      "| Loss std:   0.022449 |\n",
      "| Loss mean:   0.237337 |\n",
      "g_step: 44600 loss std/mean: 0.023205172270536423 0.23996272683143616\n",
      "| Loss std:   0.023205 |\n",
      "| Loss mean:   0.239963 |\n",
      "g_step: 44700 loss std/mean: 0.019499439746141434 0.2385806292295456\n",
      "| Loss std:   0.019499 |\n",
      "| Loss mean:   0.238581 |\n",
      "g_step: 44800 loss std/mean: 0.01833389326930046 0.23857492208480835\n",
      "| Loss std:   0.018334 |\n",
      "| Loss mean:   0.238575 |\n",
      "g_step: 44900 loss std/mean: 0.020574823021888733 0.2408558875322342\n",
      "| Loss std:   0.020575 |\n",
      "| Loss mean:   0.240856 |\n",
      "g_step: 45000 loss std/mean: 0.02232091687619686 0.23844029009342194\n",
      "| Loss std:   0.022321 |\n",
      "| Loss mean:   0.238440 |\n",
      "\tValidation NWRMSLE  : 0.523274345871\n",
      "| Validation NWRMSLE:   0.523274 |\n",
      "\tValidation NWRMSLE_5: 0.513550721562\n",
      "| Validation NWRMSLE_5:   0.513551 |\n",
      "g_step: 45100 loss std/mean: 0.019696472212672234 0.23879384994506836\n",
      "| Loss std:   0.019696 |\n",
      "| Loss mean:   0.238794 |\n",
      "g_step: 45200 loss std/mean: 0.022145943716168404 0.23638339340686798\n",
      "| Loss std:   0.022146 |\n",
      "| Loss mean:   0.236383 |\n",
      "g_step: 45300 loss std/mean: 0.019914565607905388 0.24500231444835663\n",
      "| Loss std:   0.019915 |\n",
      "| Loss mean:   0.245002 |\n",
      "g_step: 45400 loss std/mean: 0.02034248784184456 0.23912428319454193\n",
      "| Loss std:   0.020342 |\n",
      "| Loss mean:   0.239124 |\n",
      "g_step: 45500 loss std/mean: 0.018593955785036087 0.2438374161720276\n",
      "| Loss std:   0.018594 |\n",
      "| Loss mean:   0.243837 |\n",
      "g_step: 45600 loss std/mean: 0.020283419638872147 0.2342221438884735\n",
      "| Loss std:   0.020283 |\n",
      "| Loss mean:   0.234222 |\n",
      "g_step: 45700 loss std/mean: 0.017936108633875847 0.23637443780899048\n",
      "| Loss std:   0.017936 |\n",
      "| Loss mean:   0.236374 |\n",
      "g_step: 45800 loss std/mean: 0.02154082991182804 0.24170880019664764\n",
      "| Loss std:   0.021541 |\n",
      "| Loss mean:   0.241709 |\n",
      "g_step: 45900 loss std/mean: 0.021981848403811455 0.23525157570838928\n",
      "| Loss std:   0.021982 |\n",
      "| Loss mean:   0.235252 |\n",
      "g_step: 46000 loss std/mean: 0.023207897320389748 0.24212056398391724\n",
      "| Loss std:   0.023208 |\n",
      "| Loss mean:   0.242121 |\n",
      "g_step: 46100 loss std/mean: 0.018851300701498985 0.2456406205892563\n",
      "| Loss std:   0.018851 |\n",
      "| Loss mean:   0.245641 |\n",
      "g_step: 46200 loss std/mean: 0.019234154373407364 0.23786816000938416\n",
      "| Loss std:   0.019234 |\n",
      "| Loss mean:   0.237868 |\n",
      "g_step: 46300 loss std/mean: 0.02224067784845829 0.23816803097724915\n",
      "| Loss std:   0.022241 |\n",
      "| Loss mean:   0.238168 |\n",
      "g_step: 46400 loss std/mean: 0.024892659857869148 0.23834261298179626\n",
      "| Loss std:   0.024893 |\n",
      "| Loss mean:   0.238343 |\n",
      "g_step: 46500 loss std/mean: 0.021024104207754135 0.23940618336200714\n",
      "| Loss std:   0.021024 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Loss mean:   0.239406 |\n",
      "g_step: 46600 loss std/mean: 0.01989794336259365 0.2397851198911667\n",
      "| Loss std:   0.019898 |\n",
      "| Loss mean:   0.239785 |\n",
      "g_step: 46700 loss std/mean: 0.022866861894726753 0.23761315643787384\n",
      "| Loss std:   0.022867 |\n",
      "| Loss mean:   0.237613 |\n",
      "g_step: 46800 loss std/mean: 0.020946327596902847 0.24187244474887848\n",
      "| Loss std:   0.020946 |\n",
      "| Loss mean:   0.241872 |\n",
      "g_step: 46900 loss std/mean: 0.02036040462553501 0.23270395398139954\n",
      "| Loss std:   0.020360 |\n",
      "| Loss mean:   0.232704 |\n",
      "g_step: 47000 loss std/mean: 0.02017219550907612 0.24021470546722412\n",
      "| Loss std:   0.020172 |\n",
      "| Loss mean:   0.240215 |\n",
      "g_step: 47100 loss std/mean: 0.021778754889965057 0.2412554770708084\n",
      "| Loss std:   0.021779 |\n",
      "| Loss mean:   0.241255 |\n",
      "g_step: 47200 loss std/mean: 0.023647958412766457 0.24190954864025116\n",
      "| Loss std:   0.023648 |\n",
      "| Loss mean:   0.241910 |\n",
      "g_step: 47300 loss std/mean: 0.021639296784996986 0.24254852533340454\n",
      "| Loss std:   0.021639 |\n",
      "| Loss mean:   0.242549 |\n",
      "g_step: 47400 loss std/mean: 0.01982121169567108 0.24130073189735413\n",
      "| Loss std:   0.019821 |\n",
      "| Loss mean:   0.241301 |\n",
      "g_step: 47500 loss std/mean: 0.021882597357034683 0.23676973581314087\n",
      "| Loss std:   0.021883 |\n",
      "| Loss mean:   0.236770 |\n",
      "g_step: 47600 loss std/mean: 0.020992275327444077 0.24088169634342194\n",
      "| Loss std:   0.020992 |\n",
      "| Loss mean:   0.240882 |\n",
      "g_step: 47700 loss std/mean: 0.021222800016403198 0.23609885573387146\n",
      "| Loss std:   0.021223 |\n",
      "| Loss mean:   0.236099 |\n",
      "g_step: 47800 loss std/mean: 0.023456647992134094 0.2412644922733307\n",
      "| Loss std:   0.023457 |\n",
      "| Loss mean:   0.241264 |\n",
      "g_step: 47900 loss std/mean: 0.02236657217144966 0.23459738492965698\n",
      "| Loss std:   0.022367 |\n",
      "| Loss mean:   0.234597 |\n",
      "g_step: 48000 loss std/mean: 0.020380139350891113 0.2358330935239792\n",
      "| Loss std:   0.020380 |\n",
      "| Loss mean:   0.235833 |\n",
      "g_step: 48100 loss std/mean: 0.020741058513522148 0.2400786578655243\n",
      "| Loss std:   0.020741 |\n",
      "| Loss mean:   0.240079 |\n",
      "g_step: 48200 loss std/mean: 0.019218161702156067 0.2382095456123352\n",
      "| Loss std:   0.019218 |\n",
      "| Loss mean:   0.238210 |\n",
      "g_step: 48300 loss std/mean: 0.02021912857890129 0.23662057518959045\n",
      "| Loss std:   0.020219 |\n",
      "| Loss mean:   0.236621 |\n",
      "g_step: 48400 loss std/mean: 0.01982785575091839 0.24357980489730835\n",
      "| Loss std:   0.019828 |\n",
      "| Loss mean:   0.243580 |\n",
      "g_step: 48500 loss std/mean: 0.023193612694740295 0.242063507437706\n",
      "| Loss std:   0.023194 |\n",
      "| Loss mean:   0.242064 |\n",
      "g_step: 48600 loss std/mean: 0.02228064276278019 0.2404535859823227\n",
      "| Loss std:   0.022281 |\n",
      "| Loss mean:   0.240454 |\n",
      "g_step: 48700 loss std/mean: 0.021424273028969765 0.2390374392271042\n",
      "| Loss std:   0.021424 |\n",
      "| Loss mean:   0.239037 |\n",
      "g_step: 48800 loss std/mean: 0.02166842669248581 0.23715652525424957\n",
      "| Loss std:   0.021668 |\n",
      "| Loss mean:   0.237157 |\n",
      "g_step: 48900 loss std/mean: 0.023806264623999596 0.2310321033000946\n",
      "| Loss std:   0.023806 |\n",
      "| Loss mean:   0.231032 |\n",
      "g_step: 49000 loss std/mean: 0.02198069542646408 0.24057641625404358\n",
      "| Loss std:   0.021981 |\n",
      "| Loss mean:   0.240576 |\n",
      "g_step: 49100 loss std/mean: 0.021459100767970085 0.2366996705532074\n",
      "| Loss std:   0.021459 |\n",
      "| Loss mean:   0.236700 |\n",
      "g_step: 49200 loss std/mean: 0.019773028790950775 0.2401580810546875\n",
      "| Loss std:   0.019773 |\n",
      "| Loss mean:   0.240158 |\n",
      "g_step: 49300 loss std/mean: 0.0207695160061121 0.24116559326648712\n",
      "| Loss std:   0.020770 |\n",
      "| Loss mean:   0.241166 |\n",
      "g_step: 49400 loss std/mean: 0.021130289882421494 0.24016456305980682\n",
      "| Loss std:   0.021130 |\n",
      "| Loss mean:   0.240165 |\n",
      "g_step: 49500 loss std/mean: 0.022683139890432358 0.23401693999767303\n",
      "| Loss std:   0.022683 |\n",
      "| Loss mean:   0.234017 |\n",
      "g_step: 49600 loss std/mean: 0.022930489853024483 0.23959875106811523\n",
      "| Loss std:   0.022930 |\n",
      "| Loss mean:   0.239599 |\n",
      "g_step: 49700 loss std/mean: 0.01812034286558628 0.24323411285877228\n",
      "| Loss std:   0.018120 |\n",
      "| Loss mean:   0.243234 |\n",
      "g_step: 49800 loss std/mean: 0.018803460523486137 0.24332726001739502\n",
      "| Loss std:   0.018803 |\n",
      "| Loss mean:   0.243327 |\n",
      "g_step: 49900 loss std/mean: 0.017661863937973976 0.24275155365467072\n",
      "| Loss std:   0.017662 |\n",
      "| Loss mean:   0.242752 |\n",
      "g_step: 50000 loss std/mean: 0.023122407495975494 0.24140019714832306\n",
      "| Loss std:   0.023122 |\n",
      "| Loss mean:   0.241400 |\n",
      "\tValidation NWRMSLE  : 0.521881366425\n",
      "| Validation NWRMSLE:   0.521881 |\n",
      "\tValidation NWRMSLE_5: 0.511118961131\n",
      "| Validation NWRMSLE_5:   0.511119 |\n",
      "g_step: 50100 loss std/mean: 0.018757538869976997 0.2435232400894165\n",
      "| Loss std:   0.018758 |\n",
      "| Loss mean:   0.243523 |\n",
      "g_step: 50200 loss std/mean: 0.02134917862713337 0.23780027031898499\n",
      "| Loss std:   0.021349 |\n",
      "| Loss mean:   0.237800 |\n",
      "g_step: 50300 loss std/mean: 0.01990746334195137 0.2401595115661621\n",
      "| Loss std:   0.019907 |\n",
      "| Loss mean:   0.240160 |\n",
      "g_step: 50400 loss std/mean: 0.0224270299077034 0.23612797260284424\n",
      "| Loss std:   0.022427 |\n",
      "| Loss mean:   0.236128 |\n",
      "g_step: 50500 loss std/mean: 0.020402047783136368 0.23766198754310608\n",
      "| Loss std:   0.020402 |\n",
      "| Loss mean:   0.237662 |\n",
      "g_step: 50600 loss std/mean: 0.023149127140641212 0.23269450664520264\n",
      "| Loss std:   0.023149 |\n",
      "| Loss mean:   0.232695 |\n",
      "g_step: 50700 loss std/mean: 0.022634120658040047 0.23311643302440643\n",
      "| Loss std:   0.022634 |\n",
      "| Loss mean:   0.233116 |\n",
      "g_step: 50800 loss std/mean: 0.020857209339737892 0.23812410235404968\n",
      "| Loss std:   0.020857 |\n",
      "| Loss mean:   0.238124 |\n",
      "g_step: 50900 loss std/mean: 0.019180281087756157 0.2423561066389084\n",
      "| Loss std:   0.019180 |\n",
      "| Loss mean:   0.242356 |\n",
      "g_step: 51000 loss std/mean: 0.019869975745677948 0.23985637724399567\n",
      "| Loss std:   0.019870 |\n",
      "| Loss mean:   0.239856 |\n",
      "g_step: 51100 loss std/mean: 0.02384856902062893 0.2365875095129013\n",
      "| Loss std:   0.023849 |\n",
      "| Loss mean:   0.236588 |\n",
      "g_step: 51200 loss std/mean: 0.01904323324561119 0.24257585406303406\n",
      "| Loss std:   0.019043 |\n",
      "| Loss mean:   0.242576 |\n",
      "g_step: 51300 loss std/mean: 0.019155241549015045 0.24052967131137848\n",
      "| Loss std:   0.019155 |\n",
      "| Loss mean:   0.240530 |\n",
      "Ctrl+C\n",
      "This run of RNN fav 3 ran for 18:30:50 and logs are available locally at: /home/dmitry/.hyperdash/logs/rnn-fav-3/rnn-fav-3_2018-01-01t03-14-49-095399.log\n"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "\n",
    "history = 400\n",
    "time_to_predict = 16\n",
    "freq=1\n",
    "\n",
    "last_day_train = '2017-07-30'\n",
    "window=300\n",
    "epochs = 100\n",
    "validation_day = pd.to_datetime(last_day_train) #+ pd.Timedelta('{} days'.format(time_to_predict))\n",
    "batch_size = 1000\n",
    "sum_W = 3574368.0/16\n",
    "skip=0\n",
    "\n",
    "print(validation_day)\n",
    "\n",
    "batch_gen = get_random_train_test(\n",
    "    df_pivot,\n",
    "    last_day_train,\n",
    "    window=window,\n",
    "    history=history,\n",
    "    size=batch_size,\n",
    "    predict_days=time_to_predict,\n",
    "    epochs=epochs,\n",
    "    skip=skip,\n",
    "    freq=freq\n",
    ")\n",
    "\n",
    "val_set = get_validation(df_pivot, validation_day, history=history,\n",
    "                        predict_days=time_to_predict, skip=skip)\n",
    "\n",
    "from model import RNNModel\n",
    "\n",
    "m = RNNModel(\n",
    "    history=history,\n",
    "    n_days_predict=time_to_predict,\n",
    "    clip_gradients=1.,\n",
    "    starter_learning_rate=0.0001,\n",
    "    #starter_learning_rate=0.0005,\n",
    "    n_layers_rnn=1,\n",
    "    rnn_size_encoder=400,\n",
    "    rnn_size_decoder=400,\n",
    "    #output_droupouts_kp=[.9, .9, .9, .95, 1.]\n",
    ")\n",
    "print(1)\n",
    "m.build_graph(batch_gen)\n",
    "\n",
    "\n",
    "try:\n",
    "    hd_exp.end()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "hd_exp = hd.Experiment('RNN fav 3')\n",
    "\n",
    "m.train(val_set, coef=unit_std, sum_W=sum_W,\n",
    "        report_every=100, validate_every=0,\n",
    "        hd_exp=hd_exp, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales = df[\n",
    "    (df['item_nbr'] == 1503844) &\n",
    "    (df['store_nbr'] == 44) \n",
    "    \n",
    "]['unit_sales_scaled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEICAYAAABWJCMKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2aaf2878cdd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sm.graphics.tsa.plot_pacf(sales, lags=450)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEICAYAAABWJCMKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2aad0004ceb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sales2, tmp = stats.boxcox((sales+unit_mean))\n",
    "sm.graphics.tsa.plot_pacf(sales2, lags=450)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
